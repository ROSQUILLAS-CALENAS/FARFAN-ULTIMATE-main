"""
Pipeline State Manager

Tracks stage completion status using a JSON state file, recording timestamps, 
artifacts generated, and completion status for each of the 12 pipeline stages 
per document. Provides checkpoint save/restore functionality with automatic 
state validation and integrity checks.
"""

import json
import logging
import os
# # # from dataclasses import dataclass, field  # Module not found  # Module not found  # Module not found
# # # from datetime import datetime  # Module not found  # Module not found  # Module not found
# # # from pathlib import Path  # Module not found  # Module not found  # Module not found
# # # from typing import Any, Dict, List, Optional, Set  # Module not found  # Module not found  # Module not found
# # # from enum import Enum  # Module not found  # Module not found  # Module not found


# Mandatory Pipeline Contract Annotations
__phase__ = "O"
__code__ = "124O"
__stage_order__ = 7

logger = logging.getLogger(__name__)


class StageStatus(Enum):
    """Pipeline stage execution status"""
    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    SKIPPED = "skipped"


@dataclass
class StageArtifact:
    """Represents an artifact generated by a pipeline stage"""
    file_path: str
    artifact_type: str  # e.g., "pdf", "json", "text", "embedding"
    file_size: Optional[int] = None
    checksum: Optional[str] = None
    created_at: Optional[datetime] = None
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "file_path": self.file_path,
            "artifact_type": self.artifact_type,
            "file_size": self.file_size,
            "checksum": self.checksum,
            "created_at": self.created_at.isoformat() if self.created_at else None
        }
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'StageArtifact':
        created_at = None
        if data.get("created_at"):
            created_at = datetime.fromisoformat(data["created_at"])
        
        return cls(
            file_path=data["file_path"],
            artifact_type=data["artifact_type"],
            file_size=data.get("file_size"),
            checksum=data.get("checksum"),
            created_at=created_at
        )


@dataclass
class StageExecution:
    """Represents execution information for a pipeline stage"""
    stage_name: str
    status: StageStatus = StageStatus.PENDING
    start_time: Optional[datetime] = None
    end_time: Optional[datetime] = None
    duration_seconds: Optional[float] = None
    error_message: Optional[str] = None
    artifacts: List[StageArtifact] = field(default_factory=list)
    metrics: Dict[str, Any] = field(default_factory=dict)
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "stage_name": self.stage_name,
            "status": self.status.value,
            "start_time": self.start_time.isoformat() if self.start_time else None,
            "end_time": self.end_time.isoformat() if self.end_time else None,
            "duration_seconds": self.duration_seconds,
            "error_message": self.error_message,
            "artifacts": [artifact.to_dict() for artifact in self.artifacts],
            "metrics": self.metrics
        }
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'StageExecution':
        start_time = None
        if data.get("start_time"):
            start_time = datetime.fromisoformat(data["start_time"])
        
        end_time = None
        if data.get("end_time"):
            end_time = datetime.fromisoformat(data["end_time"])
            
        artifacts = [
            StageArtifact.from_dict(artifact_data) 
            for artifact_data in data.get("artifacts", [])
        ]
        
        return cls(
            stage_name=data["stage_name"],
            status=StageStatus(data.get("status", "pending")),
            start_time=start_time,
            end_time=end_time,
            duration_seconds=data.get("duration_seconds"),
            error_message=data.get("error_message"),
            artifacts=artifacts,
            metrics=data.get("metrics", {})
        )


@dataclass
class DocumentPipelineState:
    """Represents complete pipeline state for a document"""
    document_id: str
    document_path: Optional[str] = None
    pipeline_version: str = "1.0"
    created_at: datetime = field(default_factory=datetime.now)
    updated_at: datetime = field(default_factory=datetime.now)
    stages: Dict[str, StageExecution] = field(default_factory=dict)
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "document_id": self.document_id,
            "document_path": self.document_path,
            "pipeline_version": self.pipeline_version,
            "created_at": self.created_at.isoformat(),
            "updated_at": self.updated_at.isoformat(),
            "stages": {name: stage.to_dict() for name, stage in self.stages.items()}
        }
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'DocumentPipelineState':
        created_at = datetime.fromisoformat(data["created_at"])
        updated_at = datetime.fromisoformat(data["updated_at"])
        
        stages = {}
        for stage_name, stage_data in data.get("stages", {}).items():
            stages[stage_name] = StageExecution.from_dict(stage_data)
            
        return cls(
            document_id=data["document_id"],
            document_path=data.get("document_path"),
            pipeline_version=data.get("pipeline_version", "1.0"),
            created_at=created_at,
            updated_at=updated_at,
            stages=stages
        )


class PipelineStateManager:
    """
    Manages pipeline execution state with checkpointing and recovery capabilities.
    
    Tracks completion status for 12 pipeline stages:
    1. INGESTION - PDF reading, loading, feature extraction, validation
    2. CONTEXT_BUILD - Immutable context, adaptation, lineage tracking
    3. KNOWLEDGE - Knowledge graph, causal graph, embeddings
    4. ANALYSIS - Adaptive analysis, question analysis, evidence processing
    5. CLASSIFICATION - Scoring, risk control
    6. ORCHESTRATION - Routing, decision making, control
    7. SEARCH - Retrieval, indexing, recommendations
    8. ORCHESTRATION - Core orchestration, distribution, workflow
    9. MONITORING - Circuit breaker, monitoring, validation
    10. AGGREGATION - Synthesis, formatting, compilation
    11. INTEGRATION - Metrics, analytics, feedback
    12. SYNTHESIS - Final output generation
    """
    
    # Define the 12 pipeline stages in execution order
    PIPELINE_STAGES = [
        "ingestion_preparation",
        "context_construction", 
        "knowledge_extraction",
        "analysis_nlp",
        "classification_evaluation",
        "routing_decision",
        "search_retrieval",
        "orchestration_control",
        "monitoring_validation",
        "aggregation_reporting", 
        "integration_storage",
        "synthesis_output"
    ]
    
    def __init__(self, state_file_path: Optional[str] = None):
        """
        Initialize PipelineStateManager.
        
        Args:
            state_file_path: Path to state JSON file. Defaults to canonical_flow/pipeline_state.json
        """
        if state_file_path is None:
            canonical_flow_dir = Path(__file__).parent
            state_file_path = canonical_flow_dir / "pipeline_state.json"
        
        self.state_file_path = Path(state_file_path)
        self.state_file_path.parent.mkdir(parents=True, exist_ok=True)
        
        # In-memory state cache
        self.document_states: Dict[str, DocumentPipelineState] = {}
        
        # Load existing state
        self._load_state_from_file()
        
        logger.info(f"PipelineStateManager initialized with state file: {self.state_file_path}")
    
    def _load_state_from_file(self) -> None:
# # #         """Load pipeline state from JSON file"""  # Module not found  # Module not found  # Module not found
        if not self.state_file_path.exists():
            logger.info("No existing state file found, starting with empty state")
            return
        
        try:
            with open(self.state_file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # Load document states
            for doc_id, doc_data in data.get("documents", {}).items():
                try:
                    self.document_states[doc_id] = DocumentPipelineState.from_dict(doc_data)
                except Exception as e:
                    logger.warning(f"Failed to load state for document {doc_id}: {e}")
                    
            logger.info(f"Loaded state for {len(self.document_states)} documents")
            
        except Exception as e:
            logger.error(f"Failed to load state file: {e}")
            # Create backup of corrupted file
            backup_path = f"{self.state_file_path}.backup.{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            try:
                os.rename(self.state_file_path, backup_path)
                logger.info(f"Corrupted state file backed up to: {backup_path}")
            except Exception as backup_error:
                logger.error(f"Failed to backup corrupted state file: {backup_error}")
    
    def _save_state_to_file(self) -> None:
        """Save current pipeline state to JSON file"""
        try:
            # Prepare data for serialization
            data = {
                "version": "1.0",
                "saved_at": datetime.now().isoformat(),
                "documents": {
                    doc_id: doc_state.to_dict() 
                    for doc_id, doc_state in self.document_states.items()
                }
            }
            
            # Write atomically - first to temp file, then rename
            temp_path = f"{self.state_file_path}.tmp"
            with open(temp_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, indent=2, ensure_ascii=False)
            
            os.rename(temp_path, self.state_file_path)
            
        except Exception as e:
            logger.error(f"Failed to save state file: {e}")
            raise
    
    def initialize_document_state(self, document_id: str, document_path: Optional[str] = None) -> DocumentPipelineState:
        """
        Initialize pipeline state for a new document.
        
        Args:
            document_id: Unique identifier for the document
            document_path: Optional path to the document file
            
        Returns:
            Initialized DocumentPipelineState
        """
        if document_id in self.document_states:
            logger.info(f"Document {document_id} already has state initialized")
            return self.document_states[document_id]
        
        doc_state = DocumentPipelineState(
            document_id=document_id,
            document_path=document_path
        )
        
        # Initialize all pipeline stages with PENDING status
        for stage_name in self.PIPELINE_STAGES:
            doc_state.stages[stage_name] = StageExecution(stage_name=stage_name)
        
        self.document_states[document_id] = doc_state
        self._save_state_to_file()
        
        logger.info(f"Initialized pipeline state for document: {document_id}")
        return doc_state
    
    def start_stage(self, document_id: str, stage_name: str) -> None:
        """
        Mark a pipeline stage as started.
        
        Args:
            document_id: Document identifier
            stage_name: Name of the pipeline stage
        """
        if document_id not in self.document_states:
            self.initialize_document_state(document_id)
        
        doc_state = self.document_states[document_id]
        
        if stage_name not in doc_state.stages:
            doc_state.stages[stage_name] = StageExecution(stage_name=stage_name)
        
        stage = doc_state.stages[stage_name]
        stage.status = StageStatus.RUNNING
        stage.start_time = datetime.now()
        stage.error_message = None  # Clear any previous error
        
        doc_state.updated_at = datetime.now()
        
        logger.info(f"Started stage {stage_name} for document {document_id}")
    
    def complete_stage(self, document_id: str, stage_name: str, 
                      artifacts: Optional[List[StageArtifact]] = None,
                      metrics: Optional[Dict[str, Any]] = None) -> None:
        """
        Mark a pipeline stage as completed successfully.
        
        Args:
            document_id: Document identifier 
            stage_name: Name of the pipeline stage
            artifacts: List of artifacts generated by this stage
# # #             metrics: Optional metrics/metadata from stage execution  # Module not found  # Module not found  # Module not found
        """
        if document_id not in self.document_states:
            logger.warning(f"No state found for document {document_id}")
            return
        
        doc_state = self.document_states[document_id]
        
        if stage_name not in doc_state.stages:
            logger.warning(f"Stage {stage_name} not found for document {document_id}")
            return
        
        stage = doc_state.stages[stage_name]
        stage.status = StageStatus.COMPLETED
        stage.end_time = datetime.now()
        
        if stage.start_time:
            stage.duration_seconds = (stage.end_time - stage.start_time).total_seconds()
        
        if artifacts:
            stage.artifacts = artifacts
        
        if metrics:
            stage.metrics = metrics
        
        doc_state.updated_at = datetime.now()
        self._save_state_to_file()
        
        duration_str = f"{stage.duration_seconds:.2f}s" if stage.duration_seconds is not None else "N/A"
        logger.info(f"Completed stage {stage_name} for document {document_id} (duration: {duration_str})")
    
    def fail_stage(self, document_id: str, stage_name: str, error_message: str) -> None:
        """
        Mark a pipeline stage as failed.
        
        Args:
            document_id: Document identifier
            stage_name: Name of the pipeline stage  
            error_message: Error description
        """
        if document_id not in self.document_states:
            logger.warning(f"No state found for document {document_id}")
            return
        
        doc_state = self.document_states[document_id]
        
        if stage_name not in doc_state.stages:
            logger.warning(f"Stage {stage_name} not found for document {document_id}")
            return
        
        stage = doc_state.stages[stage_name]
        stage.status = StageStatus.FAILED
        stage.end_time = datetime.now()
        stage.error_message = error_message
        
        if stage.start_time:
            stage.duration_seconds = (stage.end_time - stage.start_time).total_seconds()
        
        doc_state.updated_at = datetime.now()
        self._save_state_to_file()
        
        logger.error(f"Failed stage {stage_name} for document {document_id}: {error_message}")
    
    def skip_stage(self, document_id: str, stage_name: str, reason: str) -> None:
        """
        Mark a pipeline stage as skipped.
        
        Args:
            document_id: Document identifier
            stage_name: Name of the pipeline stage
            reason: Reason for skipping
        """
        if document_id not in self.document_states:
            logger.warning(f"No state found for document {document_id}")
            return
        
        doc_state = self.document_states[document_id]
        
        if stage_name not in doc_state.stages:
            logger.warning(f"Stage {stage_name} not found for document {document_id}")
            return
        
        stage = doc_state.stages[stage_name]
        stage.status = StageStatus.SKIPPED
        stage.end_time = datetime.now()
        stage.error_message = reason
        
        doc_state.updated_at = datetime.now()
        self._save_state_to_file()
        
        logger.info(f"Skipped stage {stage_name} for document {document_id}: {reason}")
    
    def get_completed_stages(self, document_id: str) -> Set[str]:
        """
        Get set of completed stages for a document.
        
        Args:
            document_id: Document identifier
            
        Returns:
            Set of completed stage names
        """
        if document_id not in self.document_states:
            return set()
        
        doc_state = self.document_states[document_id]
        return {
            stage_name for stage_name, stage in doc_state.stages.items()
            if stage.status == StageStatus.COMPLETED
        }
    
    def get_next_pending_stage(self, document_id: str) -> Optional[str]:
        """
        Get the next pending stage in the pipeline for a document.
        
        Args:
            document_id: Document identifier
            
        Returns:
            Next stage name to execute, or None if all completed
        """
        if document_id not in self.document_states:
# # #             return self.PIPELINE_STAGES[0]  # Start from beginning  # Module not found  # Module not found  # Module not found
        
        doc_state = self.document_states[document_id]
        
        for stage_name in self.PIPELINE_STAGES:
            if stage_name not in doc_state.stages:
                return stage_name
            
            stage = doc_state.stages[stage_name]
            if stage.status in [StageStatus.PENDING, StageStatus.FAILED]:
                return stage_name
        
        return None  # All stages completed
    
    def validate_stage_artifacts(self, document_id: str, stage_name: str) -> bool:
        """
        Validate that all artifacts for a stage still exist and are accessible.
        
        Args:
            document_id: Document identifier
            stage_name: Name of the pipeline stage
            
        Returns:
            True if all artifacts are valid, False otherwise
        """
        if document_id not in self.document_states:
            return False
        
        doc_state = self.document_states[document_id]
        
        if stage_name not in doc_state.stages:
            return False
        
        stage = doc_state.stages[stage_name]
        
        if stage.status != StageStatus.COMPLETED:
            return False
        
        # Check all artifacts exist
        for artifact in stage.artifacts:
            artifact_path = Path(artifact.file_path)
            if not artifact_path.exists():
                logger.warning(f"Missing artifact: {artifact.file_path}")
                return False
            
            # Basic file size validation
            if artifact.file_size is not None:
                actual_size = artifact_path.stat().st_size
                if actual_size != artifact.file_size:
                    logger.warning(f"Artifact size mismatch: {artifact.file_path}")
                    return False
        
        return True
    
    def reset_document_state(self, document_id: str, reason: str = "State corruption detected") -> None:
        """
# # #         Reset pipeline state for a document (force restart from beginning).  # Module not found  # Module not found  # Module not found
        
        Args:
            document_id: Document identifier
            reason: Reason for reset
        """
        if document_id in self.document_states:
            del self.document_states[document_id]
            self._save_state_to_file()
        
        logger.warning(f"Reset pipeline state for document {document_id}: {reason}")
    
    def get_pipeline_progress(self, document_id: str) -> Dict[str, Any]:
        """
        Get pipeline progress summary for a document.
        
        Args:
            document_id: Document identifier
            
        Returns:
            Progress summary with completion percentage and stage details
        """
        if document_id not in self.document_states:
            return {
                "document_id": document_id,
                "progress_percentage": 0.0,
                "completed_stages": 0,
                "total_stages": len(self.PIPELINE_STAGES),
                "current_stage": None,
                "stage_details": {}
            }
        
        doc_state = self.document_states[document_id]
        
        completed_count = 0
        failed_count = 0
        running_count = 0
        current_stage = None
        stage_details = {}
        
        for stage_name in self.PIPELINE_STAGES:
            if stage_name in doc_state.stages:
                stage = doc_state.stages[stage_name]
                stage_details[stage_name] = {
                    "status": stage.status.value,
                    "duration": stage.duration_seconds,
                    "error": stage.error_message,
                    "artifacts_count": len(stage.artifacts)
                }
                
                if stage.status == StageStatus.COMPLETED:
                    completed_count += 1
                elif stage.status == StageStatus.FAILED:
                    failed_count += 1
                elif stage.status == StageStatus.RUNNING:
                    running_count += 1
                    current_stage = stage_name
            else:
                stage_details[stage_name] = {
                    "status": "pending",
                    "duration": None,
                    "error": None,
                    "artifacts_count": 0
                }
        
        if current_stage is None and completed_count < len(self.PIPELINE_STAGES):
            current_stage = self.get_next_pending_stage(document_id)
        
        progress_pct = (completed_count / len(self.PIPELINE_STAGES)) * 100.0
        
        return {
            "document_id": document_id,
            "progress_percentage": progress_pct,
            "completed_stages": completed_count,
            "failed_stages": failed_count,
            "running_stages": running_count,
            "total_stages": len(self.PIPELINE_STAGES),
            "current_stage": current_stage,
            "stage_details": stage_details,
            "created_at": doc_state.created_at.isoformat(),
            "updated_at": doc_state.updated_at.isoformat()
        }
    
    def get_all_documents_summary(self) -> Dict[str, Any]:
        """
        Get summary of all documents in the pipeline state.
        
        Returns:
            Summary with document counts and overall progress
        """
        total_docs = len(self.document_states)
        completed_docs = 0
        in_progress_docs = 0
        failed_docs = 0
        
        for doc_id in self.document_states:
            progress = self.get_pipeline_progress(doc_id)
            
            if progress["progress_percentage"] >= 100.0:
                completed_docs += 1
            elif progress["failed_stages"] > 0:
                failed_docs += 1
            elif progress["progress_percentage"] > 0.0:
                in_progress_docs += 1
        
        return {
            "total_documents": total_docs,
            "completed_documents": completed_docs,
            "in_progress_documents": in_progress_docs,
            "failed_documents": failed_docs,
            "pending_documents": total_docs - completed_docs - in_progress_docs - failed_docs,
            "overall_progress_percentage": (completed_docs / max(1, total_docs)) * 100.0
        }