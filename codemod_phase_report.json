{
  "timestamp": "2025-09-03T17:22:20.114567",
  "dry_run": false,
  "backup_directory": "backups/codemod_phase_20250903_172021",
  "files_moved": [
    {
      "from": "05I_raw_data_generator.py",
      "to": "canonical_flow/I_ingestion_preparation/raw_data_generator.py"
    }
  ],
  "import_changes": {},
  "validation_errors": [
    "Error processing evidence_processor.py: Syntax Error @ 1:1.\ntokenizer error: no matching outer block for dedent\n\n\"\"\"\n^",
    "Error processing run_l_stage_tests.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n#!/usr/bin/env python3\n^",
    "Error processing contracts_validation_utility.py: Syntax Error @ 92:5.\nparser error: error at 91:8: expected one of !=, %, &, (, *, **, +, ,, -, ., /, //, <, <<, <=, ==, >, >=, >>, @, STRING, [, ^, and, f\", if, in, is, not, or, |, }\n\n        }\n        ^",
    "Error processing step_handlers.py: Syntax Error @ 1:1.\ntokenizer error: no matching outer block for dedent\n\n\"\"\"\n^",
    "Error processing validate_contract_imports.py: Syntax Error @ 1:1.\ntokenizer error: no matching outer block for dedent\n\n#!/usr/bin/env python3\n^",
    "Error processing run_contract_validation.py: Syntax Error @ 97:9.\nparser error: error at 96:8: expected INDENT\n\n            print(\"  Import statements:\")\n        ^",
    "Error processing fix_imports_venv.py: Syntax Error @ 153:18.\nparser error: error at 152:17: expected one of (, *, +, -, ..., AWAIT, DEDENT, False, NUMBER, None, True, [, break, continue, lambda, match, not, pass, ~\n\n                line = line.replace('import ', 'import ')\n                 ^",
    "Error processing validate_recovery_system.py: Syntax Error @ 1:1.\ntokenizer error: no matching outer block for dedent\n\n#!/usr/bin/env python3\n^",
    "Error processing implementacion_mapeo.py: Syntax Error @ 25:8.\nparser error: error at 24:7: expected INDENT\n\n    # Fallback when audit logger is not available\n       ^",
    "Error processing run_mcc_tests.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n#!/usr/bin/env python3\n^",
    "Error processing EXTRACTOR DE EVIDENCIAS CONTEXTUAL.py: Syntax Error @ 26:8.\nparser error: error at 25:7: expected INDENT\n\n    # Fallback when audit logger is not available\n       ^",
    "Error processing validate_l_orchestrator.py: Syntax Error @ 350:16.\nparser error: error at 349:15: expected INDENT\n\n        shutil.rmtree(temp_dir, ignore_errors=True)\n               ^",
    "Error processing config_loader.py: Syntax Error @ 1:1.\ntokenizer error: unterminated triple-quoted string literal\n\n\"\"\"\n^",
    "Error processing run_demo_quick.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n#!/usr/bin/env python3\n^",
    "Error processing pdf_reader.py: Syntax Error @ 728:12.\nparser error: error at 727:11: expected INDENT\n\n        return {\"error\": \"ArtifactManager not available\"}\n           ^",
    "Error processing adaptive_analyzer.py: Syntax Error @ 1:1.\ntokenizer error: no matching outer block for dedent\n\n\"\"\"\n^",
    "Error processing validate_g_aggregation.py: Syntax Error @ 182:16.\nparser error: error at 181:15: expected INDENT\n\n            if \"numpy\" in str(e) or \"scipy\" in str(e):\n               ^",
    "Error processing answer_formatter.py: Syntax Error @ 1:1.\ntokenizer error: no matching outer block for dedent\n\n\"\"\"\n^",
    "Error processing serializable_wrappers.py: Syntax Error @ 86:12.\nparser error: error at 85:11: expected INDENT\n\n        logging.warning(f\"Failed to import EGW components: {e}\")\n           ^",
    "Error processing models.py: Syntax Error @ 19:8.\nparser error: error at 18:7: expected INDENT\n\n    # Fallback to pydantic BaseModel if optional custom base is missing\n       ^",
    "Error processing analysis_nlp_orchestrator.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing telemetry_collector.py: Syntax Error @ 1:1.\ntokenizer error: no matching outer block for dedent\n\n\"\"\"\n^",
    "Error processing conformal_risk_demo.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing workflow_engine.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing simple_test_math_enhancer.py: Syntax Error @ 1:1.\ntokenizer error: no matching outer block for dedent\n\n#!/usr/bin/env python3\n^",
    "Error processing core_orchestrator.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing integration_example.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n#!/usr/bin/env python3\n^",
    "Error processing pipeline_orchestrator.py: Syntax Error @ 14:8.\nparser error: error at 13:7: expected INDENT\n\n    DeterministicFlowRiskGuard = None  # type: ignore\n       ^",
    "Error processing simple_import_test.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n#!/usr/bin/env python3\n^",
    "Error processing distributed_processor.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n#!/usr/bin/env python3\n^",
    "Error processing test_environment_automation.py: Syntax Error @ 1:1.\ntokenizer error: no matching outer block for dedent\n\n#!/usr/bin/env python3\n^",
    "Error processing raw_data_generator.py: Syntax Error @ 32:8.\nparser error: error at 31:7: expected INDENT\n\n    class TfidfVectorizer:  # minimal fallback\n       ^",
    "Error processing mathematical_compatibility_matrix.py: Syntax Error @ 65:5.\nparser error: error at 64:4: expected INDENT\n\n        \"\"\"Check if a version satisfies this constraint\"\"\"\n    ^",
    "Error processing simple_g_aggregation_test.py: Syntax Error @ 45:12.\nparser error: error at 44:11: expected INDENT\n\n        print(\"\u2717 Cannot import meso_aggregator\")\n           ^",
    "Error processing run_basic_tests.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n#!/usr/bin/env python3\n^",
    "Error processing quickstart_notebook.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing schema_registry.py: Syntax Error @ 19:8.\nparser error: error at 18:7: expected INDENT\n\n    # Fallback for version comparison if packaging is not available\n       ^",
    "Error processing pipeline_orchestrator_audit.py: Syntax Error @ 71:1.\nparser error: error at 70:11: expected INDENT\n\n    return nodes, errors\n                       ^",
    "Error processing validate_pipeline_analysis.py: Syntax Error @ 1:1.\ntokenizer error: no matching outer block for dedent\n\n#!/usr/bin/env python3\n^",
    "Error processing score_calculator.py: Syntax Error @ 1:1.\ntokenizer error: no matching outer block for dedent\n\n\"\"\"\n^",
    "Error processing analytics_enhancement.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing handoff_validation_system.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing intelligent_recommendation_engine.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing question_analyzer.py: Syntax Error @ 36:8.\nparser error: error at 35:7: expected INDENT\n\n    SentenceTransformer = None\n       ^",
    "Error processing cluster_execution_controller.py: Syntax Error @ 1:1.\ntokenizer error: no matching outer block for dedent\n\n\"\"\"\n^",
    "Error processing process_inventory.py: Syntax Error @ 27:8.\nparser error: error at 26:7: expected INDENT\n\n    Repo = None\n       ^",
    "Error processing pipeline_value_analysis_system.py: Syntax Error @ 1:1.\ntokenizer error: no matching outer block for dedent\n\n#!/usr/bin/env python3\n^",
    "Error processing validate_dependencies.py: Syntax Error @ 1:1.\ntokenizer error: no matching outer block for dedent\n\n#!/usr/bin/env python3\n^",
    "Error processing validate_monitoring.py: Syntax Error @ 1:1.\ntokenizer error: no matching outer block for dedent\n\n#!/usr/bin/env python3\n^",
    "Error processing src:nlp_engine:semantic_inference_engine.py: Syntax Error @ 170:16.\nparser error: error at 169:15: expected one of !=, %, &, (, *, **, +, ,, -, ., /, //, <, <<, <=, ==, >, >=, >>, @, ASYNC, STRING, [, ^, and, f\", for, if, in, is, not, or, |\n\n            logger.error(f\"Invalid YAML in configuration file: {e}\")\n               ^",
    "Error processing run_safety_demo.py: Syntax Error @ 1:1.\ntokenizer error: no matching outer block for dedent\n\n#!/usr/bin/env python3\n^",
    "Error processing normative_validator.py: Syntax Error @ 1826:12.\nparser error: error at 1825:11: expected INDENT\n\n        return {\"error\": \"ArtifactManager not available\"}\n           ^",
    "Error processing gw_alignment.py: Syntax Error @ 19:8.\nparser error: error at 18:7: expected INDENT\n\n    def cosine_distances(A, B):\n       ^",
    "Error processing test_determinism_verification.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n#!/usr/bin/env python3\n^",
    "Error processing canonical_web_server.py: Syntax Error @ 492:35.\nparser error: error at 491:34: expected one of +, -, ..., AWAIT, False, NAME, NUMBER, None, True, lambda, not, ~\n\n                .then(data => updateDashboard(data))\n                                  ^",
    "Error processing validate_stage_middleware.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing metrics_collector.py: Syntax Error @ 1:1.\ntokenizer error: no matching outer block for dedent\n\n\"\"\"\n^",
    "Error processing validate_parallel_processor.py: Syntax Error @ 1:1.\ntokenizer error: no matching outer block for dedent\n\n#!/usr/bin/env python3\n^",
    "Error processing feature_extractor.py: Syntax Error @ 1:1.\ntokenizer error: unmatched '}'\n\nimport json\n^",
    "Error processing dnp_alignment_adapter.py: Syntax Error @ 1:1.\ntokenizer error: no matching outer block for dedent\n\n\"\"\"\n^",
    "Error processing validate_l_stage_tests.py: Syntax Error @ 1:1.\ntokenizer error: no matching outer block for dedent\n\n#!/usr/bin/env python3\n^",
    "Error processing import_test.py: Syntax Error @ 1:1.\ntokenizer error: leading zeros in decimal integer literals are not permitted; use an 0o prefix for octal integers\n\nfrom 05I_raw_data_generator import test\n^",
    "Error processing enhanced_core_orchestrator.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing validate_decalogo_registry.py: Syntax Error @ 1:1.\ntokenizer error: no matching outer block for dedent\n\n#!/usr/bin/env python3\n^",
    "Error processing report_compiler.py: Syntax Error @ 21:8.\nparser error: error at 20:7: expected INDENT\n\n    # Fallback for testing - these will be mocked\n       ^",
    "Error processing validate_import_safety.py: Syntax Error @ 1:1.\ntokenizer error: no matching outer block for dedent\n\n#!/usr/bin/env python3\n^",
    "Error processing evidence_validation_model.py: Syntax Error @ 26:8.\nparser error: error at 25:7: expected INDENT\n\n    # Fallback when audit logger is not available\n       ^",
    "Error processing advanced_loader.py: Syntax Error @ 300:12.\nparser error: error at 299:11: expected INDENT\n\n        return {\"error\": \"ArtifactManager not available\"}\n           ^",
    "Error processing PIPELINEORCHESTRATOR.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing validate_safety_controller.py: Syntax Error @ 1:1.\ntokenizer error: no matching outer block for dedent\n\n#!/usr/bin/env python3\n^",
    "Error processing validate_installation.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n#!/usr/bin/env python3\n^",
    "Error processing adaptive_scoring_engine.py: Syntax Error @ 1:1.\ntokenizer error: no matching outer block for dedent\n\n\"\"\"\n^",
    "Error processing run_canonical_stability.py: Syntax Error @ 37:8.\nparser error: error at 36:7: expected INDENT\n\n    pdf_text_process = None  # type: ignore\n       ^",
    "Error processing validate_operadic_implementation.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n#!/usr/bin/env python3\n^",
    "Error processing test_visual_framework.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing validate_handoff_system.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n#!/usr/bin/env python3\n^",
    "Error processing run_g_aggregation_tests.py: Syntax Error @ 138:12.\nparser error: error at 137:11: expected INDENT\n\n        print(\"\u2717 Cannot import meso_aggregator\")\n           ^",
    "Error processing evidence_router.py: Syntax Error @ 19:8.\nparser error: error at 18:7: expected INDENT\n\n    EvidenceType = None  # type: ignore\n       ^",
    "Error processing answer_synthesizer.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing validate_dashboard_implementation.py: Syntax Error @ 1:1.\ntokenizer error: no matching outer block for dedent\n\n#!/usr/bin/env python3\n^",
    "Error processing conformal_risk_certification_demo.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing simple_test_decalogo.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing validate_mathematical_foundations.py: Syntax Error @ 1:1.\ntokenizer error: no matching outer block for dedent\n\n#!/usr/bin/env python3\n^",
    "Error processing simple_integration_test.py: Syntax Error @ 1:1.\ntokenizer error: no matching outer block for dedent\n\n\"\"\"\n^",
    "Error processing test_wasserstein_fisher_rao.py: Syntax Error @ 1:1.\ntokenizer error: no matching outer block for dedent\n\n#!/usr/bin/env python3\n^",
    "Error processing question_level_scoring_pipeline.py: Syntax Error @ 34:8.\nparser error: error at 33:7: expected INDENT\n\n    get_audit_logger = None\n       ^",
    "Error processing validate_mathematical_pipeline.py: Syntax Error @ 1:1.\ntokenizer error: no matching outer block for dedent\n\n#!/usr/bin/env python3\n^",
    "Error processing run_k_workflow_tests.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n#!/usr/bin/env python3\n^",
    "Error processing test_serializable_wrappers.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n#!/usr/bin/env python3\n^",
    "Error processing pdf_processing_error_handler.py: Syntax Error @ 549:9.\nparser error: error at 548:8: expected INDENT\n\n            # Validate all files first\n        ^",
    "Error processing demo_import_safety.py: Syntax Error @ 1:1.\ntokenizer error: no matching outer block for dedent\n\n#!/usr/bin/env python3\n^",
    "Error processing service_discovery.py: Syntax Error @ 277:1.\nparser error: error at 276:15: expected INDENT\n\n        return instances\n                       ^",
    "Error processing embedding_builder.py: Syntax Error @ 27:8.\nparser error: error at 26:7: expected INDENT\n\n    def cosine_similarity(A, B=None):\n       ^",
    "Error processing connection_pool.py: Syntax Error @ 40:8.\nparser error: error at 39:7: expected INDENT\n\n    Gauge = None  # type: ignore\n       ^",
    "Error processing standards_alignment/__init__.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"Standards Alignment System using Gromov-Wasserstein Optimal Transport.\n^",
    "Error processing tools/snapshot_guard.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n#!/usr/bin/env python3\n^",
    "Error processing tools/canonical_path_auditor.py: Syntax Error @ 1:1.\ntokenizer error: no matching outer block for dedent\n\n#!/usr/bin/env python3\n^",
    "Error processing tools/dependency_audit.py: Syntax Error @ 1:1.\ntokenizer error: unterminated triple-quoted string literal\n\n#!/usr/bin/env python3\n^",
    "Error processing tools/sort_sanity.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n#!/usr/bin/env python3\n^",
    "Error processing tools/plan_diff.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n#!/usr/bin/env python3\n^",
    "Error processing tools/certificate_generator.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n#!/usr/bin/env python3\n^",
    "Error processing tools/rc_check.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n#!/usr/bin/env python3\n^",
    "Error processing tools/generate_flux_report.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n#!/usr/bin/env python3\n^",
    "Error processing tools/pic_probe.py: Syntax Error @ 1:1.\ntokenizer error: no matching outer block for dedent\n\n#!/usr/bin/env python3\n^",
    "Error processing tools/retrieval_trace.py: Syntax Error @ 1:1.\ntokenizer error: no matching outer block for dedent\n\n\"\"\"\n^",
    "Error processing contracts/demo_integration.py: Syntax Error @ 1:1.\ntokenizer error: no matching outer block for dedent\n\n#!/usr/bin/env python3\n^",
    "Error processing contracts/__init__.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing contracts/schemas.py: Syntax Error @ 33:5.\nparser error: error at 32:4: expected INDENT\n\n    # For Pydantic v1 compatibility\n    ^",
    "Error processing contracts/test_schemas.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing tests/test_rc.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing tests/test_pdf_error_handling.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing tests/test_full_deterministic_router.py: Syntax Error @ 1:1.\ntokenizer error: no matching outer block for dedent\n\n#!/usr/bin/env python3\n^",
    "Error processing tests/test_l_stage_determinism.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing tests/test_report_compiler.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing tests/test_lineage_tracker.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing tests/test_calibration_ci.py: Syntax Error @ 46:8.\nparser error: error at 45:7: expected INDENT\n\n    warnings.warn(f\"Could not import all modules: {e}\")\n       ^",
    "Error processing tests/test_toc.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing tests/test_evidence_validation.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing tests/test_total_ordering.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\nimport unittest\n^",
    "Error processing tests/test_basic_functionality.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n#!/usr/bin/env python3\n^",
    "Error processing tests/test_code_quality_fixes_minimal.py: Syntax Error @ 1:1.\ntokenizer error: no matching outer block for dedent\n\n#!/usr/bin/env python3\n^",
    "Error processing tests/test_gw_alignment.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"Tests for Gromov-Wasserstein alignment functionality.\"\"\"\n^",
    "Error processing tests/test_retriever_determinism.py: Syntax Error @ 1:1.\ntokenizer error: no matching outer block for dedent\n\n\"\"\"\n^",
    "Error processing tests/test_code_quality_fixes.py: Syntax Error @ 1:1.\ntokenizer error: no matching outer block for dedent\n\n\"\"\"\n^",
    "Error processing tests/test_l_stage_preflight.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing tests/test_routing_contract.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\nimport unittest\n^",
    "Error processing tests/test_bmc.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing tests/test_stable_gw_aligner.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"Tests for stable EGW aligner.\"\"\"\n^",
    "Error processing tests/test_fixtures.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing tests/test_snapshot.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing tests/test_immutable_context_basic.py: Syntax Error @ 1:1.\ntokenizer error: no matching outer block for dedent\n\n#!/usr/bin/env python3\n^",
    "Error processing tests/test_l_stage_assertions.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing tests/test_api.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"Tests for main API functionality.\"\"\"\n^",
    "Error processing tests/test_constraint_validator.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing tests/test_deterministic_retrieval.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing tests/test_evidence_processor.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing tests/test_meso_aggregator.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing tests/test_answer_formatter.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing tests/test_snapshot_contract.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\nimport unittest\n^",
    "Error processing tests/test_deterministic_router_simple.py: Syntax Error @ 1:1.\ntokenizer error: no matching outer block for dedent\n\n#!/usr/bin/env python3\n^",
    "Error processing tests/test_ffc.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n#!/usr/bin/env python3\n^",
    "Error processing tests/test_conformal_risk_system.py: Syntax Error @ 1:1.\ntokenizer error: no matching outer block for dedent\n\n\"\"\"\n^",
    "Error processing tests/test_pic.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing calibration_safety_governance/provenance_tracker.py: Syntax Error @ 159:13.\nparser error: error at 158:12: expected INDENT\n\n            self.logger.info(f\"Loaded metadata for {len(self.enhancement_metadata)} enhancements\")\n                                                                                                 ^",
    "Error processing calibration_safety_governance/test_orchestration_system.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing dashboard/persistence.py: Syntax Error @ 182:5.\nparser error: error at 181:15: expected INDENT\n\n        return None\n                  ^",
    "Error processing retrieval_engine/vector_index.py: Syntax Error @ 59:8.\nparser error: error at 58:7: expected INDENT\n\n    SentenceTransformer = None\n       ^",
    "Error processing retrieval_engine/lexical_index.py: Syntax Error @ 225:5.\nparser error: error at 224:4: expected INDENT\n\n        \"\"\"\n    ^",
    "Error processing examples/conformal_risk_demo.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n#!/usr/bin/env python3\n^",
    "Error processing examples/early_error_detector_demo.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n#!/usr/bin/env python3\n^",
    "Error processing scripts/validate_dependency_compatibility.py: Syntax Error @ 1:1.\ntokenizer error: no matching outer block for dedent\n\n#!/usr/bin/env python3\n^",
    "Error processing canonical_flow/pipeline_state_manager.py: Syntax Error @ 454:9.\nparser error: error at 453:18: expected INDENT\n\n        doc_state = self.document_states[document_id]\n                                                    ^",
    "Error processing egw_query_expansion/mathematical_foundations.py: Syntax Error @ 24:8.\nparser error: error at 23:7: expected INDENT\n\n    # Fallback if import_safety not available\n       ^",
    "Error processing egw_query_expansion/__init__.py: Syntax Error @ 27:8.\nparser error: error at 26:7: expected INDENT\n\n    pass\n       ^",
    "Error processing src/stages/K_knowledge_extraction/deterministic_embedder.py: Syntax Error @ 232:5.\nparser error: error at 231:4: expected INDENT\n\n        \"\"\"Generate embeddings for chunks with deterministic behavior\"\"\"\n    ^",
    "Error processing backups/codemod_phase_20250903_172021/canonical_flow/pipeline_state_manager.py: Syntax Error @ 454:9.\nparser error: error at 453:18: expected INDENT\n\n        doc_state = self.document_states[document_id]\n                                                    ^",
    "Error processing backups/codemod_phase_20250903_172021/canonical_flow/analysis/integration_demo.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n#!/usr/bin/env python3\n^",
    "Error processing backups/codemod_phase_20250903_172021/canonical_flow/analysis/test_artifact_generator.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n#!/usr/bin/env python3\n^",
    "Error processing backups/codemod_phase_20250903_172021/canonical_flow/analysis/test_unit_artifact_generator.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n#!/usr/bin/env python3\n^",
    "Error processing backups/codemod_phase_20250903_172021/canonical_flow/calibration/calibration_dashboard.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing backups/codemod_phase_20250903_172021/canonical_flow/calibration/__init__.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing backups/codemod_phase_20250903_172021/canonical_flow/A_analysis_nlp/evidence_processor.py: Syntax Error @ 1:1.\ntokenizer error: no matching outer block for dedent\n\n\"\"\"\n^",
    "Error processing backups/codemod_phase_20250903_172021/canonical_flow/A_analysis_nlp/evaluation_driven_processor.py: Syntax Error @ 34:8.\nparser error: error at 33:7: expected INDENT\n\n    IntelligentRecommendationEngine = None\n       ^",
    "Error processing backups/codemod_phase_20250903_172021/canonical_flow/A_analysis_nlp/question_analyzer.py: Syntax Error @ 42:8.\nparser error: error at 41:7: expected INDENT\n\n    SentenceTransformer = None\n       ^",
    "Error processing backups/codemod_phase_20250903_172021/canonical_flow/mathematical_enhancers/mathematical_pipeline_coordinator.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing backups/codemod_phase_20250903_172021/canonical_flow/mathematical_enhancers/__init__.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing backups/codemod_phase_20250903_172021/canonical_flow/mathematical_enhancers/knowledge_enhancer.py: Syntax Error @ 73:8.\nparser error: error at 72:7: expected INDENT\n\n    # Mock classes for standalone operation\n       ^",
    "Error processing backups/codemod_phase_20250903_172021/canonical_flow/mathematical_enhancers/ingestion_enhancer.py: Syntax Error @ 32:8.\nparser error: error at 31:7: expected INDENT\n\n    # Fallback definitions for testing\n       ^",
    "Error processing backups/codemod_phase_20250903_172021/canonical_flow/mathematical_enhancers/pre_flight_validator.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing backups/codemod_phase_20250903_172021/canonical_flow/mathematical_enhancers/orchestration_enhancer.py: Syntax Error @ 1024:9.\nparser error: error at 1023:9: expected one of +, -, ..., AWAIT, False, NAME, NUMBER, None, True, not, ~\n\n        )\n        ^",
    "Error processing backups/codemod_phase_20250903_172021/canonical_flow/mathematical_enhancers/analysis_enhancer.py: Syntax Error @ 33:8.\nparser error: error at 32:7: expected INDENT\n\n    class TfidfVectorizer:  # minimal fallback\n       ^",
    "Error processing backups/codemod_phase_20250903_172021/canonical_flow/mathematical_enhancers/context_enhancer.py: Syntax Error @ 37:8.\nparser error: error at 36:7: expected INDENT\n\n    # Minimal fallback definitions for testing\n       ^",
    "Error processing backups/codemod_phase_20250903_172021/canonical_flow/K_knowledge_extraction/test_causal_graph_constructor.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing backups/codemod_phase_20250903_172021/canonical_flow/L_classification_evaluation/demo_schema_validation.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing backups/codemod_phase_20250903_172021/canonical_flow/L_classification_evaluation/stage_orchestrator.py: Syntax Error @ 34:8.\nparser error: error at 33:7: expected INDENT\n\n    # Fallback for standalone execution\n       ^",
    "Error processing backups/codemod_phase_20250903_172021/canonical_flow/L_classification_evaluation/test_conformal_prediction.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing backups/codemod_phase_20250903_172021/canonical_flow/L_classification_evaluation/__init__.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"Stage: L_classification_evaluation\"\"\"\n^",
    "Error processing backups/codemod_phase_20250903_172021/canonical_flow/L_classification_evaluation/question_registry.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing backups/codemod_phase_20250903_172021/canonical_flow/L_classification_evaluation/test_evidence_adapter.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing backups/codemod_phase_20250903_172021/canonical_flow/L_classification_evaluation/schemas.py: Syntax Error @ 603:9.\nparser error: error at 602:8: expected one of !=, %, &, (, *, **, +, ,, -, ., /, //, <, <<, <=, ==, >, >=, >>, @, ASYNC, STRING, [, ^, and, f\", for, if, in, is, not, or, |\n\n    'ResponseValue',\n        ^",
    "Error processing backups/codemod_phase_20250903_172021/canonical_flow/L_classification_evaluation/test_schemas.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing backups/codemod_phase_20250903_172021/canonical_flow/knowledge/knowledge_audit_demo.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing backups/codemod_phase_20250903_172021/canonical_flow/knowledge/__init__.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing backups/codemod_phase_20250903_172021/canonical_flow/G_aggregation_reporting/meso_aggregator.py: Syntax Error @ 1:1.\ntokenizer error: unterminated triple-quoted string literal\n\n#!/usr/bin/env python3\n^",
    "Error processing backups/codemod_phase_20250903_172021/canonical_flow/I_ingestion_preparation/gate_validation_system.py: Syntax Error @ 1:1.\ntokenizer error: no matching outer block for dedent\n\n\"\"\"\n^",
    "Error processing backups/codemod_phase_20250903_172021/canonical_flow/I_ingestion_preparation/raw_data_generator.py: Syntax Error @ 58:8.\nparser error: error at 57:7: expected INDENT\n\n    TfidfVectorizer = None\n       ^",
    "Error processing backups/codemod_phase_20250903_172021/canonical_flow/I_ingestion_preparation/__init__.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing backups/codemod_phase_20250903_172021/canonical_flow/I_ingestion_preparation/ingestion_orchestrator.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing egw_query_expansion/mocks/__init__.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing egw_query_expansion/core/m_c_c_engine_monotone_compliance_evaluator.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n# mcc.py\n^",
    "Error processing egw_query_expansion/core/deterministic_router.py: Syntax Error @ 1:1.\ntokenizer error: unterminated triple-quoted string literal\n\n\"\"\"\n^",
    "Error processing egw_query_expansion/core/m_c_c_label_evaluation_engine.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n# mcc.py\n^",
    "Error processing egw_query_expansion/core/confluent_orchestrator.py: Syntax Error @ 429:17.\nparser error: error at 428:16: expected INDENT\n\n            self.message_queue.append(message)\n                ^",
    "Error processing egw_query_expansion/core/__init__.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing egw_query_expansion/core/context_adapter.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing egw_query_expansion/core/gw_alignment.py: Syntax Error @ 31:21.\nparser error: error at 30:20: expected INDENT\n\nif scipy_linalg_result.success:\n                    ^",
    "Error processing egw_query_expansion/core/task_selector_demo.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing egw_query_expansion/tests/test_library_compatibility.py: Syntax Error @ 34:8.\nparser error: error at 33:7: expected INDENT\n\n    # Fallback for older Python versions\n       ^",
    "Error processing egw_query_expansion/tests/test_submodular_selector.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing egw_query_expansion/tests/test_submodular_task_selector.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing egw_query_expansion/tests/test_safety_import.py: Syntax Error @ 1:1.\ntokenizer error: no matching outer block for dedent\n\n\"\"\"\n^",
    "Error processing egw_query_expansion/tests/test_auto_enhancement_orchestrator.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing egw_query_expansion/tests/test_deterministic_router.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing egw_query_expansion/tests/test_early_error_detector.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing egw_query_expansion/tests/test_distributed_processor.py: Syntax Error @ 1:1.\ntokenizer error: no matching outer block for dedent\n\n\"\"\"\n^",
    "Error processing egw_query_expansion/tests/test_permutation_invariant_processor.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing egw_query_expansion/tests/test_mathematical_foundations.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing egw_query_expansion/tests/test_beir_evaluation.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing egw_query_expansion/tests/test_conformal_risk_control.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing egw_query_expansion/tests/test_immutable_context.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing egw_query_expansion/tests/test_confluent_orchestrator.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing egw_query_expansion/tests/test_mathematical_safety_controller.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing egw_query_expansion/cli/troubleshoot.py: Syntax Error @ 1:1.\ntokenizer error: no matching outer block for dedent\n\n#!/usr/bin/env python3\n^",
    "Error processing canonical_flow/analysis/integration_demo.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n#!/usr/bin/env python3\n^",
    "Error processing canonical_flow/analysis/test_artifact_generator.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n#!/usr/bin/env python3\n^",
    "Error processing canonical_flow/analysis/test_unit_artifact_generator.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n#!/usr/bin/env python3\n^",
    "Error processing canonical_flow/calibration/calibration_dashboard.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing canonical_flow/calibration/__init__.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing canonical_flow/A_analysis_nlp/evidence_processor.py: Syntax Error @ 1:1.\ntokenizer error: no matching outer block for dedent\n\n\"\"\"\n^",
    "Error processing canonical_flow/A_analysis_nlp/evaluation_driven_processor.py: Syntax Error @ 34:8.\nparser error: error at 33:7: expected INDENT\n\n    IntelligentRecommendationEngine = None\n       ^",
    "Error processing canonical_flow/A_analysis_nlp/question_analyzer.py: Syntax Error @ 42:8.\nparser error: error at 41:7: expected INDENT\n\n    SentenceTransformer = None\n       ^",
    "Error processing canonical_flow/mathematical_enhancers/mathematical_pipeline_coordinator.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing canonical_flow/mathematical_enhancers/__init__.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing canonical_flow/mathematical_enhancers/knowledge_enhancer.py: Syntax Error @ 73:8.\nparser error: error at 72:7: expected INDENT\n\n    # Mock classes for standalone operation\n       ^",
    "Error processing canonical_flow/mathematical_enhancers/ingestion_enhancer.py: Syntax Error @ 32:8.\nparser error: error at 31:7: expected INDENT\n\n    # Fallback definitions for testing\n       ^",
    "Error processing canonical_flow/mathematical_enhancers/pre_flight_validator.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing canonical_flow/mathematical_enhancers/orchestration_enhancer.py: Syntax Error @ 1024:9.\nparser error: error at 1023:9: expected one of +, -, ..., AWAIT, False, NAME, NUMBER, None, True, not, ~\n\n        )\n        ^",
    "Error processing canonical_flow/mathematical_enhancers/analysis_enhancer.py: Syntax Error @ 33:8.\nparser error: error at 32:7: expected INDENT\n\n    class TfidfVectorizer:  # minimal fallback\n       ^",
    "Error processing canonical_flow/mathematical_enhancers/context_enhancer.py: Syntax Error @ 37:8.\nparser error: error at 36:7: expected INDENT\n\n    # Minimal fallback definitions for testing\n       ^",
    "Error processing canonical_flow/K_knowledge_extraction/test_causal_graph_constructor.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing canonical_flow/L_classification_evaluation/demo_schema_validation.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing canonical_flow/L_classification_evaluation/stage_orchestrator.py: Syntax Error @ 34:8.\nparser error: error at 33:7: expected INDENT\n\n    # Fallback for standalone execution\n       ^",
    "Error processing canonical_flow/L_classification_evaluation/test_conformal_prediction.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing canonical_flow/L_classification_evaluation/__init__.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"Stage: L_classification_evaluation\"\"\"\n^",
    "Error processing canonical_flow/L_classification_evaluation/question_registry.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing canonical_flow/L_classification_evaluation/test_evidence_adapter.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing canonical_flow/L_classification_evaluation/schemas.py: Syntax Error @ 603:9.\nparser error: error at 602:8: expected one of !=, %, &, (, *, **, +, ,, -, ., /, //, <, <<, <=, ==, >, >=, >>, @, ASYNC, STRING, [, ^, and, f\", for, if, in, is, not, or, |\n\n    'ResponseValue',\n        ^",
    "Error processing canonical_flow/L_classification_evaluation/test_schemas.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing canonical_flow/knowledge/knowledge_audit_demo.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing canonical_flow/knowledge/__init__.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing canonical_flow/G_aggregation_reporting/meso_aggregator.py: Syntax Error @ 1:1.\ntokenizer error: unterminated triple-quoted string literal\n\n#!/usr/bin/env python3\n^",
    "Error processing canonical_flow/I_ingestion_preparation/gate_validation_system.py: Syntax Error @ 1:1.\ntokenizer error: no matching outer block for dedent\n\n\"\"\"\n^",
    "Error processing canonical_flow/I_ingestion_preparation/__init__.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing canonical_flow/I_ingestion_preparation/ingestion_orchestrator.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing tests/integration/test_k_knowledge_extraction_workflow.py: Syntax Error @ 1:1.\ntokenizer error: no matching outer block for dedent\n\n#!/usr/bin/env python3\n^",
    "Error processing tests/evaluation/test_audit_logger.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n#!/usr/bin/env python3\n^",
    "Error processing canonical_flow/pipeline_state_manager.py: Syntax Error @ 454:9.\nparser error: error at 453:18: expected INDENT\n\n        doc_state = self.document_states[document_id]\n                                                    ^",
    "Error processing canonical_flow/analysis/integration_demo.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n#!/usr/bin/env python3\n^",
    "Error processing canonical_flow/analysis/test_artifact_generator.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n#!/usr/bin/env python3\n^",
    "Error processing canonical_flow/analysis/test_unit_artifact_generator.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n#!/usr/bin/env python3\n^",
    "Error processing canonical_flow/calibration/calibration_dashboard.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing canonical_flow/calibration/__init__.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing canonical_flow/A_analysis_nlp/evidence_processor.py: Syntax Error @ 1:1.\ntokenizer error: no matching outer block for dedent\n\n\"\"\"\n^",
    "Error processing canonical_flow/A_analysis_nlp/evaluation_driven_processor.py: Syntax Error @ 34:8.\nparser error: error at 33:7: expected INDENT\n\n    IntelligentRecommendationEngine = None\n       ^",
    "Error processing canonical_flow/A_analysis_nlp/question_analyzer.py: Syntax Error @ 42:8.\nparser error: error at 41:7: expected INDENT\n\n    SentenceTransformer = None\n       ^",
    "Error processing canonical_flow/mathematical_enhancers/mathematical_pipeline_coordinator.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing canonical_flow/mathematical_enhancers/__init__.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing canonical_flow/mathematical_enhancers/knowledge_enhancer.py: Syntax Error @ 73:8.\nparser error: error at 72:7: expected INDENT\n\n    # Mock classes for standalone operation\n       ^",
    "Error processing canonical_flow/mathematical_enhancers/ingestion_enhancer.py: Syntax Error @ 32:8.\nparser error: error at 31:7: expected INDENT\n\n    # Fallback definitions for testing\n       ^",
    "Error processing canonical_flow/mathematical_enhancers/pre_flight_validator.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing canonical_flow/mathematical_enhancers/orchestration_enhancer.py: Syntax Error @ 1024:9.\nparser error: error at 1023:9: expected one of +, -, ..., AWAIT, False, NAME, NUMBER, None, True, not, ~\n\n        )\n        ^",
    "Error processing canonical_flow/mathematical_enhancers/analysis_enhancer.py: Syntax Error @ 33:8.\nparser error: error at 32:7: expected INDENT\n\n    class TfidfVectorizer:  # minimal fallback\n       ^",
    "Error processing canonical_flow/mathematical_enhancers/context_enhancer.py: Syntax Error @ 37:8.\nparser error: error at 36:7: expected INDENT\n\n    # Minimal fallback definitions for testing\n       ^",
    "Error processing canonical_flow/K_knowledge_extraction/test_causal_graph_constructor.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing canonical_flow/L_classification_evaluation/demo_schema_validation.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing canonical_flow/L_classification_evaluation/stage_orchestrator.py: Syntax Error @ 34:8.\nparser error: error at 33:7: expected INDENT\n\n    # Fallback for standalone execution\n       ^",
    "Error processing canonical_flow/L_classification_evaluation/test_conformal_prediction.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing canonical_flow/L_classification_evaluation/__init__.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"Stage: L_classification_evaluation\"\"\"\n^",
    "Error processing canonical_flow/L_classification_evaluation/question_registry.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing canonical_flow/L_classification_evaluation/test_evidence_adapter.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing canonical_flow/L_classification_evaluation/schemas.py: Syntax Error @ 603:9.\nparser error: error at 602:8: expected one of !=, %, &, (, *, **, +, ,, -, ., /, //, <, <<, <=, ==, >, >=, >>, @, ASYNC, STRING, [, ^, and, f\", for, if, in, is, not, or, |\n\n    'ResponseValue',\n        ^",
    "Error processing canonical_flow/L_classification_evaluation/test_schemas.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing canonical_flow/knowledge/knowledge_audit_demo.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing canonical_flow/knowledge/__init__.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing canonical_flow/G_aggregation_reporting/meso_aggregator.py: Syntax Error @ 1:1.\ntokenizer error: unterminated triple-quoted string literal\n\n#!/usr/bin/env python3\n^",
    "Error processing canonical_flow/I_ingestion_preparation/gate_validation_system.py: Syntax Error @ 1:1.\ntokenizer error: no matching outer block for dedent\n\n\"\"\"\n^",
    "Error processing canonical_flow/I_ingestion_preparation/__init__.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing canonical_flow/I_ingestion_preparation/ingestion_orchestrator.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing tests/test_rc.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing tests/test_pdf_error_handling.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing tests/test_full_deterministic_router.py: Syntax Error @ 1:1.\ntokenizer error: no matching outer block for dedent\n\n#!/usr/bin/env python3\n^",
    "Error processing tests/test_l_stage_determinism.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing tests/test_report_compiler.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing tests/test_lineage_tracker.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing tests/test_calibration_ci.py: Syntax Error @ 46:8.\nparser error: error at 45:7: expected INDENT\n\n    warnings.warn(f\"Could not import all modules: {e}\")\n       ^",
    "Error processing tests/test_toc.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing tests/test_evidence_validation.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing tests/test_total_ordering.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\nimport unittest\n^",
    "Error processing tests/test_basic_functionality.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n#!/usr/bin/env python3\n^",
    "Error processing tests/test_code_quality_fixes_minimal.py: Syntax Error @ 1:1.\ntokenizer error: no matching outer block for dedent\n\n#!/usr/bin/env python3\n^",
    "Error processing tests/test_gw_alignment.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"Tests for Gromov-Wasserstein alignment functionality.\"\"\"\n^",
    "Error processing tests/test_retriever_determinism.py: Syntax Error @ 1:1.\ntokenizer error: no matching outer block for dedent\n\n\"\"\"\n^",
    "Error processing tests/test_code_quality_fixes.py: Syntax Error @ 1:1.\ntokenizer error: no matching outer block for dedent\n\n\"\"\"\n^",
    "Error processing tests/test_l_stage_preflight.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing tests/test_routing_contract.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\nimport unittest\n^",
    "Error processing tests/test_bmc.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing tests/test_stable_gw_aligner.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"Tests for stable EGW aligner.\"\"\"\n^",
    "Error processing tests/test_fixtures.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing tests/test_snapshot.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing tests/test_immutable_context_basic.py: Syntax Error @ 1:1.\ntokenizer error: no matching outer block for dedent\n\n#!/usr/bin/env python3\n^",
    "Error processing tests/test_l_stage_assertions.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing tests/test_api.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"Tests for main API functionality.\"\"\"\n^",
    "Error processing tests/test_constraint_validator.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing tests/test_deterministic_retrieval.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing tests/test_evidence_processor.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing tests/test_meso_aggregator.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing tests/test_answer_formatter.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing tests/test_snapshot_contract.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\nimport unittest\n^",
    "Error processing tests/test_deterministic_router_simple.py: Syntax Error @ 1:1.\ntokenizer error: no matching outer block for dedent\n\n#!/usr/bin/env python3\n^",
    "Error processing tests/test_ffc.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n#!/usr/bin/env python3\n^",
    "Error processing tests/test_conformal_risk_system.py: Syntax Error @ 1:1.\ntokenizer error: no matching outer block for dedent\n\n\"\"\"\n^",
    "Error processing tests/test_pic.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n\"\"\"\n^",
    "Error processing tests/integration/test_k_knowledge_extraction_workflow.py: Syntax Error @ 1:1.\ntokenizer error: no matching outer block for dedent\n\n#!/usr/bin/env python3\n^",
    "Error processing tests/evaluation/test_audit_logger.py: Syntax Error @ 1:1.\ntokenizer error: unmatched ')'\n\n#!/usr/bin/env python3\n^",
    "Error processing src/stages/K_knowledge_extraction/deterministic_embedder.py: Syntax Error @ 232:5.\nparser error: error at 231:4: expected INDENT\n\n        \"\"\"Generate embeddings for chunks with deterministic behavior\"\"\"\n    ^",
    "MyPy validation failed:\n\nFARFAN-ULTIMATE-main_15OjgmS7YYuHW2MP3qHfZ is not a valid Python package name\n",
    "Ruff validation failed:\nF401 [*] `statistics` imported but unused\n  --> canonical_flow/A_analysis_nlp/adaptive_analyzer.py:13:8\n   |\n11 | import json\n12 | import logging\n13 | import statistics\n   |        ^^^^^^^^^^\n14 | import os\n15 | # # # from collections import deque, OrderedDict  # Module not found  # Module not found  # Module not found\n   |\nhelp: Remove unused import: `statistics`\n\nF401 [*] `os` imported but unused\n  --> canonical_flow/A_analysis_nlp/adaptive_analyzer.py:14:8\n   |\n12 | import logging\n13 | import statistics\n14 | import os\n   |        ^^\n15 | # # # from collections import deque, OrderedDict  # Module not found  # Module not found  # Module not found\n16 | # # # from dataclasses import dataclass, field  # Module not found  # Module not found  # Module not found\n   |\nhelp: Remove unused import: `os`\n\nF821 Undefined name `Path`\n  --> canonical_flow/A_analysis_nlp/adaptive_analyzer.py:24:24\n   |\n23 | # Import total ordering base\n24 | sys.path.insert(0, str(Path(__file__).resolve().parents[2]))\n   |                        ^^^^\n25 | # # # from total_ordering_base import TotalOrderingBase, DeterministicCollectionMixin  # Module not found  # Module not found  # Modul\u2026\n   |\n\nF821 Undefined name `Enum`\n  --> canonical_flow/A_analysis_nlp/adaptive_analyzer.py:30:25\n   |\n30 | class AnalysisMode(str, Enum):\n   |                         ^^^^\n31 |     REACTIVE = \"reactive\"\n32 |     PREDICTIVE = \"predictive\"\n   |\n\nF821 Undefined name `Enum`\n  --> canonical_flow/A_analysis_nlp/adaptive_analyzer.py:37:24\n   |\n37 | class SystemState(str, Enum):\n   |                        ^^^^\n38 |     OPTIMAL = \"optimal\"\n39 |     STABLE = \"stable\"\n   |\n\nF821 Undefined name `Enum`\n  --> canonical_flow/A_analysis_nlp/adaptive_analyzer.py:46:29\n   |\n46 | class AdaptationAction(str, Enum):\n   |                             ^^^^\n47 |     SCALE_UP = \"scale_up\"\n48 |     SCALE_DOWN = \"scale_down\"\n   |\n\nF821 Undefined name `dataclass`\n  --> canonical_flow/A_analysis_nlp/adaptive_analyzer.py:59:2\n   |\n59 | @dataclass\n   |  ^^^^^^^^^\n60 | class AdaptationRecommendation:\n61 |     action: AdaptationAction\n   |\n\nF821 Undefined name `Dict`\n  --> canonical_flow/A_analysis_nlp/adaptive_analyzer.py:66:17\n   |\n64 |     confidence: float\n65 |     expected_impact: str\n66 |     parameters: Dict[str, Any] = field(default_factory=dict)\n   |                 ^^^^\n67 |     priority: int = 0\n68 |     estimated_execution_time: float = 0.0\n   |\n\nF821 Undefined name `Any`\n  --> canonical_flow/A_analysis_nlp/adaptive_analyzer.py:66:27\n   |\n64 |     confidence: float\n65 |     expected_impact: str\n66 |     parameters: Dict[str, Any] = field(default_factory=dict)\n   |                           ^^^\n67 |     priority: int = 0\n68 |     estimated_execution_time: float = 0.0\n   |\n\nF821 Undefined name `field`\n  --> canonical_flow/A_analysis_nlp/adaptive_analyzer.py:66:34\n   |\n64 |     confidence: float\n65 |     expected_impact: str\n66 |     parameters: Dict[str, Any] = field(default_factory=dict)\n   |                                  ^^^^^\n67 |     priority: int = 0\n68 |     estimated_execution_time: float = 0.0\n   |\n\nF821 Undefined name `Optional`\n  --> canonical_flow/A_analysis_nlp/adaptive_analyzer.py:69:25\n   |\n67 |     priority: int = 0\n68 |     estimated_execution_time: float = 0.0\n69 |     rollback_procedure: Optional[str] = None\n   |                         ^^^^^^^^\n70 |     \n71 |     def __post_init__(self):\n   |\n\nF821 Undefined name `OrderedDict`\n  --> canonical_flow/A_analysis_nlp/adaptive_analyzer.py:74:31\n   |\n72 |         # Ensure deterministic ordering of parameters\n73 |         if self.parameters:\n74 |             self.parameters = OrderedDict(sorted(self.parameters.items()))\n   |                               ^^^^^^^^^^^\n   |\n\nF821 Undefined name `TotalOrderingBase`\n  --> canonical_flow/A_analysis_nlp/adaptive_analyzer.py:77:24\n   |\n77 | class AdaptiveAnalyzer(TotalOrderingBase, DeterministicCollectionMixin):\n   |                        ^^^^^^^^^^^^^^^^^\n78 |     \"\"\"\n79 |     Adaptive Analyzer with deterministic processing, total ordering, and comprehensive audit logging.\n   |\n\nF821 Undefined name `DeterministicCollectionMixin`\n  --> canonical_flow/A_analysis_nlp/adaptive_analyzer.py:77:43\n   |\n77 | class AdaptiveAnalyzer(TotalOrderingBase, DeterministicCollectionMixin):\n   |                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n78 |     \"\"\"\n79 |     Adaptive Analyzer with deterministic processing, total ordering, and comprehensive audit logging.\n   |\n\nF821 Undefined name `Dict`\n  --> canonical_flow/A_analysis_nlp/adaptive_analyzer.py:89:22\n   |\n87 |         self.mode = mode\n88 |         self.stage_name = \"A_analysis_nlp\"  # Set stage name for audit logging\n89 |         self.models: Dict[str, Any] = {}\n   |                      ^^^^\n90 |         self.thresholds: Dict[str, Dict[str, float]] = {}\n91 |         self.adaptation_handlers: Dict[str, Callable] = {}\n   |\n\nF821 Undefined name `Any`\n  --> canonical_flow/A_analysis_nlp/adaptive_analyzer.py:89:32\n   |\n87 |         self.mode = mode\n88 |         self.stage_name = \"A_analysis_nlp\"  # Set stage name for audit logging\n89 |         self.models: Dict[str, Any] = {}\n   |                                ^^^\n90 |         self.thresholds: Dict[str, Dict[str, float]] = {}\n91 |         self.adaptation_handlers: Dict[str, Callable] = {}\n   |\n\nF821 Undefined name `Dict`\n  --> canonical_flow/A_analysis_nlp/adaptive_analyzer.py:90:26\n   |\n88 |         self.stage_name = \"A_analysis_nlp\"  # Set stage name for audit logging\n89 |         self.models: Dict[str, Any] = {}\n90 |         self.thresholds: Dict[str, Dict[str, float]] = {}\n   |                          ^^^^\n91 |         self.adaptation_handlers: Dict[str, Callable] = {}\n92 |         self.history: List[Dict[str, Any]] = []\n   |\n\nF821 Undefined name `Dict`\n  --> canonical_flow/A_analysis_nlp/adaptive_analyzer.py:90:36\n   |\n88 |         self.stage_name = \"A_analysis_nlp\"  # Set stage name for audit logging\n89 |         self.models: Dict[str, Any] = {}\n90 |         self.thresholds: Dict[str, Dict[str, float]] = {}\n   |                                    ^^^^\n91 |         self.adaptation_handlers: Dict[str, Callable] = {}\n92 |         self.history: List[Dict[str, Any]] = []\n   |\n\nF821 Undefined name `Dict`\n  --> canonical_flow/A_analysis_nlp/adaptive_analyzer.py:91:35\n   |\n89 |         self.models: Dict[str, Any] = {}\n90 |         self.thresholds: Dict[str, Dict[str, float]] = {}\n91 |         self.adaptation_handlers: Dict[str, Callable] = {}\n   |                                   ^^^^\n92 |         self.history: List[Dict[str, Any]] = []\n93 |         self.last_analysis: Optional[Dict[str, Any]] = None\n   |\n\nF821 Undefined name `Callable`\n  --> canonical_flow/A_analysis_nlp/adaptive_analyzer.py:91:45\n   |\n89 |         self.models: Dict[str, Any] = {}\n90 |         self.thresholds: Dict[str, Dict[str, float]] = {}\n91 |         self.adaptation_handlers: Dict[str, Callable] = {}\n   |                                             ^^^^^^^^\n92 |         self.history: List[Dict[str, Any]] = []\n93 |         self.last_analysis: Optional[Dict[str, Any]] = None\n   |\n\nF821 Undefined name `List`\n  --> canonical_flow/A_analysis_nlp/adaptive_analyzer.py:92:23\n   |\n90 |         self.thresholds: Dict[str, Dict[str, float]] = {}\n91 |         self.adaptation_handlers: Dict[str, Callable] = {}\n92 |         self.history: List[Dict[str, Any]] = []\n   |                       ^^^^\n93 |         self.last_analysis: Optional[Dict[str, Any]] = None\n   |\n\nF821 Undefined name `Dict`\n  --> canonical_flow/A_analysis_nlp/adaptive_analyzer.py:92:28\n   |\n90 |         self.thresholds: Dict[str, Dict[str, float]] = {}\n91 |         self.adaptation_handlers: Dict[str, Callable] = {}\n92 |         self.history: List[Dict[str, Any]] = []\n   |                            ^^^^\n93 |         self.last_analysis: Optional[Dict[str, Any]] = None\n   |\n\nF821 Undefined name `Any`\n  --> canonical_flow/A_analysis_nlp/adaptive_analyzer.py:92:38\n   |\n90 |         self.thresholds: Dict[str, Dict[str, float]] = {}\n91 |         self.adaptation_handlers: Dict[str, Callable] = {}\n92 |         self.history: List[Dict[str, Any]] = []\n   |                                      ^^^\n93 |         self.last_analysis: Optional[Dict[str, Any]] = None\n   |\n\nF821 Undefined name `Optional`\n  --> canonical_flow/A_analysis_nlp/adaptive_analyzer.py:93:29\n   |\n91 |         self.adaptation_handlers: Dict[str, Callable] = {}\n92 |         self.history: List[Dict[str, Any]] = []\n93 |         self.last_analysis: Optional[Dict[str, Any]] = None\n   |                             ^^^^^^^^\n94 |         \n95 |         # Configuration with deterministic defaults\n   |\n\nF821 Undefined name `Dict`\n  --> canonical_flow/A_analysis_nlp/adaptive_analyzer.py:93:38\n   |\n91 |         self.adaptation_handlers: Dict[str, Callable] = {}\n92 |         self.history: List[Dict[str, Any]] = []\n93 |         self.last_analysis: Optional[Dict[str, Any]] = None\n   |                                      ^^^^\n94 |         \n95 |         # Configuration with deterministic defaults\n   |\n\nF821 Undefined name `Any`\n  --> canonical_flow/A_analysis_nlp/adaptive_analyzer.py:93:48\n   |\n91 |         self.adaptation_handlers: Dict[str, Callable] = {}\n92 |         self.history: List[Dict[str, Any]] = []\n93 |         self.last_analysis: Optional[Dict[str, Any]] = None\n   |                                                ^^^\n94 |         \n95 |         # Configuration with deterministic defaults\n   |\n\nF821 Undefined name `Optional`\n   --> canonical_flow/A_analysis_nlp/adaptive_analyzer.py:103:30\n    |\n101 |         # Internal state\n102 |         self._running = False\n103 |         self._analysis_task: Optional[asyncio.Task] = None\n    |                              ^^^^^^^^\n104 |         \n105 |         self._setup_default_thresholds()\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/A_analysis_nlp/adaptive_analyzer.py:108:37\n    |\n106 |         self.update_state_hash(self._get_initial_state())\n107 |     \n108 |     def _get_initial_state(self) -> Dict[str, Any]:\n    |                                     ^^^^\n109 |         \"\"\"Get initial state for hash calculation\"\"\"\n110 |         return {\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/A_analysis_nlp/adaptive_analyzer.py:108:47\n    |\n106 |         self.update_state_hash(self._get_initial_state())\n107 |     \n108 |     def _get_initial_state(self) -> Dict[str, Any]:\n    |                                               ^^^\n109 |         \"\"\"Get initial state for hash calculation\"\"\"\n110 |         return {\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/A_analysis_nlp/adaptive_analyzer.py:133:29\n    |\n131 |         self.thresholds = self.sort_dict_by_keys(default_thresholds)\n132 |     \n133 |     def process(self, data: Any = None, context: Any = None) -> Dict[str, Any]:\n    |                             ^^^\n134 |         \"\"\"\n135 |         Main processing function with deterministic output and comprehensive audit logging.\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/A_analysis_nlp/adaptive_analyzer.py:133:50\n    |\n131 |         self.thresholds = self.sort_dict_by_keys(default_thresholds)\n132 |     \n133 |     def process(self, data: Any = None, context: Any = None) -> Dict[str, Any]:\n    |                                                  ^^^\n134 |         \"\"\"\n135 |         Main processing function with deterministic output and comprehensive audit logging.\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/A_analysis_nlp/adaptive_analyzer.py:133:65\n    |\n131 |         self.thresholds = self.sort_dict_by_keys(default_thresholds)\n132 |     \n133 |     def process(self, data: Any = None, context: Any = None) -> Dict[str, Any]:\n    |                                                                 ^^^^\n134 |         \"\"\"\n135 |         Main processing function with deterministic output and comprehensive audit logging.\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/A_analysis_nlp/adaptive_analyzer.py:133:75\n    |\n131 |         self.thresholds = self.sort_dict_by_keys(default_thresholds)\n132 |     \n133 |     def process(self, data: Any = None, context: Any = None) -> Dict[str, Any]:\n    |                                                                           ^^^\n134 |         \"\"\"\n135 |         Main processing function with deterministic output and comprehensive audit logging.\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/A_analysis_nlp/adaptive_analyzer.py:180:53\n    |\n178 |             return self.sort_dict_by_keys(error_output)\n179 |     \n180 |     def _perform_deterministic_analysis(self, data: Dict[str, Any], context: Dict[str, Any]) -> Dict[str, Any]:\n    |                                                     ^^^^\n181 |         \"\"\"Perform deterministic analysis with stable results\"\"\"\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/A_analysis_nlp/adaptive_analyzer.py:180:63\n    |\n178 |             return self.sort_dict_by_keys(error_output)\n179 |     \n180 |     def _perform_deterministic_analysis(self, data: Dict[str, Any], context: Dict[str, Any]) -> Dict[str, Any]:\n    |                                                               ^^^\n181 |         \"\"\"Perform deterministic analysis with stable results\"\"\"\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/A_analysis_nlp/adaptive_analyzer.py:180:78\n    |\n178 |             return self.sort_dict_by_keys(error_output)\n179 |     \n180 |     def _perform_deterministic_analysis(self, data: Dict[str, Any], context: Dict[str, Any]) -> Dict[str, Any]:\n    |                                                                              ^^^^\n181 |         \"\"\"Perform deterministic analysis with stable results\"\"\"\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/A_analysis_nlp/adaptive_analyzer.py:180:88\n    |\n178 |             return self.sort_dict_by_keys(error_output)\n179 |     \n180 |     def _perform_deterministic_analysis(self, data: Dict[str, Any], context: Dict[str, Any]) -> Dict[str, Any]:\n    |                                                                                        ^^^\n181 |         \"\"\"Perform deterministic analysis with stable results\"\"\"\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/A_analysis_nlp/adaptive_analyzer.py:180:97\n    |\n178 |             return self.sort_dict_by_keys(error_output)\n179 |     \n180 |     def _perform_deterministic_analysis(self, data: Dict[str, Any], context: Dict[str, Any]) -> Dict[str, Any]:\n    |                                                                                                 ^^^^\n181 |         \"\"\"Perform deterministic analysis with stable results\"\"\"\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/A_analysis_nlp/adaptive_analyzer.py:180:107\n    |\n178 |             return self.sort_dict_by_keys(error_output)\n179 |     \n180 |     def _perform_deterministic_analysis(self, data: Dict[str, Any], context: Dict[str, Any]) -> Dict[str, Any]:\n    |                                                                                                           ^^^\n181 |         \"\"\"Perform deterministic analysis with stable results\"\"\"\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/A_analysis_nlp/adaptive_analyzer.py:199:52\n    |\n197 |         }\n198 |     \n199 |     def _extract_metrics_deterministic(self, data: Dict[str, Any]) -> Dict[str, Any]:\n    |                                                    ^^^^\n200 |         \"\"\"Extract metrics with deterministic processing\"\"\"\n201 |         metrics = {}\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/A_analysis_nlp/adaptive_analyzer.py:199:62\n    |\n197 |         }\n198 |     \n199 |     def _extract_metrics_deterministic(self, data: Dict[str, Any]) -> Dict[str, Any]:\n    |                                                              ^^^\n200 |         \"\"\"Extract metrics with deterministic processing\"\"\"\n201 |         metrics = {}\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/A_analysis_nlp/adaptive_analyzer.py:199:71\n    |\n197 |         }\n198 |     \n199 |     def _extract_metrics_deterministic(self, data: Dict[str, Any]) -> Dict[str, Any]:\n    |                                                                       ^^^^\n200 |         \"\"\"Extract metrics with deterministic processing\"\"\"\n201 |         metrics = {}\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/A_analysis_nlp/adaptive_analyzer.py:199:81\n    |\n197 |         }\n198 |     \n199 |     def _extract_metrics_deterministic(self, data: Dict[str, Any]) -> Dict[str, Any]:\n    |                                                                                 ^^^\n200 |         \"\"\"Extract metrics with deterministic processing\"\"\"\n201 |         metrics = {}\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/A_analysis_nlp/adaptive_analyzer.py:227:60\n    |\n225 |         return self.sort_dict_by_keys(metrics)\n226 |     \n227 |     def _analyze_system_state_deterministic(self, metrics: Dict[str, Any]) -> Dict[str, Any]:\n    |                                                            ^^^^\n228 |         \"\"\"Analyze system state with deterministic logic\"\"\"\n229 |         state_scores = []\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/A_analysis_nlp/adaptive_analyzer.py:227:70\n    |\n225 |         return self.sort_dict_by_keys(metrics)\n226 |     \n227 |     def _analyze_system_state_deterministic(self, metrics: Dict[str, Any]) -> Dict[str, Any]:\n    |                                                                      ^^^\n228 |         \"\"\"Analyze system state with deterministic logic\"\"\"\n229 |         state_scores = []\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/A_analysis_nlp/adaptive_analyzer.py:227:79\n    |\n225 |         return self.sort_dict_by_keys(metrics)\n226 |     \n227 |     def _analyze_system_state_deterministic(self, metrics: Dict[str, Any]) -> Dict[str, Any]:\n    |                                                                               ^^^^\n228 |         \"\"\"Analyze system state with deterministic logic\"\"\"\n229 |         state_scores = []\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/A_analysis_nlp/adaptive_analyzer.py:227:89\n    |\n225 |         return self.sort_dict_by_keys(metrics)\n226 |     \n227 |     def _analyze_system_state_deterministic(self, metrics: Dict[str, Any]) -> Dict[str, Any]:\n    |                                                                                         ^^^\n228 |         \"\"\"Analyze system state with deterministic logic\"\"\"\n229 |         state_scores = []\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/A_analysis_nlp/adaptive_analyzer.py:255:65\n    |\n253 |         }\n254 |     \n255 |     def _calculate_metric_state(self, value: float, thresholds: Dict[str, float]) -> str:\n    |                                                                 ^^^^\n256 |         \"\"\"Calculate state for a metric value\"\"\"\n257 |         if \"critical\" in thresholds and value >= thresholds[\"critical\"]:\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/A_analysis_nlp/adaptive_analyzer.py:288:69\n    |\n286 |             return \"critical\"\n287 |     \n288 |     def _generate_recommendations_deterministic(self, system_state: Dict[str, Any], metrics: Dict[str, Any]) -> List[AdaptationRecomm\u2026\n    |                                                                     ^^^^\n289 |         \"\"\"Generate recommendations with deterministic ordering\"\"\"\n290 |         recommendations = []\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/A_analysis_nlp/adaptive_analyzer.py:288:79\n    |\n286 |             return \"critical\"\n287 |     \n288 |     def _generate_recommendations_deterministic(self, system_state: Dict[str, Any], metrics: Dict[str, Any]) -> List[AdaptationRecomm\u2026\n    |                                                                               ^^^\n289 |         \"\"\"Generate recommendations with deterministic ordering\"\"\"\n290 |         recommendations = []\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/A_analysis_nlp/adaptive_analyzer.py:288:94\n    |\n286 |             return \"critical\"\n287 |     \n288 |     def _generate_recommendations_deterministic(self, system_state: Dict[str, Any], metrics: Dict[str, Any]) -> List[AdaptationRecomm\u2026\n    |                                                                                              ^^^^\n289 |         \"\"\"Generate recommendations with deterministic ordering\"\"\"\n290 |         recommendations = []\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/A_analysis_nlp/adaptive_analyzer.py:288:104\n    |\n286 |             return \"critical\"\n287 |     \n288 |     def _generate_recommendations_deterministic(self, system_state: Dict[str, Any], metrics: Dict[str, Any]) -> List[AdaptationRecomm\u2026\n    |                                                                                                        ^^^\n289 |         \"\"\"Generate recommendations with deterministic ordering\"\"\"\n290 |         recommendations = []\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/A_analysis_nlp/adaptive_analyzer.py:288:113\n    |\n286 |             return \"critical\"\n287 |     \n288 |     def _generate_recommendations_deterministic(self, system_state: Dict[str, Any], metrics: Dict[str, Any]) -> List[AdaptationRecomm\u2026\n    |                                                                                                                 ^^^^\n289 |         \"\"\"Generate recommendations with deterministic ordering\"\"\"\n290 |         recommendations = []\n    |\n\nF841 Local variable `overall_state` is assigned to but never used\n   --> canonical_flow/A_analysis_nlp/adaptive_analyzer.py:292:9\n    |\n290 |         recommendations = []\n291 |         \n292 |         overall_state = system_state.get(\"overall_state\", \"unknown\")\n    |         ^^^^^^^^^^^^^\n293 |         component_states = system_state.get(\"component_states\", {})\n    |\nhelp: Remove assignment to unused variable `overall_state`\n\nF821 Undefined name `Dict`\n   --> canonical_flow/A_analysis_nlp/adaptive_analyzer.py:322:64\n    |\n320 |         return recommendations\n321 |     \n322 |     def _generate_deterministic_output(self, analysis_results: Dict[str, Any], operation_id: str) -> Dict[str, Any]:\n    |                                                                ^^^^\n323 |         \"\"\"Generate deterministic output structure\"\"\"\n324 |         output = {\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/A_analysis_nlp/adaptive_analyzer.py:322:74\n    |\n320 |         return recommendations\n321 |     \n322 |     def _generate_deterministic_output(self, analysis_results: Dict[str, Any], operation_id: str) -> Dict[str, Any]:\n    |                                                                          ^^^\n323 |         \"\"\"Generate deterministic output structure\"\"\"\n324 |         output = {\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/A_analysis_nlp/adaptive_analyzer.py:322:102\n    |\n320 |         return recommendations\n321 |     \n322 |     def _generate_deterministic_output(self, analysis_results: Dict[str, Any], operation_id: str) -> Dict[str, Any]:\n    |                                                                                                      ^^^^\n323 |         \"\"\"Generate deterministic output structure\"\"\"\n324 |         output = {\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/A_analysis_nlp/adaptive_analyzer.py:322:112\n    |\n320 |         return recommendations\n321 |     \n322 |     def _generate_deterministic_output(self, analysis_results: Dict[str, Any], operation_id: str) -> Dict[str, Any]:\n    |                                                                                                                ^^^\n323 |         \"\"\"Generate deterministic output structure\"\"\"\n324 |         output = {\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/A_analysis_nlp/adaptive_analyzer.py:337:35\n    |\n335 |         return self.sort_dict_by_keys(output)\n336 |     \n337 |     def save_artifact(self, data: Dict[str, Any], document_stem: str, output_dir: str = \"canonical_flow/analysis\") -> str:\n    |                                   ^^^^\n338 |         \"\"\"\n339 |         Save analysis output to canonical_flow/analysis/ directory with standardized naming.\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/A_analysis_nlp/adaptive_analyzer.py:337:45\n    |\n335 |         return self.sort_dict_by_keys(output)\n336 |     \n337 |     def save_artifact(self, data: Dict[str, Any], document_stem: str, output_dir: str = \"canonical_flow/analysis\") -> str:\n    |                                             ^^^\n338 |         \"\"\"\n339 |         Save analysis output to canonical_flow/analysis/ directory with standardized naming.\n    |\n\nF821 Undefined name `Path`\n   --> canonical_flow/A_analysis_nlp/adaptive_analyzer.py:351:13\n    |\n349 |         try:\n350 |             # Create output directory if it doesn't exist\n351 |             Path(output_dir).mkdir(parents=True, exist_ok=True)\n    |             ^^^^\n352 |             \n353 |             # Generate standardized filename\n    |\n\nF821 Undefined name `Path`\n   --> canonical_flow/A_analysis_nlp/adaptive_analyzer.py:355:29\n    |\n353 |             # Generate standardized filename\n354 |             filename = f\"{document_stem}_adaptive.json\"\n355 |             artifact_path = Path(output_dir) / filename\n    |                             ^^^^\n356 |             \n357 |             # Save with consistent JSON formatting\n    |\n\nF821 Undefined name `dataclass`\n  --> canonical_flow/A_analysis_nlp/decalogo_question_registry.py:27:2\n   |\n27 | @dataclass \n   |  ^^^^^^^^^\n28 | class DecalogoQuestion:\n29 | # # #     \"\"\"Single question from Dec\u00e1logo registry\"\"\"  # Module not found  # Module not found  # Module not found\n   |\n\nF821 Undefined name `Dict`\n  --> canonical_flow/A_analysis_nlp/decalogo_question_registry.py:34:15\n   |\n32 |     dimension_code: str\n33 |     question_text: str\n34 |     metadata: Dict[str, Any] = field(default_factory=dict)\n   |               ^^^^\n35 |     \n36 |     def __post_init__(self):\n   |\n\nF821 Undefined name `Any`\n  --> canonical_flow/A_analysis_nlp/decalogo_question_registry.py:34:25\n   |\n32 |     dimension_code: str\n33 |     question_text: str\n34 |     metadata: Dict[str, Any] = field(default_factory=dict)\n   |                         ^^^\n35 |     \n36 |     def __post_init__(self):\n   |\n\nF821 Undefined name `field`\n  --> canonical_flow/A_analysis_nlp/decalogo_question_registry.py:34:32\n   |\n32 |     dimension_code: str\n33 |     question_text: str\n34 |     metadata: Dict[str, Any] = field(default_factory=dict)\n   |                                ^^^^^\n35 |     \n36 |     def __post_init__(self):\n   |\n\nF821 Undefined name `dataclass`\n  --> canonical_flow/A_analysis_nlp/decalogo_question_registry.py:48:2\n   |\n48 | @dataclass\n   |  ^^^^^^^^^\n49 | class CoverageCell:\n50 |     \"\"\"Represents a single cell in the coverage matrix\"\"\"\n   |\n\nF821 Undefined name `List`\n  --> canonical_flow/A_analysis_nlp/decalogo_question_registry.py:57:16\n   |\n55 |     completion_percentage: float = 0.0\n56 |     has_gap: bool = True\n57 |     questions: List[str] = field(default_factory=list)\n   |                ^^^^\n58 |\n59 |     def __post_init__(self):\n   |\n\nF821 Undefined name `field`\n  --> canonical_flow/A_analysis_nlp/decalogo_question_registry.py:57:28\n   |\n55 |     completion_percentage: float = 0.0\n56 |     has_gap: bool = True\n57 |     questions: List[str] = field(default_factory=list)\n   |                            ^^^^^\n58 |\n59 |     def __post_init__(self):\n   |\n\nF821 Undefined name `dataclass`\n  --> canonical_flow/A_analysis_nlp/decalogo_question_registry.py:72:2\n   |\n72 | @dataclass\n   |  ^^^^^^^^^\n73 | class CoverageMatrix:\n74 |     \"\"\"10x4 coverage matrix for Dec\u00e1logo points vs DE dimensions\"\"\"\n   |\n\nF821 Undefined name `Dict`\n  --> canonical_flow/A_analysis_nlp/decalogo_question_registry.py:75:13\n   |\n73 | class CoverageMatrix:\n74 |     \"\"\"10x4 coverage matrix for Dec\u00e1logo points vs DE dimensions\"\"\"\n75 |     matrix: Dict[str, CoverageCell] = field(default_factory=dict)\n   |             ^^^^\n76 |     total_expected: int = 470  # 47 questions \u00d7 10 points\n77 |     total_actual: int = 0\n   |\n\nF821 Undefined name `field`\n  --> canonical_flow/A_analysis_nlp/decalogo_question_registry.py:75:39\n   |\n73 | class CoverageMatrix:\n74 |     \"\"\"10x4 coverage matrix for Dec\u00e1logo points vs DE dimensions\"\"\"\n75 |     matrix: Dict[str, CoverageCell] = field(default_factory=dict)\n   |                                       ^^^^^\n76 |     total_expected: int = 470  # 47 questions \u00d7 10 points\n77 |     total_actual: int = 0\n   |\n\nF821 Undefined name `List`\n   --> canonical_flow/A_analysis_nlp/decalogo_question_registry.py:114:27\n    |\n112 |                     self.total_actual += 1\n113 |     \n114 |     def get_gaps(self) -> List[Dict[str, Any]]:\n    |                           ^^^^\n115 |         \"\"\"Get all cells with coverage gaps\"\"\"\n116 |         gaps = []\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/A_analysis_nlp/decalogo_question_registry.py:114:32\n    |\n112 |                     self.total_actual += 1\n113 |     \n114 |     def get_gaps(self) -> List[Dict[str, Any]]:\n    |                                ^^^^\n115 |         \"\"\"Get all cells with coverage gaps\"\"\"\n116 |         gaps = []\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/A_analysis_nlp/decalogo_question_registry.py:114:42\n    |\n112 |                     self.total_actual += 1\n113 |     \n114 |     def get_gaps(self) -> List[Dict[str, Any]]:\n    |                                          ^^^\n115 |         \"\"\"Get all cells with coverage gaps\"\"\"\n116 |         gaps = []\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/A_analysis_nlp/decalogo_question_registry.py:130:26\n    |\n128 |         return sorted(gaps, key=lambda x: x[\"completion_percentage\"])\n129 |     \n130 |     def to_dict(self) -> Dict[str, Any]:\n    |                          ^^^^\n131 |         \"\"\"Convert matrix to serializable dictionary\"\"\"\n132 |         matrix_data = {}\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/A_analysis_nlp/decalogo_question_registry.py:130:36\n    |\n128 |         return sorted(gaps, key=lambda x: x[\"completion_percentage\"])\n129 |     \n130 |     def to_dict(self) -> Dict[str, Any]:\n    |                                    ^^^\n131 |         \"\"\"Convert matrix to serializable dictionary\"\"\"\n132 |         matrix_data = {}\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/A_analysis_nlp/decalogo_question_registry.py:187:43\n    |\n185 |             self.run_preflight_validation()\n186 |     \n187 |     def _build_complete_registry(self) -> List[DecalogoQuestion]:\n    |                                           ^^^^\n188 |         \"\"\"Build complete registry of 470 questions with exact distribution\"\"\"\n189 |         questions = []\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/A_analysis_nlp/decalogo_question_registry.py:324:43\n    |\n322 |         return questions\n323 |     \n324 |     def run_preflight_validation(self) -> Dict[str, Any]:\n    |                                           ^^^^\n325 |         \"\"\"\n326 |         Run comprehensive validation before any question evaluation begins.\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/A_analysis_nlp/decalogo_question_registry.py:324:53\n    |\n322 |         return questions\n323 |     \n324 |     def run_preflight_validation(self) -> Dict[str, Any]:\n    |                                                     ^^^\n325 |         \"\"\"\n326 |         Run comprehensive validation before any question evaluation begins.\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/A_analysis_nlp/decalogo_question_registry.py:441:82\n    |\n439 |         return validation_results\n440 |     \n441 |     def validate_coverage_completion(self, document_id: str, answered_questions: List[str]) -> Dict[str, Any]:\n    |                                                                                  ^^^^\n442 |         \"\"\"\n443 |         Validate coverage completion for a specific document.\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/A_analysis_nlp/decalogo_question_registry.py:441:96\n    |\n439 |         return validation_results\n440 |     \n441 |     def validate_coverage_completion(self, document_id: str, answered_questions: List[str]) -> Dict[str, Any]:\n    |                                                                                                ^^^^\n442 |         \"\"\"\n443 |         Validate coverage completion for a specific document.\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/A_analysis_nlp/decalogo_question_registry.py:441:106\n    |\n439 |         return validation_results\n440 |     \n441 |     def validate_coverage_completion(self, document_id: str, answered_questions: List[str]) -> Dict[str, Any]:\n    |                                                                                                          ^^^\n442 |         \"\"\"\n443 |         Validate coverage completion for a specific document.\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/A_analysis_nlp/decalogo_question_registry.py:463:81\n    |\n461 |         return coverage_matrix.to_dict()\n462 |     \n463 |     def generate_coverage_artifacts(self, document_id: str, answered_questions: List[str], \n    |                                                                                 ^^^^\n464 |                                   output_dir: Optional[str] = None) -> Dict[str, str]:\n465 |         \"\"\"\n    |\n\nF821 Undefined name `Optional`\n   --> canonical_flow/A_analysis_nlp/decalogo_question_registry.py:464:47\n    |\n463 |     def generate_coverage_artifacts(self, document_id: str, answered_questions: List[str], \n464 |                                   output_dir: Optional[str] = None) -> Dict[str, str]:\n    |                                               ^^^^^^^^\n465 |         \"\"\"\n466 |         Generate coverage_P{n}.json artifacts in canonical_flow/classification/<doc>/ directory.\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/A_analysis_nlp/decalogo_question_registry.py:464:72\n    |\n463 |     def generate_coverage_artifacts(self, document_id: str, answered_questions: List[str], \n464 |                                   output_dir: Optional[str] = None) -> Dict[str, str]:\n    |                                                                        ^^^^\n465 |         \"\"\"\n466 |         Generate coverage_P{n}.json artifacts in canonical_flow/classification/<doc>/ directory.\n    |\n\nF821 Undefined name `Path`\n   --> canonical_flow/A_analysis_nlp/decalogo_question_registry.py:473:9\n    |\n472 |         # Ensure output directory exists\n473 |         Path(output_dir).mkdir(parents=True, exist_ok=True)\n    |         ^^^^\n474 |         \n475 |         coverage_files = {}\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/A_analysis_nlp/decalogo_question_registry.py:571:36\n    |\n569 |         self.questions.sort(key=lambda q: (q.point_number, q.dimension_code, q.question_text))\n570 |     \n571 |     def get_all_questions(self) -> List[DecalogoQuestion]:\n    |                                    ^^^^\n572 |         \"\"\"Get all 470 questions in deterministic order\"\"\"\n573 |         return self.questions.copy()\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/A_analysis_nlp/decalogo_question_registry.py:575:60\n    |\n573 |         return self.questions.copy()\n574 |     \n575 |     def get_questions_by_point(self, point_number: int) -> List[DecalogoQuestion]:\n    |                                                            ^^^^\n576 |         \"\"\"Get all 47 questions for a specific point\"\"\"\n577 |         return [q for q in self.questions if q.point_number == point_number]\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/A_analysis_nlp/decalogo_question_registry.py:579:66\n    |\n577 |         return [q for q in self.questions if q.point_number == point_number]\n578 |     \n579 |     def get_questions_by_dimension(self, dimension_code: str) -> List[DecalogoQuestion]:\n    |                                                                  ^^^^\n580 |         \"\"\"Get all questions for a specific dimension\"\"\"\n581 |         return [q for q in self.questions if q.dimension_code == dimension_code]\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/A_analysis_nlp/decalogo_question_registry.py:583:41\n    |\n581 |         return [q for q in self.questions if q.dimension_code == dimension_code]\n582 |     \n583 |     def get_validation_summary(self) -> Dict[str, Any]:\n    |                                         ^^^^\n584 |         \"\"\"Get a summary of the registry validation status\"\"\"\n585 |         try:\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/A_analysis_nlp/decalogo_question_registry.py:583:51\n    |\n581 |         return [q for q in self.questions if q.dimension_code == dimension_code]\n582 |     \n583 |     def get_validation_summary(self) -> Dict[str, Any]:\n    |                                                   ^^^\n584 |         \"\"\"Get a summary of the registry validation status\"\"\"\n585 |         try:\n    |\n\nF821 Undefined name `Path`\n  --> canonical_flow/A_analysis_nlp/dnp_alignment_adapter.py:19:24\n   |\n18 | # Import total ordering base\n19 | sys.path.insert(0, str(Path(__file__).resolve().parents[2]))\n   |                        ^^^^\n20 | # # # from total_ordering_base import TotalOrderingBase, DeterministicCollectionMixin  # Module not found  # Module not found  # Modul\u2026\n   |\n\nF821 Undefined name `TotalOrderingBase`\n  --> canonical_flow/A_analysis_nlp/dnp_alignment_adapter.py:25:27\n   |\n25 | class DNPAlignmentAdapter(TotalOrderingBase, DeterministicCollectionMixin):\n   |                           ^^^^^^^^^^^^^^^^^\n26 |     \"\"\"\n27 |     DNP Alignment Adapter with deterministic processing and total ordering.\n   |\n\nF821 Undefined name `DeterministicCollectionMixin`\n  --> canonical_flow/A_analysis_nlp/dnp_alignment_adapter.py:25:46\n   |\n25 | class DNPAlignmentAdapter(TotalOrderingBase, DeterministicCollectionMixin):\n   |                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n26 |     \"\"\"\n27 |     DNP Alignment Adapter with deterministic processing and total ordering.\n   |\n\nF821 Undefined name `Tuple`\n  --> canonical_flow/A_analysis_nlp/dnp_alignment_adapter.py:55:38\n   |\n53 |         self.update_state_hash(self._get_initial_state())\n54 |     \n55 |     def _get_comparison_key(self) -> Tuple[str, ...]:\n   |                                      ^^^^^\n56 |         \"\"\"Return comparison key for deterministic ordering\"\"\"\n57 |         return (\n   |\n\nF821 Undefined name `Dict`\n  --> canonical_flow/A_analysis_nlp/dnp_alignment_adapter.py:65:37\n   |\n63 |         )\n64 |     \n65 |     def _get_initial_state(self) -> Dict[str, Any]:\n   |                                     ^^^^\n66 |         \"\"\"Get initial state for hash calculation\"\"\"\n67 |         return {\n   |\n\nF821 Undefined name `Any`\n  --> canonical_flow/A_analysis_nlp/dnp_alignment_adapter.py:65:47\n   |\n63 |         )\n64 |     \n65 |     def _get_initial_state(self) -> Dict[str, Any]:\n   |                                               ^^^\n66 |         \"\"\"Get initial state for hash calculation\"\"\"\n67 |         return {\n   |\n\nF821 Undefined name `Dict`\n  --> canonical_flow/A_analysis_nlp/dnp_alignment_adapter.py:73:44\n   |\n71 |         }\n72 |     \n73 |     def _initialize_dnp_standards(self) -> Dict[str, Dict[str, Any]]:\n   |                                            ^^^^\n74 |         \"\"\"Initialize DNP standards with deterministic ordering\"\"\"\n75 |         standards = {\n   |\n\nF821 Undefined name `Dict`\n  --> canonical_flow/A_analysis_nlp/dnp_alignment_adapter.py:73:54\n   |\n71 |         }\n72 |     \n73 |     def _initialize_dnp_standards(self) -> Dict[str, Dict[str, Any]]:\n   |                                                      ^^^^\n74 |         \"\"\"Initialize DNP standards with deterministic ordering\"\"\"\n75 |         standards = {\n   |\n\nF821 Undefined name `Any`\n  --> canonical_flow/A_analysis_nlp/dnp_alignment_adapter.py:73:64\n   |\n71 |         }\n72 |     \n73 |     def _initialize_dnp_standards(self) -> Dict[str, Dict[str, Any]]:\n   |                                                                ^^^\n74 |         \"\"\"Initialize DNP standards with deterministic ordering\"\"\"\n75 |         standards = {\n   |\n\nF821 Undefined name `Any`\n   --> canonical_flow/A_analysis_nlp/dnp_alignment_adapter.py:110:29\n    |\n108 |         return self.sort_dict_by_keys(standards)\n109 |     \n110 |     def process(self, data: Any = None, context: Any = None) -> Dict[str, Any]:\n    |                             ^^^\n111 |         \"\"\"\n112 |         Main processing function with deterministic output.\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/A_analysis_nlp/dnp_alignment_adapter.py:110:50\n    |\n108 |         return self.sort_dict_by_keys(standards)\n109 |     \n110 |     def process(self, data: Any = None, context: Any = None) -> Dict[str, Any]:\n    |                                                  ^^^\n111 |         \"\"\"\n112 |         Main processing function with deterministic output.\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/A_analysis_nlp/dnp_alignment_adapter.py:110:65\n    |\n108 |         return self.sort_dict_by_keys(standards)\n109 |     \n110 |     def process(self, data: Any = None, context: Any = None) -> Dict[str, Any]:\n    |                                                                 ^^^^\n111 |         \"\"\"\n112 |         Main processing function with deterministic output.\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/A_analysis_nlp/dnp_alignment_adapter.py:110:75\n    |\n108 |         return self.sort_dict_by_keys(standards)\n109 |     \n110 |     def process(self, data: Any = None, context: Any = None) -> Dict[str, Any]:\n    |                                                                           ^^^\n111 |         \"\"\"\n112 |         Main processing function with deterministic output.\n    |\n\nF841 Local variable `canonical_context` is assigned to but never used\n   --> canonical_flow/A_analysis_nlp/dnp_alignment_adapter.py:126:13\n    |\n124 |             # Canonicalize inputs\n125 |             canonical_data = self.canonicalize_data(data) if data else {}\n126 |             canonical_context = self.canonicalize_data(context) if context else {}\n    |             ^^^^^^^^^^^^^^^^^\n127 |             \n128 |             # Extract PDT and evaluation data\n    |\nhelp: Remove assignment to unused variable `canonical_context`\n\nF821 Undefined name `Any`\n   --> canonical_flow/A_analysis_nlp/dnp_alignment_adapter.py:169:60\n    |\n167 |             return self.sort_dict_by_keys(error_output)\n168 |     \n169 |     def _extract_pdt_and_eval_deterministic(self, payload: Any) -> tuple[Dict[str, Any], Dict[str, Any]]:\n    |                                                            ^^^\n170 | # # #         \"\"\"Best-effort extraction of PDT data and evaluation results from arbitrary payloads\"\"\"  # Module not found  # Module n\u2026\n171 |         pdt_data = {}\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/A_analysis_nlp/dnp_alignment_adapter.py:169:74\n    |\n167 |             return self.sort_dict_by_keys(error_output)\n168 |     \n169 |     def _extract_pdt_and_eval_deterministic(self, payload: Any) -> tuple[Dict[str, Any], Dict[str, Any]]:\n    |                                                                          ^^^^\n170 | # # #         \"\"\"Best-effort extraction of PDT data and evaluation results from arbitrary payloads\"\"\"  # Module not found  # Module n\u2026\n171 |         pdt_data = {}\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/A_analysis_nlp/dnp_alignment_adapter.py:169:84\n    |\n167 |             return self.sort_dict_by_keys(error_output)\n168 |     \n169 |     def _extract_pdt_and_eval_deterministic(self, payload: Any) -> tuple[Dict[str, Any], Dict[str, Any]]:\n    |                                                                                    ^^^\n170 | # # #         \"\"\"Best-effort extraction of PDT data and evaluation results from arbitrary payloads\"\"\"  # Module not found  # Module n\u2026\n171 |         pdt_data = {}\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/A_analysis_nlp/dnp_alignment_adapter.py:169:90\n    |\n167 |             return self.sort_dict_by_keys(error_output)\n168 |     \n169 |     def _extract_pdt_and_eval_deterministic(self, payload: Any) -> tuple[Dict[str, Any], Dict[str, Any]]:\n    |                                                                                          ^^^^\n170 | # # #         \"\"\"Best-effort extraction of PDT data and evaluation results from arbitrary payloads\"\"\"  # Module not found  # Module n\u2026\n171 |         pdt_data = {}\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/A_analysis_nlp/dnp_alignment_adapter.py:169:100\n    |\n167 |             return self.sort_dict_by_keys(error_output)\n168 |     \n169 |     def _extract_pdt_and_eval_deterministic(self, payload: Any) -> tuple[Dict[str, Any], Dict[str, Any]]:\n    |                                                                                                    ^^^\n170 | # # #         \"\"\"Best-effort extraction of PDT data and evaluation results from arbitrary payloads\"\"\"  # Module not found  # Module n\u2026\n171 |         pdt_data = {}\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/A_analysis_nlp/dnp_alignment_adapter.py:211:61\n    |\n209 |         return pdt_data, eval_results\n210 |     \n211 |     def _check_dnp_compliance_deterministic(self, pdt_data: Dict[str, Any], eval_results: Dict[str, Any]) -> Dict[str, Any]:\n    |                                                             ^^^^\n212 |         \"\"\"Check DNP compliance with deterministic scoring\"\"\"\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/A_analysis_nlp/dnp_alignment_adapter.py:211:71\n    |\n209 |         return pdt_data, eval_results\n210 |     \n211 |     def _check_dnp_compliance_deterministic(self, pdt_data: Dict[str, Any], eval_results: Dict[str, Any]) -> Dict[str, Any]:\n    |                                                                       ^^^\n212 |         \"\"\"Check DNP compliance with deterministic scoring\"\"\"\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/A_analysis_nlp/dnp_alignment_adapter.py:211:91\n    |\n209 |         return pdt_data, eval_results\n210 |     \n211 |     def _check_dnp_compliance_deterministic(self, pdt_data: Dict[str, Any], eval_results: Dict[str, Any]) -> Dict[str, Any]:\n    |                                                                                           ^^^^\n212 |         \"\"\"Check DNP compliance with deterministic scoring\"\"\"\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/A_analysis_nlp/dnp_alignment_adapter.py:211:101\n    |\n209 |         return pdt_data, eval_results\n210 |     \n211 |     def _check_dnp_compliance_deterministic(self, pdt_data: Dict[str, Any], eval_results: Dict[str, Any]) -> Dict[str, Any]:\n    |                                                                                                     ^^^\n212 |         \"\"\"Check DNP compliance with deterministic scoring\"\"\"\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/A_analysis_nlp/dnp_alignment_adapter.py:211:110\n    |\n209 |         return pdt_data, eval_results\n210 |     \n211 |     def _check_dnp_compliance_deterministic(self, pdt_data: Dict[str, Any], eval_results: Dict[str, Any]) -> Dict[str, Any]:\n    |                                                                                                              ^^^^\n212 |         \"\"\"Check DNP compliance with deterministic scoring\"\"\"\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/A_analysis_nlp/dnp_alignment_adapter.py:211:120\n    |\n209 |         return pdt_data, eval_results\n210 |     \n211 |     def _check_dnp_compliance_deterministic(self, pdt_data: Dict[str, Any], eval_results: Dict[str, Any]) -> Dict[str, Any]:\n    |                                                                                                                        ^^^\n212 |         \"\"\"Check DNP compliance with deterministic scoring\"\"\"\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/A_analysis_nlp/dnp_alignment_adapter.py:310:83\n    |\n308 |         return compliance_results\n309 |     \n310 |     def _calculate_standard_score_deterministic(self, text: str, standard_config: Dict[str, Any]) -> float:\n    |                                                                                   ^^^^\n311 |         \"\"\"Calculate compliance score for a DNP standard deterministically\"\"\"\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/A_analysis_nlp/dnp_alignment_adapter.py:310:93\n    |\n308 |         return compliance_results\n309 |     \n310 |     def _calculate_standard_score_deterministic(self, text: str, standard_config: Dict[str, Any]) -> float:\n    |                                                                                             ^^^\n311 |         \"\"\"Calculate compliance score for a DNP standard deterministically\"\"\"\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/A_analysis_nlp/dnp_alignment_adapter.py:355:75\n    |\n353 |         return max(0.0, min(1.0, combined_score))\n354 |     \n355 |     def _generate_recommendations_deterministic(self, compliance_results: Dict[str, Any]) -> list[str]:\n    |                                                                           ^^^^\n356 |         \"\"\"Generate recommendations based on compliance results\"\"\"\n357 |         recommendations = []\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/A_analysis_nlp/dnp_alignment_adapter.py:355:85\n    |\n353 |         return max(0.0, min(1.0, combined_score))\n354 |     \n355 |     def _generate_recommendations_deterministic(self, compliance_results: Dict[str, Any]) -> list[str]:\n    |                                                                                     ^^^\n356 |         \"\"\"Generate recommendations based on compliance results\"\"\"\n357 |         recommendations = []\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/A_analysis_nlp/dnp_alignment_adapter.py:391:70\n    |\n389 |         return sorted(list(set(recommendations)))  # Remove duplicates and sort\n390 |     \n391 |     def _generate_dnp_report_deterministic(self, compliance_results: Dict[str, Any]) -> Dict[str, Any]:\n    |                                                                      ^^^^\n392 |         \"\"\"Generate DNP compliance report with deterministic structure\"\"\"\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/A_analysis_nlp/dnp_alignment_adapter.py:391:80\n    |\n389 |         return sorted(list(set(recommendations)))  # Remove duplicates and sort\n390 |     \n391 |     def _generate_dnp_report_deterministic(self, compliance_results: Dict[str, Any]) -> Dict[str, Any]:\n    |                                                                                ^^^\n392 |         \"\"\"Generate DNP compliance report with deterministic structure\"\"\"\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/A_analysis_nlp/dnp_alignment_adapter.py:391:89\n    |\n389 |         return sorted(list(set(recommendations)))  # Remove duplicates and sort\n390 |     \n391 |     def _generate_dnp_report_deterministic(self, compliance_results: Dict[str, Any]) -> Dict[str, Any]:\n    |                                                                                         ^^^^\n392 |         \"\"\"Generate DNP compliance report with deterministic structure\"\"\"\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/A_analysis_nlp/dnp_alignment_adapter.py:391:99\n    |\n389 |         return sorted(list(set(recommendations)))  # Remove duplicates and sort\n390 |     \n391 |     def _generate_dnp_report_deterministic(self, compliance_results: Dict[str, Any]) -> Dict[str, Any]:\n    |                                                                                                   ^^^\n392 |         \"\"\"Generate DNP compliance report with deterministic structure\"\"\"\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/A_analysis_nlp/dnp_alignment_adapter.py:430:68\n    |\n428 |         return dnp_report\n429 |     \n430 |     def _create_enriched_output_deterministic(self, original_data: Dict[str, Any], compliance_results: Dict[str, Any], dnp_report: Di\u2026\n    |                                                                    ^^^^\n431 |         \"\"\"Create enriched output with DNP compliance data\"\"\"\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/A_analysis_nlp/dnp_alignment_adapter.py:430:78\n    |\n428 |         return dnp_report\n429 |     \n430 |     def _create_enriched_output_deterministic(self, original_data: Dict[str, Any], compliance_results: Dict[str, Any], dnp_report: Di\u2026\n    |                                                                              ^^^\n431 |         \"\"\"Create enriched output with DNP compliance data\"\"\"\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/A_analysis_nlp/dnp_alignment_adapter.py:430:104\n    |\n428 |         return dnp_report\n429 |     \n430 |     def _create_enriched_output_deterministic(self, original_data: Dict[str, Any], compliance_results: Dict[str, Any], dnp_report: Di\u2026\n    |                                                                                                        ^^^^\n431 |         \"\"\"Create enriched output with DNP compliance data\"\"\"\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/A_analysis_nlp/dnp_alignment_adapter.py:430:114\n    |\n428 |         return dnp_report\n429 |     \n430 |     def _create_enriched_output_deterministic(self, original_data: Dict[str, Any], compliance_results: Dict[str, Any], dnp_report: Di\u2026\n    |                                                                                                                  ^^^\n431 |         \"\"\"Create enriched output with DNP compliance data\"\"\"\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/A_analysis_nlp/dnp_alignment_adapter.py:430:132\n    |\n428 | \u2026\n429 | \u2026\n430 | \u2026Dict[str, Any], compliance_results: Dict[str, Any], dnp_report: Dict[str, Any], operation_id: str) -> Dict[str, Any]:\n    |                                                                  ^^^^\n431 | \u2026\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/A_analysis_nlp/dnp_alignment_adapter.py:430:142\n    |\n428 | \u2026\n429 | \u2026\n430 | \u2026Any], compliance_results: Dict[str, Any], dnp_report: Dict[str, Any], operation_id: str) -> Dict[str, Any]:\n    |                                                                  ^^^\n431 | \u2026\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/A_analysis_nlp/dnp_alignment_adapter.py:430:170\n    |\n428 | \u2026\n429 | \u2026\n430 | \u2026ct[str, Any], dnp_report: Dict[str, Any], operation_id: str) -> Dict[str, Any]:\n    |                                                                  ^^^^\n431 | \u2026\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/A_analysis_nlp/dnp_alignment_adapter.py:430:180\n    |\n428 | \u2026\n429 | \u2026\n430 | \u2026y], dnp_report: Dict[str, Any], operation_id: str) -> Dict[str, Any]:\n    |                                                                  ^^^\n431 | \u2026\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/A_analysis_nlp/dnp_alignment_adapter.py:455:60\n    |\n453 |         return self.sort_dict_by_keys(enriched_data)\n454 |     \n455 |     def _update_compliance_stats(self, compliance_results: Dict[str, Any]):\n    |                                                            ^^^^\n456 |         \"\"\"Update compliance statistics\"\"\"\n457 |         self.compliance_stats[\"documents_processed\"] += 1\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/A_analysis_nlp/dnp_alignment_adapter.py:455:70\n    |\n453 |         return self.sort_dict_by_keys(enriched_data)\n454 |     \n455 |     def _update_compliance_stats(self, compliance_results: Dict[str, Any]):\n    |                                                                      ^^^\n456 |         \"\"\"Update compliance statistics\"\"\"\n457 |         self.compliance_stats[\"documents_processed\"] += 1\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/A_analysis_nlp/dnp_alignment_adapter.py:474:35\n    |\n472 |         self.compliance_stats[\"last_update\"] = self._get_deterministic_timestamp()\n473 |     \n474 |     def save_artifact(self, data: Dict[str, Any], document_stem: str, output_dir: str = \"canonical_flow/analysis\") -> str:\n    |                                   ^^^^\n475 |         \"\"\"\n476 |         Save DNP alignment output to canonical_flow/analysis/ directory with standardized naming.\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/A_analysis_nlp/dnp_alignment_adapter.py:474:45\n    |\n472 |         self.compliance_stats[\"last_update\"] = self._get_deterministic_timestamp()\n473 |     \n474 |     def save_artifact(self, data: Dict[str, Any], document_stem: str, output_dir: str = \"canonical_flow/analysis\") -> str:\n    |                                             ^^^\n475 |         \"\"\"\n476 |         Save DNP alignment output to canonical_flow/analysis/ directory with standardized naming.\n    |\n\nF821 Undefined name `Path`\n   --> canonical_flow/A_analysis_nlp/dnp_alignment_adapter.py:488:13\n    |\n486 |         try:\n487 |             # Create output directory if it doesn't exist\n488 |             Path(output_dir).mkdir(parents=True, exist_ok=True)\n    |             ^^^^\n489 |             \n490 |             # Generate standardized filename\n    |\n\nF821 Undefined name `Path`\n   --> canonical_flow/A_analysis_nlp/dnp_alignment_adapter.py:492:29\n    |\n490 |             # Generate standardized filename\n491 |             filename = f\"{document_stem}_dnp.json\"\n492 |             artifact_path = Path(output_dir) / filename\n    |                             ^^^^\n493 |             \n494 |             # Save with consistent JSON formatting\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/A_analysis_nlp/dnp_alignment_adapter.py:508:32\n    |\n507 | # Backward compatibility functions\n508 | def check_dnp_compliance(data: Any) -> Dict[str, Any]:\n    |                                ^^^\n509 |     \"\"\"Check DNP compliance for given data\"\"\"\n510 |     adapter = DNPAlignmentAdapter()\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/A_analysis_nlp/dnp_alignment_adapter.py:508:40\n    |\n507 | # Backward compatibility functions\n508 | def check_dnp_compliance(data: Any) -> Dict[str, Any]:\n    |                                        ^^^^\n509 |     \"\"\"Check DNP compliance for given data\"\"\"\n510 |     adapter = DNPAlignmentAdapter()\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/A_analysis_nlp/dnp_alignment_adapter.py:508:50\n    |\n507 | # Backward compatibility functions\n508 | def check_dnp_compliance(data: Any) -> Dict[str, Any]:\n    |                                                  ^^^\n509 |     \"\"\"Check DNP compliance for given data\"\"\"\n510 |     adapter = DNPAlignmentAdapter()\n    |\n\ninvalid-syntax: Expected an indented block after `try` statement\n  --> canonical_flow/A_analysis_nlp/evaluation_driven_processor.py:33:1\n   |\n31 | try:\n32 | # # #     from intelligent_recommendation_engine import IntelligentRecommendationEngine  # Module not found  # Module not found  # Mod\u2026\n33 | except ImportError:\n   | ^^^^^^\n34 |     IntelligentRecommendationEngine = None\n   |\n\ninvalid-syntax: Expected an indented block after `try` statement\n  --> canonical_flow/A_analysis_nlp/evaluation_driven_processor.py:38:1\n   |\n36 | try:\n37 | # # #     from models import DocumentPackage, PDTContext, AdaptiveScoringResults, IntelligentRecommendations  # Module not found  # Mo\u2026\n38 | except ImportError:\n   | ^^^^^^\n39 |     DocumentPackage = None\n40 |     PDTContext = None\n   |\n\ninvalid-syntax: unindent does not match any outer indentation level\n  --> canonical_flow/A_analysis_nlp/evidence_processor.py:35:1\n   |\n33 |         EvidenceValidationRequest,\n34 |         EvidenceValidationResponse\n35 |     )\n   | ^^^^\n36 | except ImportError:\n37 |     EvidenceValidationModel = None\n   |\n\ninvalid-syntax: Expected a statement\n  --> canonical_flow/A_analysis_nlp/evidence_processor.py:35:5\n   |\n33 |         EvidenceValidationRequest,\n34 |         EvidenceValidationResponse\n35 |     )\n   |     ^\n36 | except ImportError:\n37 |     EvidenceValidationModel = None\n   |\n\ninvalid-syntax: Expected a statement\n  --> canonical_flow/A_analysis_nlp/evidence_processor.py:35:6\n   |\n33 |         EvidenceValidationRequest,\n34 |         EvidenceValidationResponse\n35 |     )\n   |      ^\n36 | except ImportError:\n37 |     EvidenceValidationModel = None\n   |\n\ninvalid-syntax: Expected a statement\n  --> canonical_flow/A_analysis_nlp/evidence_processor.py:36:1\n   |\n34 |         EvidenceValidationResponse\n35 |     )\n36 | except ImportError:\n   | ^^^^^^\n37 |     EvidenceValidationModel = None\n38 |     ValidationSeverity = None\n   |\n\ninvalid-syntax: Expected an expression\n  --> canonical_flow/A_analysis_nlp/evidence_processor.py:36:20\n   |\n34 |         EvidenceValidationResponse\n35 |     )\n36 | except ImportError:\n   |                    ^\n37 |     EvidenceValidationModel = None\n38 |     ValidationSeverity = None\n   |\n\ninvalid-syntax: Unexpected indentation\n  --> canonical_flow/A_analysis_nlp/evidence_processor.py:37:1\n   |\n35 |     )\n36 | except ImportError:\n37 |     EvidenceValidationModel = None\n   | ^^^^\n38 |     ValidationSeverity = None\n39 |     DNPEvidenceValidator = None\n   |\n\ninvalid-syntax: Expected `except` or `finally` after `try` block\n  --> canonical_flow/A_analysis_nlp/evidence_processor.py:44:1\n   |\n44 | class EvidenceType(Enum):\n   | ^^^^^\n45 |     \"\"\"Types of evidence that can be processed.\"\"\"\n46 |     DIRECT_QUOTE = \"direct_quote\"\n   |\n\ninvalid-syntax: Expected an indented block after `if` statement\n   --> canonical_flow/A_analysis_nlp/evidence_processor.py:129:9\n    |\n127 |         if self.metadata.url:\n128 | # # #             return f\"{author} ({year}). {title}. Retrieved from {self.metadata.url}\"  # Module not found  # Module not found  #\u2026\n129 |         else:\n    |         ^^^^\n130 |             return f\"{author} ({year}). {title}.\"\n    |\n\nF401 [*] `hashlib` imported but unused\n  --> canonical_flow/A_analysis_nlp/evidence_validation_model.py:10:8\n   |\n 8 | \"\"\"\n 9 |\n10 | import hashlib\n   |        ^^^^^^^\n11 | import hmac\n12 | import secrets\n   |\nhelp: Remove unused import: `hashlib`\n\nF401 [*] `hmac` imported but unused\n  --> canonical_flow/A_analysis_nlp/evidence_validation_model.py:11:8\n   |\n10 | import hashlib\n11 | import hmac\n   |        ^^^^\n12 | import secrets\n13 | import time\n   |\nhelp: Remove unused import: `hmac`\n\nF401 [*] `secrets` imported but unused\n  --> canonical_flow/A_analysis_nlp/evidence_validation_model.py:12:8\n   |\n10 | import hashlib\n11 | import hmac\n12 | import secrets\n   |        ^^^^^^^\n13 | import time\n14 | import sys\n   |\nhelp: Remove unused import: `secrets`\n\nF821 Undefined name `Path`\n  --> canonical_flow/A_analysis_nlp/evidence_validation_model.py:25:24\n   |\n24 | # Import total ordering base\n25 | sys.path.insert(0, str(Path(__file__).resolve().parents[2]))\n   |                        ^^^^\n26 | # # # from total_ordering_base import TotalOrderingBase, DeterministicCollectionMixin  # Module not found  # Module not found  # Modul\u2026\n   |\n\nF821 Undefined name `Enum`\n  --> canonical_flow/A_analysis_nlp/evidence_validation_model.py:55:31\n   |\n53 | logger = logging.getLogger(__name__)\n54 |\n55 | class ValidationSeverity(str, Enum):\n   |                               ^^^^\n56 |     \"\"\"Severity levels for validation issues\"\"\"\n57 |     CRITICAL = \"critical\"\n   |\n\nF821 Undefined name `Enum`\n  --> canonical_flow/A_analysis_nlp/evidence_validation_model.py:64:31\n   |\n64 | class ValidationCategory(str, Enum):\n   |                               ^^^^\n65 |     \"\"\"Categories of validation checks\"\"\"\n66 |     FACTUAL_ACCURACY = \"factual_accuracy\"\n   |\n\nF821 Undefined name `Enum`\n  --> canonical_flow/A_analysis_nlp/evidence_validation_model.py:74:33\n   |\n74 | class DNPAlignmentCategory(str, Enum):\n   |                                 ^^^^\n75 |     \"\"\"DNP alignment categories\"\"\"\n76 |     CONSTITUTIONAL = \"constitutional\"\n   |\n\nF821 Undefined name `TotalOrderingBase`\n  --> canonical_flow/A_analysis_nlp/evidence_validation_model.py:83:33\n   |\n83 | class EvidenceValidationRequest(TotalOrderingBase):\n   |                                 ^^^^^^^^^^^^^^^^^\n84 |     \"\"\"Request for evidence validation with deterministic processing\"\"\"\n   |\n\nF821 Undefined name `TotalOrderingBase`\n   --> canonical_flow/A_analysis_nlp/evidence_validation_model.py:106:34\n    |\n106 | class EvidenceValidationResponse(TotalOrderingBase):\n    |                                  ^^^^^^^^^^^^^^^^^\n107 | # # #     \"\"\"Response from evidence validation with deterministic processing\"\"\"  # Module not found  # Module not found  # Module not\u2026\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/A_analysis_nlp/evidence_validation_model.py:116:33\n    |\n114 |         self.validation_score = 0.0\n115 |         self.is_valid = False\n116 |         self.validation_issues: List[Dict[str, Any]] = []\n    |                                 ^^^^\n117 |         self.dnp_alignment_score = 0.0\n118 |         self.confidence_interval = (0.0, 1.0)\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/A_analysis_nlp/evidence_validation_model.py:116:38\n    |\n114 |         self.validation_score = 0.0\n115 |         self.is_valid = False\n116 |         self.validation_issues: List[Dict[str, Any]] = []\n    |                                      ^^^^\n117 |         self.dnp_alignment_score = 0.0\n118 |         self.confidence_interval = (0.0, 1.0)\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/A_analysis_nlp/evidence_validation_model.py:116:48\n    |\n114 |         self.validation_score = 0.0\n115 |         self.is_valid = False\n116 |         self.validation_issues: List[Dict[str, Any]] = []\n    |                                                ^^^\n117 |         self.dnp_alignment_score = 0.0\n118 |         self.confidence_interval = (0.0, 1.0)\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/A_analysis_nlp/evidence_validation_model.py:125:38\n    |\n123 |         self.update_state_hash(self._get_response_state())\n124 |     \n125 |     def _get_response_state(self) -> Dict[str, Any]:\n    |                                      ^^^^\n126 |         \"\"\"Get response state for hash calculation\"\"\"\n127 |         return {\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/A_analysis_nlp/evidence_validation_model.py:125:48\n    |\n123 |         self.update_state_hash(self._get_response_state())\n124 |     \n125 |     def _get_response_state(self) -> Dict[str, Any]:\n    |                                                ^^^\n126 |         \"\"\"Get response state for hash calculation\"\"\"\n127 |         return {\n    |\n\nF821 Undefined name `TotalOrderingBase`\n   --> canonical_flow/A_analysis_nlp/evidence_validation_model.py:136:31\n    |\n136 | class EvidenceValidationModel(TotalOrderingBase, DeterministicCollectionMixin):\n    |                               ^^^^^^^^^^^^^^^^^\n137 |     \"\"\"\n138 |     Evidence Validation Model with deterministic processing and total ordering.\n    |\n\nF821 Undefined name `DeterministicCollectionMixin`\n   --> canonical_flow/A_analysis_nlp/evidence_validation_model.py:136:50\n    |\n136 | class EvidenceValidationModel(TotalOrderingBase, DeterministicCollectionMixin):\n    |                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n137 |     \"\"\"\n138 |     Evidence Validation Model with deterministic processing and total ordering.\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/A_analysis_nlp/evidence_validation_model.py:167:37\n    |\n165 |         self.update_state_hash(self._get_initial_state())\n166 |     \n167 |     def _get_initial_state(self) -> Dict[str, Any]:\n    |                                     ^^^^\n168 |         \"\"\"Get initial state for hash calculation\"\"\"\n169 |         return {\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/A_analysis_nlp/evidence_validation_model.py:167:47\n    |\n165 |         self.update_state_hash(self._get_initial_state())\n166 |     \n167 |     def _get_initial_state(self) -> Dict[str, Any]:\n    |                                               ^^^\n168 |         \"\"\"Get initial state for hash calculation\"\"\"\n169 |         return {\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/A_analysis_nlp/evidence_validation_model.py:176:47\n    |\n174 |         }\n175 |     \n176 |     def _initialize_validation_rules(self) -> Dict[str, Dict[str, Any]]:\n    |                                               ^^^^\n177 |         \"\"\"Initialize validation rules with deterministic ordering\"\"\"\n178 |         rules = {\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/A_analysis_nlp/evidence_validation_model.py:176:57\n    |\n174 |         }\n175 |     \n176 |     def _initialize_validation_rules(self) -> Dict[str, Dict[str, Any]]:\n    |                                                         ^^^^\n177 |         \"\"\"Initialize validation rules with deterministic ordering\"\"\"\n178 |         rules = {\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/A_analysis_nlp/evidence_validation_model.py:176:67\n    |\n174 |         }\n175 |     \n176 |     def _initialize_validation_rules(self) -> Dict[str, Dict[str, Any]]:\n    |                                                                   ^^^\n177 |         \"\"\"Initialize validation rules with deterministic ordering\"\"\"\n178 |         rules = {\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/A_analysis_nlp/evidence_validation_model.py:219:43\n    |\n217 |         return self.sort_dict_by_keys(rules)\n218 |     \n219 |     def _initialize_dnp_criteria(self) -> Dict[str, Dict[str, Any]]:\n    |                                           ^^^^\n220 |         \"\"\"Initialize DNP alignment criteria with deterministic ordering\"\"\"\n221 |         criteria = {\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/A_analysis_nlp/evidence_validation_model.py:219:53\n    |\n217 |         return self.sort_dict_by_keys(rules)\n218 |     \n219 |     def _initialize_dnp_criteria(self) -> Dict[str, Dict[str, Any]]:\n    |                                                     ^^^^\n220 |         \"\"\"Initialize DNP alignment criteria with deterministic ordering\"\"\"\n221 |         criteria = {\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/A_analysis_nlp/evidence_validation_model.py:219:63\n    |\n217 |         return self.sort_dict_by_keys(rules)\n218 |     \n219 |     def _initialize_dnp_criteria(self) -> Dict[str, Dict[str, Any]]:\n    |                                                               ^^^\n220 |         \"\"\"Initialize DNP alignment criteria with deterministic ordering\"\"\"\n221 |         criteria = {\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/A_analysis_nlp/evidence_validation_model.py:319:93\n    |\n317 |         return response\n318 |     \n319 |     def _perform_validation_checks_deterministic(self, evidence_text: str, context: str) -> Dict[str, Any]:\n    |                                                                                             ^^^^\n320 |         \"\"\"Perform validation checks with deterministic results\"\"\"\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/A_analysis_nlp/evidence_validation_model.py:319:103\n    |\n317 |         return response\n318 |     \n319 |     def _perform_validation_checks_deterministic(self, evidence_text: str, context: str) -> Dict[str, Any]:\n    |                                                                                                       ^^^\n320 |         \"\"\"Perform validation checks with deterministic results\"\"\"\n    |\n\nF841 Local variable `context_lower` is assigned to but never used\n   --> canonical_flow/A_analysis_nlp/evidence_validation_model.py:323:9\n    |\n322 |         evidence_lower = evidence_text.lower()\n323 |         context_lower = context.lower()\n    |         ^^^^^^^^^^^^^\n324 |         \n325 |         validation_results = {\n    |\nhelp: Remove assignment to unused variable `context_lower`\n\nF821 Undefined name `List`\n   --> canonical_flow/A_analysis_nlp/evidence_validation_model.py:371:72\n    |\n369 |         return validation_results\n370 |     \n371 |     def _calculate_rule_score_deterministic(self, text: str, patterns: List[str]) -> float:\n    |                                                                        ^^^^\n372 |         \"\"\"Calculate score for a validation rule deterministically\"\"\"\n373 |         if not patterns:\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/A_analysis_nlp/evidence_validation_model.py:395:77\n    |\n393 |         return max(0.0, min(1.0, score))\n394 |     \n395 |     def _calculate_validation_score_deterministic(self, validation_results: Dict[str, Any]) -> float:\n    |                                                                             ^^^^\n396 |         \"\"\"Calculate overall validation score deterministically\"\"\"\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/A_analysis_nlp/evidence_validation_model.py:395:87\n    |\n393 |         return max(0.0, min(1.0, score))\n394 |     \n395 |     def _calculate_validation_score_deterministic(self, validation_results: Dict[str, Any]) -> float:\n    |                                                                                       ^^^\n396 |         \"\"\"Calculate overall validation score deterministically\"\"\"\n    |\n\nF821 Undefined name `Tuple`\n   --> canonical_flow/A_analysis_nlp/evidence_validation_model.py:450:95\n    |\n448 |         return max(0.0, min(1.0, dnp_score))\n449 |     \n450 |     def _calculate_confidence_interval_deterministic(self, score: float, text_length: int) -> Tuple[float, float]:\n    |                                                                                               ^^^^^\n451 |         \"\"\"Calculate confidence interval deterministically\"\"\"\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/A_analysis_nlp/evidence_validation_model.py:494:29\n    |\n492 |         self.validation_stats[\"avg_processing_time_ms\"] = new_avg_time\n493 |     \n494 |     def process(self, data: Any = None, context: Any = None) -> Dict[str, Any]:\n    |                             ^^^\n495 |         \"\"\"\n496 |         Main processing function with deterministic output.\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/A_analysis_nlp/evidence_validation_model.py:494:50\n    |\n492 |         self.validation_stats[\"avg_processing_time_ms\"] = new_avg_time\n493 |     \n494 |     def process(self, data: Any = None, context: Any = None) -> Dict[str, Any]:\n    |                                                  ^^^\n495 |         \"\"\"\n496 |         Main processing function with deterministic output.\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/A_analysis_nlp/evidence_validation_model.py:494:65\n    |\n492 |         self.validation_stats[\"avg_processing_time_ms\"] = new_avg_time\n493 |     \n494 |     def process(self, data: Any = None, context: Any = None) -> Dict[str, Any]:\n    |                                                                 ^^^^\n495 |         \"\"\"\n496 |         Main processing function with deterministic output.\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/A_analysis_nlp/evidence_validation_model.py:494:75\n    |\n492 |         self.validation_stats[\"avg_processing_time_ms\"] = new_avg_time\n493 |     \n494 |     def process(self, data: Any = None, context: Any = None) -> Dict[str, Any]:\n    |                                                                           ^^^\n495 |         \"\"\"\n496 |         Main processing function with deterministic output.\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/A_analysis_nlp/evidence_validation_model.py:551:142\n    |\n549 | \u2026\n550 | \u2026\n551 | \u2026st, response: EvidenceValidationResponse, operation_id: str) -> Dict[str, Any]:\n    |                                                                  ^^^^\n552 | \u2026\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/A_analysis_nlp/evidence_validation_model.py:551:152\n    |\n549 | \u2026\n550 | \u2026\n551 | \u2026se: EvidenceValidationResponse, operation_id: str) -> Dict[str, Any]:\n    |                                                                  ^^^\n552 | \u2026\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/A_analysis_nlp/evidence_validation_model.py:583:35\n    |\n581 |         return self.sort_dict_by_keys(output)\n582 |     \n583 |     def save_artifact(self, data: Dict[str, Any], document_stem: str, output_dir: str = \"canonical_flow/analysis\") -> str:\n    |                                   ^^^^\n584 |         \"\"\"\n585 |         Save evidence validation output to canonical_flow/analysis/ directory with standardized naming.\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/A_analysis_nlp/evidence_validation_model.py:583:45\n    |\n581 |         return self.sort_dict_by_keys(output)\n582 |     \n583 |     def save_artifact(self, data: Dict[str, Any], document_stem: str, output_dir: str = \"canonical_flow/analysis\") -> str:\n    |                                             ^^^\n584 |         \"\"\"\n585 |         Save evidence validation output to canonical_flow/analysis/ directory with standardized naming.\n    |\n\nF821 Undefined name `Path`\n   --> canonical_flow/A_analysis_nlp/evidence_validation_model.py:597:13\n    |\n595 |         try:\n596 |             # Create output directory if it doesn't exist\n597 |             Path(output_dir).mkdir(parents=True, exist_ok=True)\n    |             ^^^^\n598 |             \n599 |             # Generate standardized filename\n    |\n\nF821 Undefined name `Path`\n   --> canonical_flow/A_analysis_nlp/evidence_validation_model.py:601:29\n    |\n599 |             # Generate standardized filename\n600 |             filename = f\"{document_stem}_validation.json\"\n601 |             artifact_path = Path(output_dir) / filename\n    |                             ^^^^\n602 |             \n603 |             # Save with consistent JSON formatting\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/A_analysis_nlp/evidence_validation_model.py:638:61\n    |\n637 | # Backward compatibility functions\n638 | def validate_evidence_text(text: str, context: str = \"\") -> Dict[str, Any]:\n    |                                                             ^^^^\n639 |     \"\"\"Validate evidence text\"\"\"\n640 |     model = EvidenceValidationModel()\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/A_analysis_nlp/evidence_validation_model.py:638:71\n    |\n637 | # Backward compatibility functions\n638 | def validate_evidence_text(text: str, context: str = \"\") -> Dict[str, Any]:\n    |                                                                       ^^^\n639 |     \"\"\"Validate evidence text\"\"\"\n640 |     model = EvidenceValidationModel()\n    |\n\nF821 Undefined name `Path`\n  --> canonical_flow/A_analysis_nlp/extractor_evidencias_contextual.py:22:24\n   |\n21 | # Import total ordering base\n22 | sys.path.insert(0, str(Path(__file__).resolve().parents[2]))\n   |                        ^^^^\n23 | # # # from total_ordering_base import TotalOrderingBase, DeterministicCollectionMixin  # Module not found  # Module not found  # Modul\u2026\n   |\n\nF821 Undefined name `Enum`\n  --> canonical_flow/A_analysis_nlp/extractor_evidencias_contextual.py:28:25\n   |\n28 | class EvidenceRelevance(Enum):\n   |                         ^^^^\n29 |     \"\"\"Niveles de relevancia de evidencia\"\"\"\n30 |     ALTA = \"alta\"\n   |\n\nF821 Undefined name `Enum`\n  --> canonical_flow/A_analysis_nlp/extractor_evidencias_contextual.py:36:20\n   |\n36 | class EvidenceType(Enum):\n   |                    ^^^^\n37 |     \"\"\"Tipos de evidencia\"\"\"\n38 |     DIRECT_QUOTE = \"direct_quote\"\n   |\n\nF821 Undefined name `dataclass`\n  --> canonical_flow/A_analysis_nlp/extractor_evidencias_contextual.py:47:2\n   |\n47 | @dataclass\n   |  ^^^^^^^^^\n48 | class EvidenceContext:\n49 |     \"\"\"Contexto de evidencia extra\u00edda\"\"\"\n   |\n\nF821 Undefined name `Optional`\n  --> canonical_flow/A_analysis_nlp/extractor_evidencias_contextual.py:54:18\n   |\n52 |     text_after: str = \"\"\n53 |     section_header: str = \"\"\n54 |     page_number: Optional[int] = None\n   |                  ^^^^^^^^\n55 |     document_section: str = \"\"\n   |\n\nF821 Undefined name `dataclass`\n  --> canonical_flow/A_analysis_nlp/extractor_evidencias_contextual.py:62:2\n   |\n62 | @dataclass\n   |  ^^^^^^^^^\n63 | class ExtractedEvidence:\n64 |     \"\"\"Evidencia extra\u00edda con contexto completo\"\"\"\n   |\n\nF821 Undefined name `List`\n  --> canonical_flow/A_analysis_nlp/extractor_evidencias_contextual.py:71:15\n   |\n69 |     confidence: float\n70 |     context: EvidenceContext\n71 |     keywords: List[str] = field(default_factory=list)\n   |               ^^^^\n72 |     metadata: Dict[str, Any] = field(default_factory=dict)\n73 |     extraction_timestamp: str = \"\"\n   |\n\nF821 Undefined name `field`\n  --> canonical_flow/A_analysis_nlp/extractor_evidencias_contextual.py:71:27\n   |\n69 |     confidence: float\n70 |     context: EvidenceContext\n71 |     keywords: List[str] = field(default_factory=list)\n   |                           ^^^^^\n72 |     metadata: Dict[str, Any] = field(default_factory=dict)\n73 |     extraction_timestamp: str = \"\"\n   |\n\nF821 Undefined name `Dict`\n  --> canonical_flow/A_analysis_nlp/extractor_evidencias_contextual.py:72:15\n   |\n70 |     context: EvidenceContext\n71 |     keywords: List[str] = field(default_factory=list)\n72 |     metadata: Dict[str, Any] = field(default_factory=dict)\n   |               ^^^^\n73 |     extraction_timestamp: str = \"\"\n   |\n\nF821 Undefined name `Any`\n  --> canonical_flow/A_analysis_nlp/extractor_evidencias_contextual.py:72:25\n   |\n70 |     context: EvidenceContext\n71 |     keywords: List[str] = field(default_factory=list)\n72 |     metadata: Dict[str, Any] = field(default_factory=dict)\n   |                         ^^^\n73 |     extraction_timestamp: str = \"\"\n   |\n\nF821 Undefined name `field`\n  --> canonical_flow/A_analysis_nlp/extractor_evidencias_contextual.py:72:32\n   |\n70 |     context: EvidenceContext\n71 |     keywords: List[str] = field(default_factory=list)\n72 |     metadata: Dict[str, Any] = field(default_factory=dict)\n   |                                ^^^^^\n73 |     extraction_timestamp: str = \"\"\n   |\n\nF821 Undefined name `OrderedDict`\n  --> canonical_flow/A_analysis_nlp/extractor_evidencias_contextual.py:79:29\n   |\n77 |         self.keywords = sorted(self.keywords)\n78 |         if self.metadata:\n79 |             self.metadata = OrderedDict(sorted(self.metadata.items()))\n   |                             ^^^^^^^^^^^\n   |\n\nF821 Undefined name `TotalOrderingBase`\n  --> canonical_flow/A_analysis_nlp/extractor_evidencias_contextual.py:82:37\n   |\n82 | class ExtractorEvidenciasContextual(TotalOrderingBase, DeterministicCollectionMixin):\n   |                                     ^^^^^^^^^^^^^^^^^\n83 |     \"\"\"\n84 |     Extractor de Evidencias Contextual with deterministic processing and total ordering.\n   |\n\nF821 Undefined name `DeterministicCollectionMixin`\n  --> canonical_flow/A_analysis_nlp/extractor_evidencias_contextual.py:82:56\n   |\n82 | class ExtractorEvidenciasContextual(TotalOrderingBase, DeterministicCollectionMixin):\n   |                                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n83 |     \"\"\"\n84 |     Extractor de Evidencias Contextual with deterministic processing and total ordering.\n   |\n\nF821 Undefined name `Tuple`\n   --> canonical_flow/A_analysis_nlp/extractor_evidencias_contextual.py:112:38\n    |\n110 |         self.update_state_hash(self._get_initial_state())\n111 |     \n112 |     def _get_comparison_key(self) -> Tuple[str, ...]:\n    |                                      ^^^^^\n113 |         \"\"\"Return comparison key for deterministic ordering\"\"\"\n114 |         return (\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/A_analysis_nlp/extractor_evidencias_contextual.py:123:37\n    |\n121 |         )\n122 |     \n123 |     def _get_initial_state(self) -> Dict[str, Any]:\n    |                                     ^^^^\n124 |         \"\"\"Get initial state for hash calculation\"\"\"\n125 |         return {\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/A_analysis_nlp/extractor_evidencias_contextual.py:123:47\n    |\n121 |         )\n122 |     \n123 |     def _get_initial_state(self) -> Dict[str, Any]:\n    |                                               ^^^\n124 |         \"\"\"Get initial state for hash calculation\"\"\"\n125 |         return {\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/A_analysis_nlp/extractor_evidencias_contextual.py:132:48\n    |\n130 |         }\n131 |     \n132 |     def _initialize_evidence_patterns(self) -> Dict[str, List[str]]:\n    |                                                ^^^^\n133 |         \"\"\"Initialize evidence detection patterns with deterministic ordering\"\"\"\n134 |         patterns = {\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/A_analysis_nlp/extractor_evidencias_contextual.py:132:58\n    |\n130 |         }\n131 |     \n132 |     def _initialize_evidence_patterns(self) -> Dict[str, List[str]]:\n    |                                                          ^^^^\n133 |         \"\"\"Initialize evidence detection patterns with deterministic ordering\"\"\"\n134 |         patterns = {\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/A_analysis_nlp/extractor_evidencias_contextual.py:182:29\n    |\n180 |         return {k: sorted(v) for k, v in sorted(patterns.items())}\n181 |     \n182 |     def process(self, data: Any = None, context: Any = None) -> Dict[str, Any]:\n    |                             ^^^\n183 |         \"\"\"\n184 |         Main processing function with deterministic output.\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/A_analysis_nlp/extractor_evidencias_contextual.py:182:50\n    |\n180 |         return {k: sorted(v) for k, v in sorted(patterns.items())}\n181 |     \n182 |     def process(self, data: Any = None, context: Any = None) -> Dict[str, Any]:\n    |                                                  ^^^\n183 |         \"\"\"\n184 |         Main processing function with deterministic output.\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/A_analysis_nlp/extractor_evidencias_contextual.py:182:65\n    |\n180 |         return {k: sorted(v) for k, v in sorted(patterns.items())}\n181 |     \n182 |     def process(self, data: Any = None, context: Any = None) -> Dict[str, Any]:\n    |                                                                 ^^^^\n183 |         \"\"\"\n184 |         Main processing function with deterministic output.\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/A_analysis_nlp/extractor_evidencias_contextual.py:182:75\n    |\n180 |         return {k: sorted(v) for k, v in sorted(patterns.items())}\n181 |     \n182 |     def process(self, data: Any = None, context: Any = None) -> Dict[str, Any]:\n    |                                                                           ^^^\n183 |         \"\"\"\n184 |         Main processing function with deterministic output.\n    |\n\nF841 Local variable `canonical_context` is assigned to but never used\n   --> canonical_flow/A_analysis_nlp/extractor_evidencias_contextual.py:198:13\n    |\n196 |             # Canonicalize inputs\n197 |             canonical_data = self.canonicalize_data(data) if data else {}\n198 |             canonical_context = self.canonicalize_data(context) if context else {}\n    |             ^^^^^^^^^^^^^^^^^\n199 |             \n200 |             # Extract documents\n    |\nhelp: Remove assignment to unused variable `canonical_context`\n\nF821 Undefined name `Dict`\n   --> canonical_flow/A_analysis_nlp/extractor_evidencias_contextual.py:233:54\n    |\n231 |             return self.sort_dict_by_keys(error_output)\n232 |     \n233 |     def _extract_documents_deterministic(self, data: Dict[str, Any]) -> List[Dict[str, Any]]:\n    |                                                      ^^^^\n234 | # # #         \"\"\"Extract documents from input data with stable ordering\"\"\"  # Module not found  # Module not found  # Module not found\n235 |         documents = []\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/A_analysis_nlp/extractor_evidencias_contextual.py:233:64\n    |\n231 |             return self.sort_dict_by_keys(error_output)\n232 |     \n233 |     def _extract_documents_deterministic(self, data: Dict[str, Any]) -> List[Dict[str, Any]]:\n    |                                                                ^^^\n234 | # # #         \"\"\"Extract documents from input data with stable ordering\"\"\"  # Module not found  # Module not found  # Module not found\n235 |         documents = []\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/A_analysis_nlp/extractor_evidencias_contextual.py:233:73\n    |\n231 |             return self.sort_dict_by_keys(error_output)\n232 |     \n233 |     def _extract_documents_deterministic(self, data: Dict[str, Any]) -> List[Dict[str, Any]]:\n    |                                                                         ^^^^\n234 | # # #         \"\"\"Extract documents from input data with stable ordering\"\"\"  # Module not found  # Module not found  # Module not found\n235 |         documents = []\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/A_analysis_nlp/extractor_evidencias_contextual.py:233:78\n    |\n231 |             return self.sort_dict_by_keys(error_output)\n232 |     \n233 |     def _extract_documents_deterministic(self, data: Dict[str, Any]) -> List[Dict[str, Any]]:\n    |                                                                              ^^^^\n234 | # # #         \"\"\"Extract documents from input data with stable ordering\"\"\"  # Module not found  # Module not found  # Module not found\n235 |         documents = []\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/A_analysis_nlp/extractor_evidencias_contextual.py:233:88\n    |\n231 |             return self.sort_dict_by_keys(error_output)\n232 |     \n233 |     def _extract_documents_deterministic(self, data: Dict[str, Any]) -> List[Dict[str, Any]]:\n    |                                                                                        ^^^\n234 | # # #         \"\"\"Extract documents from input data with stable ordering\"\"\"  # Module not found  # Module not found  # Module not found\n235 |         documents = []\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/A_analysis_nlp/extractor_evidencias_contextual.py:287:72\n    |\n285 |         return documents\n286 |     \n287 |     def _extract_evidences_from_document_deterministic(self, document: Dict[str, Any]) -> List[ExtractedEvidence]:\n    |                                                                        ^^^^\n288 | # # #         \"\"\"Extract evidences from a single document with deterministic processing\"\"\"  # Module not found  # Module not found  #\u2026\n289 |         doc_id = document[\"id\"]\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/A_analysis_nlp/extractor_evidencias_contextual.py:287:82\n    |\n285 |         return documents\n286 |     \n287 |     def _extract_evidences_from_document_deterministic(self, document: Dict[str, Any]) -> List[ExtractedEvidence]:\n    |                                                                                  ^^^\n288 | # # #         \"\"\"Extract evidences from a single document with deterministic processing\"\"\"  # Module not found  # Module not found  #\u2026\n289 |         doc_id = document[\"id\"]\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/A_analysis_nlp/extractor_evidencias_contextual.py:287:91\n    |\n285 |         return documents\n286 |     \n287 |     def _extract_evidences_from_document_deterministic(self, document: Dict[str, Any]) -> List[ExtractedEvidence]:\n    |                                                                                           ^^^^\n288 | # # #         \"\"\"Extract evidences from a single document with deterministic processing\"\"\"  # Module not found  # Module not found  #\u2026\n289 |         doc_id = document[\"id\"]\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/A_analysis_nlp/extractor_evidencias_contextual.py:343:56\n    |\n341 |         return evidences\n342 |     \n343 |     def _split_text_into_sentences(self, text: str) -> List[str]:\n    |                                                        ^^^^\n344 |         \"\"\"Split text into sentences deterministically\"\"\"\n345 |         # Simple sentence splitting using regex\n    |\n\nF821 Undefined name `Tuple`\n   --> canonical_flow/A_analysis_nlp/extractor_evidencias_contextual.py:357:64\n    |\n355 |         return cleaned_sentences\n356 |     \n357 |     def _analyze_sentence_for_evidence(self, sentence: str) -> Tuple[EvidenceType, EvidenceRelevance, float]:\n    |                                                                ^^^^^\n358 |         \"\"\"Analyze sentence to determine evidence type, relevance, and confidence\"\"\"\n359 |         sentence_lower = sentence.lower()\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/A_analysis_nlp/extractor_evidencias_contextual.py:399:52\n    |\n397 |         return evidence_type, relevance, confidence\n398 |     \n399 |     def _extract_sentence_context(self, sentences: List[str], sentence_index: int, doc_id: str) -> EvidenceContext:\n    |                                                    ^^^^\n400 |         \"\"\"Extract context around a sentence\"\"\"\n401 |         context_before = \"\"\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/A_analysis_nlp/extractor_evidencias_contextual.py:432:61\n    |\n430 |         )\n431 |     \n432 |     def _extract_keywords_deterministic(self, text: str) -> List[str]:\n    |                                                             ^^^^\n433 | # # #         \"\"\"Extract keywords from text deterministically\"\"\"  # Module not found  # Module not found  # Module not found\n434 |         # Simple keyword extraction using word frequency\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/A_analysis_nlp/extractor_evidencias_contextual.py:500:67\n    |\n498 |         return [word for word, freq in top_keywords]\n499 |     \n500 |     def _filter_and_rank_evidences_deterministic(self, evidences: List[ExtractedEvidence]) -> List[ExtractedEvidence]:\n    |                                                                   ^^^^\n501 |         \"\"\"Filter and rank evidences with deterministic ordering\"\"\"\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/A_analysis_nlp/extractor_evidencias_contextual.py:500:95\n    |\n498 |         return [word for word, freq in top_keywords]\n499 |     \n500 |     def _filter_and_rank_evidences_deterministic(self, evidences: List[ExtractedEvidence]) -> List[ExtractedEvidence]:\n    |                                                                                               ^^^^\n501 |         \"\"\"Filter and rank evidences with deterministic ordering\"\"\"\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/A_analysis_nlp/extractor_evidencias_contextual.py:536:51\n    |\n534 |         return relevance_map.get(relevance, 0.0)\n535 |     \n536 |     def _update_extraction_stats(self, evidences: List[ExtractedEvidence]):\n    |                                                   ^^^^\n537 |         \"\"\"Update extraction statistics\"\"\"\n538 |         self.extraction_stats[\"documents_processed\"] += 1\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/A_analysis_nlp/extractor_evidencias_contextual.py:549:57\n    |\n547 |                 self.extraction_stats[\"low_relevance_count\"] += 1\n548 |     \n549 |     def _generate_deterministic_output(self, evidences: List[ExtractedEvidence], operation_id: str) -> Dict[str, Any]:\n    |                                                         ^^^^\n550 |         \"\"\"Generate deterministic output structure\"\"\"\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/A_analysis_nlp/extractor_evidencias_contextual.py:549:104\n    |\n547 |                 self.extraction_stats[\"low_relevance_count\"] += 1\n548 |     \n549 |     def _generate_deterministic_output(self, evidences: List[ExtractedEvidence], operation_id: str) -> Dict[str, Any]:\n    |                                                                                                        ^^^^\n550 |         \"\"\"Generate deterministic output structure\"\"\"\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/A_analysis_nlp/extractor_evidencias_contextual.py:549:114\n    |\n547 |                 self.extraction_stats[\"low_relevance_count\"] += 1\n548 |     \n549 |     def _generate_deterministic_output(self, evidences: List[ExtractedEvidence], operation_id: str) -> Dict[str, Any]:\n    |                                                                                                                  ^^^\n550 |         \"\"\"Generate deterministic output structure\"\"\"\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/A_analysis_nlp/extractor_evidencias_contextual.py:618:35\n    |\n616 |         return self.sort_dict_by_keys(output)\n617 |     \n618 |     def save_artifact(self, data: Dict[str, Any], document_stem: str, output_dir: str = \"canonical_flow/analysis\") -> str:\n    |                                   ^^^^\n619 |         \"\"\"\n620 |         Save extractor evidencias output to canonical_flow/analysis/ directory with standardized naming.\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/A_analysis_nlp/extractor_evidencias_contextual.py:618:45\n    |\n616 |         return self.sort_dict_by_keys(output)\n617 |     \n618 |     def save_artifact(self, data: Dict[str, Any], document_stem: str, output_dir: str = \"canonical_flow/analysis\") -> str:\n    |                                             ^^^\n619 |         \"\"\"\n620 |         Save extractor evidencias output to canonical_flow/analysis/ directory with standardized naming.\n    |\n\nF821 Undefined name `Path`\n   --> canonical_flow/A_analysis_nlp/extractor_evidencias_contextual.py:632:13\n    |\n630 |         try:\n631 |             # Create output directory if it doesn't exist\n632 |             Path(output_dir).mkdir(parents=True, exist_ok=True)\n    |             ^^^^\n633 |             \n634 |             # Generate standardized filename\n    |\n\nF821 Undefined name `Path`\n   --> canonical_flow/A_analysis_nlp/extractor_evidencias_contextual.py:636:29\n    |\n634 |             # Generate standardized filename\n635 |             filename = f\"{document_stem}_extractor.json\"\n636 |             artifact_path = Path(output_dir) / filename\n    |                             ^^^^\n637 |             \n638 |             # Save with consistent JSON formatting\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/A_analysis_nlp/extractor_evidencias_contextual.py:652:62\n    |\n651 | # Backward compatibility functions\n652 | def extract_evidences(text: str, document_id: str = None) -> Dict[str, Any]:\n    |                                                              ^^^^\n653 | # # #     \"\"\"Extract evidences from text\"\"\"  # Module not found  # Module not found  # Module not found\n654 |     extractor = ExtractorEvidenciasContextual()\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/A_analysis_nlp/extractor_evidencias_contextual.py:652:72\n    |\n651 | # Backward compatibility functions\n652 | def extract_evidences(text: str, document_id: str = None) -> Dict[str, Any]:\n    |                                                                        ^^^\n653 | # # #     \"\"\"Extract evidences from text\"\"\"  # Module not found  # Module not found  # Module not found\n654 |     extractor = ExtractorEvidenciasContextual()\n    |\n\nF821 Undefined name `Path`\n  --> canonical_flow/A_analysis_nlp/implementacion_mapeo.py:19:24\n   |\n18 | # Import total ordering base\n19 | sys.path.insert(0, str(Path(__file__).resolve().parents[2]))\n   |                        ^^^^\n20 | # # # from total_ordering_base import TotalOrderingBase, DeterministicCollectionMixin  # Module not found  # Module not found  # Modul\u2026\n   |\n\nF821 Undefined name `dataclass`\n  --> canonical_flow/A_analysis_nlp/implementacion_mapeo.py:25:2\n   |\n25 | @dataclass\n   |  ^^^^^^^^^\n26 | class QuestionMapping:\n27 |     \"\"\"Estructura de datos para mapeo de preguntas\"\"\"\n   |\n\nF821 Undefined name `List`\n  --> canonical_flow/A_analysis_nlp/implementacion_mapeo.py:37:15\n   |\n35 |     sector: str\n36 |     weight: float\n37 |     keywords: List[str] = field(default_factory=list)\n   |               ^^^^\n38 |     search_patterns: List[str] = field(default_factory=list)\n   |\n\nF821 Undefined name `field`\n  --> canonical_flow/A_analysis_nlp/implementacion_mapeo.py:37:27\n   |\n35 |     sector: str\n36 |     weight: float\n37 |     keywords: List[str] = field(default_factory=list)\n   |                           ^^^^^\n38 |     search_patterns: List[str] = field(default_factory=list)\n   |\n\nF821 Undefined name `List`\n  --> canonical_flow/A_analysis_nlp/implementacion_mapeo.py:38:22\n   |\n36 |     weight: float\n37 |     keywords: List[str] = field(default_factory=list)\n38 |     search_patterns: List[str] = field(default_factory=list)\n   |                      ^^^^\n39 |     \n40 |     def __post_init__(self):\n   |\n\nF821 Undefined name `field`\n  --> canonical_flow/A_analysis_nlp/implementacion_mapeo.py:38:34\n   |\n36 |     weight: float\n37 |     keywords: List[str] = field(default_factory=list)\n38 |     search_patterns: List[str] = field(default_factory=list)\n   |                                  ^^^^^\n39 |     \n40 |     def __post_init__(self):\n   |\n\nF821 Undefined name `TotalOrderingBase`\n  --> canonical_flow/A_analysis_nlp/implementacion_mapeo.py:46:30\n   |\n46 | class QuestionDecalogoMapper(TotalOrderingBase, DeterministicCollectionMixin):\n   |                              ^^^^^^^^^^^^^^^^^\n47 |     \"\"\"\n48 |     Question Dec\u00e1logo Mapper with deterministic processing and total ordering.\n   |\n\nF821 Undefined name `DeterministicCollectionMixin`\n  --> canonical_flow/A_analysis_nlp/implementacion_mapeo.py:46:49\n   |\n46 | class QuestionDecalogoMapper(TotalOrderingBase, DeterministicCollectionMixin):\n   |                                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n47 |     \"\"\"\n48 |     Question Dec\u00e1logo Mapper with deterministic processing and total ordering.\n   |\n\nF821 Undefined name `Dict`\n  --> canonical_flow/A_analysis_nlp/implementacion_mapeo.py:65:37\n   |\n63 |         self.update_state_hash(self._get_initial_state())\n64 |     \n65 |     def _get_initial_state(self) -> Dict[str, Any]:\n   |                                     ^^^^\n66 |         \"\"\"Get initial state for hash calculation\"\"\"\n67 |         return {\n   |\n\nF821 Undefined name `Any`\n  --> canonical_flow/A_analysis_nlp/implementacion_mapeo.py:65:47\n   |\n63 |         self.update_state_hash(self._get_initial_state())\n64 |     \n65 |     def _get_initial_state(self) -> Dict[str, Any]:\n   |                                               ^^^\n66 |         \"\"\"Get initial state for hash calculation\"\"\"\n67 |         return {\n   |\n\nF821 Undefined name `Dict`\n  --> canonical_flow/A_analysis_nlp/implementacion_mapeo.py:73:42\n   |\n71 |         }\n72 |     \n73 |     def _define_decalogo_points(self) -> Dict[int, Dict[str, Any]]:\n   |                                          ^^^^\n74 |         \"\"\"Definir los 11 puntos del Dec\u00e1logo DDHH con orden determin\u00edstico\"\"\"\n75 |         points = {\n   |\n\nF821 Undefined name `Dict`\n  --> canonical_flow/A_analysis_nlp/implementacion_mapeo.py:73:52\n   |\n71 |         }\n72 |     \n73 |     def _define_decalogo_points(self) -> Dict[int, Dict[str, Any]]:\n   |                                                    ^^^^\n74 |         \"\"\"Definir los 11 puntos del Dec\u00e1logo DDHH con orden determin\u00edstico\"\"\"\n75 |         points = {\n   |\n\nF821 Undefined name `Any`\n  --> canonical_flow/A_analysis_nlp/implementacion_mapeo.py:73:62\n   |\n71 |         }\n72 |     \n73 |     def _define_decalogo_points(self) -> Dict[int, Dict[str, Any]]:\n   |                                                              ^^^\n74 |         \"\"\"Definir los 11 puntos del Dec\u00e1logo DDHH con orden determin\u00edstico\"\"\"\n75 |         points = {\n   |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/A_analysis_nlp/implementacion_mapeo.py:158:44\n    |\n156 |         return {k: self.sort_dict_by_keys(v) for k, v in sorted(points.items())}\n157 |     \n158 |     def _define_value_chain_links(self) -> Dict[str, Dict[str, Any]]:\n    |                                            ^^^^\n159 |         \"\"\"Definir eslabones de la cadena de valor del DNP con orden determin\u00edstico\"\"\"\n160 |         links = {\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/A_analysis_nlp/implementacion_mapeo.py:158:54\n    |\n156 |         return {k: self.sort_dict_by_keys(v) for k, v in sorted(points.items())}\n157 |     \n158 |     def _define_value_chain_links(self) -> Dict[str, Dict[str, Any]]:\n    |                                                      ^^^^\n159 |         \"\"\"Definir eslabones de la cadena de valor del DNP con orden determin\u00edstico\"\"\"\n160 |         links = {\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/A_analysis_nlp/implementacion_mapeo.py:158:64\n    |\n156 |         return {k: self.sort_dict_by_keys(v) for k, v in sorted(points.items())}\n157 |     \n158 |     def _define_value_chain_links(self) -> Dict[str, Dict[str, Any]]:\n    |                                                                ^^^\n159 |         \"\"\"Definir eslabones de la cadena de valor del DNP con orden determin\u00edstico\"\"\"\n160 |         links = {\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/A_analysis_nlp/implementacion_mapeo.py:206:42\n    |\n204 |         return {k: self.sort_dict_by_keys(v) for k, v in sorted(links.items())}\n205 |     \n206 |     def _build_complete_mapping(self) -> Dict[str, QuestionMapping]:\n    |                                          ^^^^\n207 |         \"\"\"Construir el mapeo completo con orden determin\u00edstico\"\"\"\n208 |         mapping = {}\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/A_analysis_nlp/implementacion_mapeo.py:271:29\n    |\n269 |         return self.sort_dict_by_keys(mapping)\n270 |     \n271 |     def process(self, data: Any = None, context: Any = None) -> Dict[str, Any]:\n    |                             ^^^\n272 |         \"\"\"\n273 |         Main processing function with deterministic output.\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/A_analysis_nlp/implementacion_mapeo.py:271:50\n    |\n269 |         return self.sort_dict_by_keys(mapping)\n270 |     \n271 |     def process(self, data: Any = None, context: Any = None) -> Dict[str, Any]:\n    |                                                  ^^^\n272 |         \"\"\"\n273 |         Main processing function with deterministic output.\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/A_analysis_nlp/implementacion_mapeo.py:271:65\n    |\n269 |         return self.sort_dict_by_keys(mapping)\n270 |     \n271 |     def process(self, data: Any = None, context: Any = None) -> Dict[str, Any]:\n    |                                                                 ^^^^\n272 |         \"\"\"\n273 |         Main processing function with deterministic output.\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/A_analysis_nlp/implementacion_mapeo.py:271:75\n    |\n269 |         return self.sort_dict_by_keys(mapping)\n270 |     \n271 |     def process(self, data: Any = None, context: Any = None) -> Dict[str, Any]:\n    |                                                                           ^^^\n272 |         \"\"\"\n273 |         Main processing function with deterministic output.\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/A_analysis_nlp/implementacion_mapeo.py:310:52\n    |\n308 |             return self.sort_dict_by_keys(error_output)\n309 |     \n310 |     def _perform_deterministic_mapping(self, data: Dict[str, Any], context: Dict[str, Any]) -> Dict[str, Any]:\n    |                                                    ^^^^\n311 |         \"\"\"Perform deterministic mapping analysis\"\"\"\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/A_analysis_nlp/implementacion_mapeo.py:310:62\n    |\n308 |             return self.sort_dict_by_keys(error_output)\n309 |     \n310 |     def _perform_deterministic_mapping(self, data: Dict[str, Any], context: Dict[str, Any]) -> Dict[str, Any]:\n    |                                                              ^^^\n311 |         \"\"\"Perform deterministic mapping analysis\"\"\"\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/A_analysis_nlp/implementacion_mapeo.py:310:77\n    |\n308 |             return self.sort_dict_by_keys(error_output)\n309 |     \n310 |     def _perform_deterministic_mapping(self, data: Dict[str, Any], context: Dict[str, Any]) -> Dict[str, Any]:\n    |                                                                             ^^^^\n311 |         \"\"\"Perform deterministic mapping analysis\"\"\"\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/A_analysis_nlp/implementacion_mapeo.py:310:87\n    |\n308 |             return self.sort_dict_by_keys(error_output)\n309 |     \n310 |     def _perform_deterministic_mapping(self, data: Dict[str, Any], context: Dict[str, Any]) -> Dict[str, Any]:\n    |                                                                                       ^^^\n311 |         \"\"\"Perform deterministic mapping analysis\"\"\"\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/A_analysis_nlp/implementacion_mapeo.py:310:96\n    |\n308 |             return self.sort_dict_by_keys(error_output)\n309 |     \n310 |     def _perform_deterministic_mapping(self, data: Dict[str, Any], context: Dict[str, Any]) -> Dict[str, Any]:\n    |                                                                                                ^^^^\n311 |         \"\"\"Perform deterministic mapping analysis\"\"\"\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/A_analysis_nlp/implementacion_mapeo.py:310:106\n    |\n308 |             return self.sort_dict_by_keys(error_output)\n309 |     \n310 |     def _perform_deterministic_mapping(self, data: Dict[str, Any], context: Dict[str, Any]) -> Dict[str, Any]:\n    |                                                                                                          ^^^\n311 |         \"\"\"Perform deterministic mapping analysis\"\"\"\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/A_analysis_nlp/implementacion_mapeo.py:339:64\n    |\n337 |         }\n338 |     \n339 |     def _map_question_to_decalogo(self, question_text: str) -> Dict[str, Any]:\n    |                                                                ^^^^\n340 |         \"\"\"Map a question to dec\u00e1logo points deterministically\"\"\"\n341 |         question_id = self.generate_stable_id(question_text, prefix=\"q\")\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/A_analysis_nlp/implementacion_mapeo.py:339:74\n    |\n337 |         }\n338 |     \n339 |     def _map_question_to_decalogo(self, question_text: str) -> Dict[str, Any]:\n    |                                                                          ^^^\n340 |         \"\"\"Map a question to dec\u00e1logo points deterministically\"\"\"\n341 |         question_id = self.generate_stable_id(question_text, prefix=\"q\")\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/A_analysis_nlp/implementacion_mapeo.py:380:50\n    |\n378 |         }\n379 |     \n380 |     def _analyze_coverage_deterministic(self) -> Dict[str, Any]:\n    |                                                  ^^^^\n381 |         \"\"\"Analyze coverage with deterministic results\"\"\"\n382 |         coverage_by_point = {}\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/A_analysis_nlp/implementacion_mapeo.py:380:60\n    |\n378 |         }\n379 |     \n380 |     def _analyze_coverage_deterministic(self) -> Dict[str, Any]:\n    |                                                            ^^^\n381 |         \"\"\"Analyze coverage with deterministic results\"\"\"\n382 |         coverage_by_point = {}\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/A_analysis_nlp/implementacion_mapeo.py:410:60\n    |\n408 |         }\n409 |     \n410 |     def _generate_validation_report_deterministic(self) -> Dict[str, Any]:\n    |                                                            ^^^^\n411 |         \"\"\"Generate validation report with deterministic results\"\"\"\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/A_analysis_nlp/implementacion_mapeo.py:410:70\n    |\n408 |         }\n409 |     \n410 |     def _generate_validation_report_deterministic(self) -> Dict[str, Any]:\n    |                                                                      ^^^\n411 |         \"\"\"Generate validation report with deterministic results\"\"\"\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/A_analysis_nlp/implementacion_mapeo.py:454:63\n    |\n452 |         }\n453 |     \n454 |     def _generate_deterministic_output(self, mapping_results: Dict[str, Any], operation_id: str) -> Dict[str, Any]:\n    |                                                               ^^^^\n455 |         \"\"\"Generate deterministic output structure\"\"\"\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/A_analysis_nlp/implementacion_mapeo.py:454:73\n    |\n452 |         }\n453 |     \n454 |     def _generate_deterministic_output(self, mapping_results: Dict[str, Any], operation_id: str) -> Dict[str, Any]:\n    |                                                                         ^^^\n455 |         \"\"\"Generate deterministic output structure\"\"\"\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/A_analysis_nlp/implementacion_mapeo.py:454:101\n    |\n452 |         }\n453 |     \n454 |     def _generate_deterministic_output(self, mapping_results: Dict[str, Any], operation_id: str) -> Dict[str, Any]:\n    |                                                                                                     ^^^^\n455 |         \"\"\"Generate deterministic output structure\"\"\"\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/A_analysis_nlp/implementacion_mapeo.py:454:111\n    |\n452 |         }\n453 |     \n454 |     def _generate_deterministic_output(self, mapping_results: Dict[str, Any], operation_id: str) -> Dict[str, Any]:\n    |                                                                                                               ^^^\n455 |         \"\"\"Generate deterministic output structure\"\"\"\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/A_analysis_nlp/implementacion_mapeo.py:469:35\n    |\n467 |         return self.sort_dict_by_keys(output)\n468 |     \n469 |     def save_artifact(self, data: Dict[str, Any], document_stem: str, output_dir: str = \"canonical_flow/analysis\") -> str:\n    |                                   ^^^^\n470 |         \"\"\"\n471 |         Save implementacion mapeo output to canonical_flow/analysis/ directory with standardized naming.\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/A_analysis_nlp/implementacion_mapeo.py:469:45\n    |\n467 |         return self.sort_dict_by_keys(output)\n468 |     \n469 |     def save_artifact(self, data: Dict[str, Any], document_stem: str, output_dir: str = \"canonical_flow/analysis\") -> str:\n    |                                             ^^^\n470 |         \"\"\"\n471 |         Save implementacion mapeo output to canonical_flow/analysis/ directory with standardized naming.\n    |\n\nF821 Undefined name `Path`\n   --> canonical_flow/A_analysis_nlp/implementacion_mapeo.py:483:13\n    |\n481 |         try:\n482 |             # Create output directory if it doesn't exist\n483 |             Path(output_dir).mkdir(parents=True, exist_ok=True)\n    |             ^^^^\n484 |             \n485 |             # Generate standardized filename\n    |\n\nF821 Undefined name `Path`\n   --> canonical_flow/A_analysis_nlp/implementacion_mapeo.py:487:29\n    |\n485 |             # Generate standardized filename\n486 |             filename = f\"{document_stem}_implementacion.json\"\n487 |             artifact_path = Path(output_dir) / filename\n    |                             ^^^^\n488 |             \n489 |             # Save with consistent JSON formatting\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/A_analysis_nlp/implementacion_mapeo.py:503:31\n    |\n502 | # Backward compatibility functions\n503 | def get_complete_mapping() -> Dict[str, Any]:\n    |                               ^^^^\n504 |     \"\"\"Get complete mapping\"\"\"\n505 |     mapper = QuestionDecalogoMapper()\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/A_analysis_nlp/implementacion_mapeo.py:503:41\n    |\n502 | # Backward compatibility functions\n503 | def get_complete_mapping() -> Dict[str, Any]:\n    |                                         ^^^\n504 |     \"\"\"Get complete mapping\"\"\"\n505 |     mapper = QuestionDecalogoMapper()\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/A_analysis_nlp/implementacion_mapeo.py:509:48\n    |\n509 | def map_question_to_decalogo(question: str) -> Dict[str, Any]:\n    |                                                ^^^^\n510 |     \"\"\"Map single question to dec\u00e1logo\"\"\"\n511 |     mapper = QuestionDecalogoMapper()\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/A_analysis_nlp/implementacion_mapeo.py:509:58\n    |\n509 | def map_question_to_decalogo(question: str) -> Dict[str, Any]:\n    |                                                          ^^^\n510 |     \"\"\"Map single question to dec\u00e1logo\"\"\"\n511 |     mapper = QuestionDecalogoMapper()\n    |\n\ninvalid-syntax: Expected an indented block after `try` statement\n  --> canonical_flow/A_analysis_nlp/question_analyzer.py:41:1\n   |\n39 | try:\n40 | # # #     from sentence_transformers import SentenceTransformer  # Module not found  # Module not found  # Module not found\n41 | except ImportError:\n   | ^^^^^^\n42 |     SentenceTransformer = None\n   |\n\ninvalid-syntax: Expected an indented block after `try` statement\n  --> canonical_flow/A_analysis_nlp/question_analyzer.py:46:1\n   |\n44 | try:\n45 | # # #     from transformers import AutoModel, AutoTokenizer  # Module not found  # Module not found  # Module not found\n46 | except ImportError:\n   | ^^^^^^\n47 |     AutoModel = None\n48 |     AutoTokenizer = None\n   |\n\nF821 Undefined name `dataclass`\n  --> canonical_flow/G_aggregation_reporting/audit_logger.py:19:2\n   |\n19 | @dataclass\n   |  ^^^^^^^^^\n20 | class ComponentTrace:\n21 |     \"\"\"Individual component execution trace.\"\"\"\n   |\n\nF821 Undefined name `Optional`\n  --> canonical_flow/G_aggregation_reporting/audit_logger.py:24:15\n   |\n22 |     component_name: str\n23 |     start_time: float\n24 |     end_time: Optional[float] = None\n   |               ^^^^^^^^\n25 |     duration_ms: Optional[float] = None\n26 |     status: str = \"running\"  # \"running\", \"success\", \"failed\"\n   |\n\nF821 Undefined name `Optional`\n  --> canonical_flow/G_aggregation_reporting/audit_logger.py:25:18\n   |\n23 |     start_time: float\n24 |     end_time: Optional[float] = None\n25 |     duration_ms: Optional[float] = None\n   |                  ^^^^^^^^\n26 |     status: str = \"running\"  # \"running\", \"success\", \"failed\"\n27 |     error_details: Optional[str] = None\n   |\n\nF821 Undefined name `Optional`\n  --> canonical_flow/G_aggregation_reporting/audit_logger.py:27:20\n   |\n25 |     duration_ms: Optional[float] = None\n26 |     status: str = \"running\"  # \"running\", \"success\", \"failed\"\n27 |     error_details: Optional[str] = None\n   |                    ^^^^^^^^\n28 |     warnings: List[str] = field(default_factory=list)\n29 |     input_summary: Optional[Dict[str, Any]] = None\n   |\n\nF821 Undefined name `List`\n  --> canonical_flow/G_aggregation_reporting/audit_logger.py:28:15\n   |\n26 |     status: str = \"running\"  # \"running\", \"success\", \"failed\"\n27 |     error_details: Optional[str] = None\n28 |     warnings: List[str] = field(default_factory=list)\n   |               ^^^^\n29 |     input_summary: Optional[Dict[str, Any]] = None\n30 |     output_summary: Optional[Dict[str, Any]] = None\n   |\n\nF821 Undefined name `field`\n  --> canonical_flow/G_aggregation_reporting/audit_logger.py:28:27\n   |\n26 |     status: str = \"running\"  # \"running\", \"success\", \"failed\"\n27 |     error_details: Optional[str] = None\n28 |     warnings: List[str] = field(default_factory=list)\n   |                           ^^^^^\n29 |     input_summary: Optional[Dict[str, Any]] = None\n30 |     output_summary: Optional[Dict[str, Any]] = None\n   |\n\nF821 Undefined name `Optional`\n  --> canonical_flow/G_aggregation_reporting/audit_logger.py:29:20\n   |\n27 |     error_details: Optional[str] = None\n28 |     warnings: List[str] = field(default_factory=list)\n29 |     input_summary: Optional[Dict[str, Any]] = None\n   |                    ^^^^^^^^\n30 |     output_summary: Optional[Dict[str, Any]] = None\n31 |     memory_usage_mb: Optional[float] = None\n   |\n\nF821 Undefined name `Dict`\n  --> canonical_flow/G_aggregation_reporting/audit_logger.py:29:29\n   |\n27 |     error_details: Optional[str] = None\n28 |     warnings: List[str] = field(default_factory=list)\n29 |     input_summary: Optional[Dict[str, Any]] = None\n   |                             ^^^^\n30 |     output_summary: Optional[Dict[str, Any]] = None\n31 |     memory_usage_mb: Optional[float] = None\n   |\n\nF821 Undefined name `Any`\n  --> canonical_flow/G_aggregation_reporting/audit_logger.py:29:39\n   |\n27 |     error_details: Optional[str] = None\n28 |     warnings: List[str] = field(default_factory=list)\n29 |     input_summary: Optional[Dict[str, Any]] = None\n   |                                       ^^^\n30 |     output_summary: Optional[Dict[str, Any]] = None\n31 |     memory_usage_mb: Optional[float] = None\n   |\n\nF821 Undefined name `Optional`\n  --> canonical_flow/G_aggregation_reporting/audit_logger.py:30:21\n   |\n28 |     warnings: List[str] = field(default_factory=list)\n29 |     input_summary: Optional[Dict[str, Any]] = None\n30 |     output_summary: Optional[Dict[str, Any]] = None\n   |                     ^^^^^^^^\n31 |     memory_usage_mb: Optional[float] = None\n   |\n\nF821 Undefined name `Dict`\n  --> canonical_flow/G_aggregation_reporting/audit_logger.py:30:30\n   |\n28 |     warnings: List[str] = field(default_factory=list)\n29 |     input_summary: Optional[Dict[str, Any]] = None\n30 |     output_summary: Optional[Dict[str, Any]] = None\n   |                              ^^^^\n31 |     memory_usage_mb: Optional[float] = None\n   |\n\nF821 Undefined name `Any`\n  --> canonical_flow/G_aggregation_reporting/audit_logger.py:30:40\n   |\n28 |     warnings: List[str] = field(default_factory=list)\n29 |     input_summary: Optional[Dict[str, Any]] = None\n30 |     output_summary: Optional[Dict[str, Any]] = None\n   |                                        ^^^\n31 |     memory_usage_mb: Optional[float] = None\n   |\n\nF821 Undefined name `Optional`\n  --> canonical_flow/G_aggregation_reporting/audit_logger.py:31:22\n   |\n29 |     input_summary: Optional[Dict[str, Any]] = None\n30 |     output_summary: Optional[Dict[str, Any]] = None\n31 |     memory_usage_mb: Optional[float] = None\n   |                      ^^^^^^^^\n32 |     \n33 |     def complete_success(self, output_summary: Optional[Dict[str, Any]] = None):\n   |\n\nF821 Undefined name `Optional`\n  --> canonical_flow/G_aggregation_reporting/audit_logger.py:33:48\n   |\n31 |     memory_usage_mb: Optional[float] = None\n32 |     \n33 |     def complete_success(self, output_summary: Optional[Dict[str, Any]] = None):\n   |                                                ^^^^^^^^\n34 |         \"\"\"Mark trace as successfully completed.\"\"\"\n35 |         self.end_time = time.time()\n   |\n\nF821 Undefined name `Dict`\n  --> canonical_flow/G_aggregation_reporting/audit_logger.py:33:57\n   |\n31 |     memory_usage_mb: Optional[float] = None\n32 |     \n33 |     def complete_success(self, output_summary: Optional[Dict[str, Any]] = None):\n   |                                                         ^^^^\n34 |         \"\"\"Mark trace as successfully completed.\"\"\"\n35 |         self.end_time = time.time()\n   |\n\nF821 Undefined name `Any`\n  --> canonical_flow/G_aggregation_reporting/audit_logger.py:33:67\n   |\n31 |     memory_usage_mb: Optional[float] = None\n32 |     \n33 |     def complete_success(self, output_summary: Optional[Dict[str, Any]] = None):\n   |                                                                   ^^^\n34 |         \"\"\"Mark trace as successfully completed.\"\"\"\n35 |         self.end_time = time.time()\n   |\n\nF821 Undefined name `Optional`\n  --> canonical_flow/G_aggregation_reporting/audit_logger.py:40:65\n   |\n38 |         self.output_summary = output_summary\n39 |     \n40 |     def complete_failure(self, error: Exception, error_context: Optional[str] = None):\n   |                                                                 ^^^^^^^^\n41 |         \"\"\"Mark trace as failed with error details.\"\"\"\n42 |         self.end_time = time.time()\n   |\n\nF821 Undefined name `datetime`\n  --> canonical_flow/G_aggregation_reporting/audit_logger.py:55:26\n   |\n53 |         \"\"\"Add warning to component trace.\"\"\"\n54 |         self.warnings.append({\n55 |             \"timestamp\": datetime.now(timezone.utc).isoformat(),\n   |                          ^^^^^^^^\n56 |             \"message\": warning_message\n57 |         })\n   |\n\nF821 Undefined name `timezone`\n  --> canonical_flow/G_aggregation_reporting/audit_logger.py:55:39\n   |\n53 |         \"\"\"Add warning to component trace.\"\"\"\n54 |         self.warnings.append({\n55 |             \"timestamp\": datetime.now(timezone.utc).isoformat(),\n   |                                       ^^^^^^^^\n56 |             \"message\": warning_message\n57 |         })\n   |\n\nF821 Undefined name `dataclass`\n  --> canonical_flow/G_aggregation_reporting/audit_logger.py:60:2\n   |\n60 | @dataclass\n   |  ^^^^^^^^^\n61 | class AuditSession:\n62 |     \"\"\"Complete audit session for a document processing run.\"\"\"\n   |\n\nF821 Undefined name `Optional`\n  --> canonical_flow/G_aggregation_reporting/audit_logger.py:66:15\n   |\n64 |     session_id: str\n65 |     start_time: float\n66 |     end_time: Optional[float] = None\n   |               ^^^^^^^^\n67 |     duration_ms: Optional[float] = None\n68 |     overall_status: str = \"running\"  # \"running\", \"success\", \"failed\"\n   |\n\nF821 Undefined name `Optional`\n  --> canonical_flow/G_aggregation_reporting/audit_logger.py:67:18\n   |\n65 |     start_time: float\n66 |     end_time: Optional[float] = None\n67 |     duration_ms: Optional[float] = None\n   |                  ^^^^^^^^\n68 |     overall_status: str = \"running\"  # \"running\", \"success\", \"failed\"\n69 |     components: List[ComponentTrace] = field(default_factory=list)\n   |\n\nF821 Undefined name `List`\n  --> canonical_flow/G_aggregation_reporting/audit_logger.py:69:17\n   |\n67 |     duration_ms: Optional[float] = None\n68 |     overall_status: str = \"running\"  # \"running\", \"success\", \"failed\"\n69 |     components: List[ComponentTrace] = field(default_factory=list)\n   |                 ^^^^\n70 |     session_warnings: List[str] = field(default_factory=list)\n71 |     context: Dict[str, Any] = field(default_factory=dict)\n   |\n\nF821 Undefined name `field`\n  --> canonical_flow/G_aggregation_reporting/audit_logger.py:69:40\n   |\n67 |     duration_ms: Optional[float] = None\n68 |     overall_status: str = \"running\"  # \"running\", \"success\", \"failed\"\n69 |     components: List[ComponentTrace] = field(default_factory=list)\n   |                                        ^^^^^\n70 |     session_warnings: List[str] = field(default_factory=list)\n71 |     context: Dict[str, Any] = field(default_factory=dict)\n   |\n\nF821 Undefined name `List`\n  --> canonical_flow/G_aggregation_reporting/audit_logger.py:70:23\n   |\n68 |     overall_status: str = \"running\"  # \"running\", \"success\", \"failed\"\n69 |     components: List[ComponentTrace] = field(default_factory=list)\n70 |     session_warnings: List[str] = field(default_factory=list)\n   |                       ^^^^\n71 |     context: Dict[str, Any] = field(default_factory=dict)\n   |\n\nF821 Undefined name `field`\n  --> canonical_flow/G_aggregation_reporting/audit_logger.py:70:35\n   |\n68 |     overall_status: str = \"running\"  # \"running\", \"success\", \"failed\"\n69 |     components: List[ComponentTrace] = field(default_factory=list)\n70 |     session_warnings: List[str] = field(default_factory=list)\n   |                                   ^^^^^\n71 |     context: Dict[str, Any] = field(default_factory=dict)\n   |\n\nF821 Undefined name `Dict`\n  --> canonical_flow/G_aggregation_reporting/audit_logger.py:71:14\n   |\n69 |     components: List[ComponentTrace] = field(default_factory=list)\n70 |     session_warnings: List[str] = field(default_factory=list)\n71 |     context: Dict[str, Any] = field(default_factory=dict)\n   |              ^^^^\n72 |     \n73 |     def add_component_trace(self, component_name: str) -> ComponentTrace:\n   |\n\nF821 Undefined name `Any`\n  --> canonical_flow/G_aggregation_reporting/audit_logger.py:71:24\n   |\n69 |     components: List[ComponentTrace] = field(default_factory=list)\n70 |     session_warnings: List[str] = field(default_factory=list)\n71 |     context: Dict[str, Any] = field(default_factory=dict)\n   |                        ^^^\n72 |     \n73 |     def add_component_trace(self, component_name: str) -> ComponentTrace:\n   |\n\nF821 Undefined name `field`\n  --> canonical_flow/G_aggregation_reporting/audit_logger.py:71:31\n   |\n69 |     components: List[ComponentTrace] = field(default_factory=list)\n70 |     session_warnings: List[str] = field(default_factory=list)\n71 |     context: Dict[str, Any] = field(default_factory=dict)\n   |                               ^^^^^\n72 |     \n73 |     def add_component_trace(self, component_name: str) -> ComponentTrace:\n   |\n\nF821 Undefined name `datetime`\n  --> canonical_flow/G_aggregation_reporting/audit_logger.py:91:26\n   |\n89 |         \"\"\"Add session-level warning.\"\"\"\n90 |         self.session_warnings.append({\n91 |             \"timestamp\": datetime.now(timezone.utc).isoformat(),\n   |                          ^^^^^^^^\n92 |             \"message\": warning_message\n93 |         })\n   |\n\nF821 Undefined name `timezone`\n  --> canonical_flow/G_aggregation_reporting/audit_logger.py:91:39\n   |\n89 |         \"\"\"Add session-level warning.\"\"\"\n90 |         self.session_warnings.append({\n91 |             \"timestamp\": datetime.now(timezone.utc).isoformat(),\n   |                                       ^^^^^^^^\n92 |             \"message\": warning_message\n93 |         })\n   |\n\nF821 Undefined name `Union`\n   --> canonical_flow/G_aggregation_reporting/audit_logger.py:104:36\n    |\n102 |     \"\"\"\n103 |     \n104 |     def __init__(self, output_dir: Union[str, Path] = \"canonical_flow/aggregation\"):\n    |                                    ^^^^^\n105 |         \"\"\"\n106 |         Initialize audit logger.\n    |\n\nF821 Undefined name `Path`\n   --> canonical_flow/G_aggregation_reporting/audit_logger.py:104:47\n    |\n102 |     \"\"\"\n103 |     \n104 |     def __init__(self, output_dir: Union[str, Path] = \"canonical_flow/aggregation\"):\n    |                                               ^^^^\n105 |         \"\"\"\n106 |         Initialize audit logger.\n    |\n\nF821 Undefined name `Path`\n   --> canonical_flow/G_aggregation_reporting/audit_logger.py:111:27\n    |\n109 |             output_dir: Directory to write audit files to\n110 |         \"\"\"\n111 |         self.output_dir = Path(output_dir)\n    |                           ^^^^\n112 |         self.output_dir.mkdir(parents=True, exist_ok=True)\n113 |         self.current_session: Optional[AuditSession] = None\n    |\n\nF821 Undefined name `Optional`\n   --> canonical_flow/G_aggregation_reporting/audit_logger.py:113:31\n    |\n111 |         self.output_dir = Path(output_dir)\n112 |         self.output_dir.mkdir(parents=True, exist_ok=True)\n113 |         self.current_session: Optional[AuditSession] = None\n    |                               ^^^^^^^^\n114 |         \n115 |     def start_session(self, doc_stem: str, context: Optional[Dict[str, Any]] = None) -> str:\n    |\n\nF821 Undefined name `Optional`\n   --> canonical_flow/G_aggregation_reporting/audit_logger.py:115:53\n    |\n113 |         self.current_session: Optional[AuditSession] = None\n114 |         \n115 |     def start_session(self, doc_stem: str, context: Optional[Dict[str, Any]] = None) -> str:\n    |                                                     ^^^^^^^^\n116 |         \"\"\"\n117 |         Start a new audit session for processing a document.\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/G_aggregation_reporting/audit_logger.py:115:62\n    |\n113 |         self.current_session: Optional[AuditSession] = None\n114 |         \n115 |     def start_session(self, doc_stem: str, context: Optional[Dict[str, Any]] = None) -> str:\n    |                                                              ^^^^\n116 |         \"\"\"\n117 |         Start a new audit session for processing a document.\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/G_aggregation_reporting/audit_logger.py:115:72\n    |\n113 |         self.current_session: Optional[AuditSession] = None\n114 |         \n115 |     def start_session(self, doc_stem: str, context: Optional[Dict[str, Any]] = None) -> str:\n    |                                                                        ^^^\n116 |         \"\"\"\n117 |         Start a new audit session for processing a document.\n    |\n\nF821 Undefined name `Optional`\n   --> canonical_flow/G_aggregation_reporting/audit_logger.py:137:52\n    |\n135 |         return session_id\n136 |     \n137 |     def end_session(self, success: bool = True) -> Optional[str]:\n    |                                                    ^^^^^^^^\n138 |         \"\"\"\n139 |         End current audit session and write audit file.\n    |\n\nE722 Do not use bare `except`\n   --> canonical_flow/G_aggregation_reporting/audit_logger.py:171:13\n    |\n169 |                 if self.current_session:\n170 |                     self.current_session.add_session_warning(error_msg)\n171 |             except:\n    |             ^^^^^^\n172 |                 pass  # Avoid recursive errors\n    |\n\nF821 Undefined name `contextmanager`\n   --> canonical_flow/G_aggregation_reporting/audit_logger.py:176:6\n    |\n174 |             return None\n175 |     \n176 |     @contextmanager\n    |      ^^^^^^^^^^^^^^\n177 |     def trace_component(self, \n178 |                        component_name: str,\n    |\n\nF821 Undefined name `Optional`\n   --> canonical_flow/G_aggregation_reporting/audit_logger.py:179:36\n    |\n177 |     def trace_component(self, \n178 |                        component_name: str,\n179 |                        input_data: Optional[Any] = None,\n    |                                    ^^^^^^^^\n180 |                        capture_memory: bool = False):\n181 |         \"\"\"\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/G_aggregation_reporting/audit_logger.py:179:45\n    |\n177 |     def trace_component(self, \n178 |                        component_name: str,\n179 |                        input_data: Optional[Any] = None,\n    |                                             ^^^\n180 |                        capture_memory: bool = False):\n181 |         \"\"\"\n    |\n\nF821 Undefined name `Optional`\n   --> canonical_flow/G_aggregation_reporting/audit_logger.py:234:38\n    |\n232 |             self.current_session.add_session_warning(warning_message)\n233 |     \n234 |     def get_session_summary(self) -> Optional[Dict[str, Any]]:\n    |                                      ^^^^^^^^\n235 |         \"\"\"Get summary of current session.\"\"\"\n236 |         if not self.current_session:\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/G_aggregation_reporting/audit_logger.py:234:47\n    |\n232 |             self.current_session.add_session_warning(warning_message)\n233 |     \n234 |     def get_session_summary(self) -> Optional[Dict[str, Any]]:\n    |                                               ^^^^\n235 |         \"\"\"Get summary of current session.\"\"\"\n236 |         if not self.current_session:\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/G_aggregation_reporting/audit_logger.py:234:57\n    |\n232 |             self.current_session.add_session_warning(warning_message)\n233 |     \n234 |     def get_session_summary(self) -> Optional[Dict[str, Any]]:\n    |                                                         ^^^\n235 |         \"\"\"Get summary of current session.\"\"\"\n236 |         if not self.current_session:\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/G_aggregation_reporting/audit_logger.py:249:60\n    |\n247 |         }\n248 |     \n249 |     def _serialize_session(self, session: AuditSession) -> Dict[str, Any]:\n    |                                                            ^^^^\n250 |         \"\"\"Serialize audit session to JSON-compatible format.\"\"\"\n251 |         return {\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/G_aggregation_reporting/audit_logger.py:249:70\n    |\n247 |         }\n248 |     \n249 |     def _serialize_session(self, session: AuditSession) -> Dict[str, Any]:\n    |                                                                      ^^^\n250 |         \"\"\"Serialize audit session to JSON-compatible format.\"\"\"\n251 |         return {\n    |\n\nF821 Undefined name `datetime`\n   --> canonical_flow/G_aggregation_reporting/audit_logger.py:254:33\n    |\n252 |             \"audit_metadata\": {\n253 |                 \"version\": \"1.0\",\n254 |                 \"generated_at\": datetime.now(timezone.utc).isoformat(),\n    |                                 ^^^^^^^^\n255 |                 \"stage\": \"G_aggregation_reporting\",\n256 |                 \"logger_type\": \"AggregationAuditLogger\"\n    |\n\nF821 Undefined name `timezone`\n   --> canonical_flow/G_aggregation_reporting/audit_logger.py:254:46\n    |\n252 |             \"audit_metadata\": {\n253 |                 \"version\": \"1.0\",\n254 |                 \"generated_at\": datetime.now(timezone.utc).isoformat(),\n    |                                              ^^^^^^^^\n255 |                 \"stage\": \"G_aggregation_reporting\",\n256 |                 \"logger_type\": \"AggregationAuditLogger\"\n    |\n\nF821 Undefined name `datetime`\n   --> canonical_flow/G_aggregation_reporting/audit_logger.py:261:31\n    |\n259 |                 \"doc_stem\": session.doc_stem,\n260 |                 \"session_id\": session.session_id,\n261 |                 \"start_time\": datetime.fromtimestamp(session.start_time, timezone.utc).isoformat(),\n    |                               ^^^^^^^^\n262 |                 \"end_time\": (\n263 |                     datetime.fromtimestamp(session.end_time, timezone.utc).isoformat() \n    |\n\nF821 Undefined name `timezone`\n   --> canonical_flow/G_aggregation_reporting/audit_logger.py:261:74\n    |\n259 |                 \"doc_stem\": session.doc_stem,\n260 |                 \"session_id\": session.session_id,\n261 |                 \"start_time\": datetime.fromtimestamp(session.start_time, timezone.utc).isoformat(),\n    |                                                                          ^^^^^^^^\n262 |                 \"end_time\": (\n263 |                     datetime.fromtimestamp(session.end_time, timezone.utc).isoformat() \n    |\n\nF821 Undefined name `datetime`\n   --> canonical_flow/G_aggregation_reporting/audit_logger.py:263:21\n    |\n261 |                 \"start_time\": datetime.fromtimestamp(session.start_time, timezone.utc).isoformat(),\n262 |                 \"end_time\": (\n263 |                     datetime.fromtimestamp(session.end_time, timezone.utc).isoformat() \n    |                     ^^^^^^^^\n264 |                     if session.end_time else None\n265 |                 ),\n    |\n\nF821 Undefined name `timezone`\n   --> canonical_flow/G_aggregation_reporting/audit_logger.py:263:62\n    |\n261 |                 \"start_time\": datetime.fromtimestamp(session.start_time, timezone.utc).isoformat(),\n262 |                 \"end_time\": (\n263 |                     datetime.fromtimestamp(session.end_time, timezone.utc).isoformat() \n    |                                                              ^^^^^^^^\n264 |                     if session.end_time else None\n265 |                 ),\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/G_aggregation_reporting/audit_logger.py:277:68\n    |\n275 |         }\n276 |     \n277 |     def _serialize_component_trace(self, trace: ComponentTrace) -> Dict[str, Any]:\n    |                                                                    ^^^^\n278 |         \"\"\"Serialize component trace to JSON-compatible format.\"\"\"\n279 |         return {\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/G_aggregation_reporting/audit_logger.py:277:78\n    |\n275 |         }\n276 |     \n277 |     def _serialize_component_trace(self, trace: ComponentTrace) -> Dict[str, Any]:\n    |                                                                              ^^^\n278 |         \"\"\"Serialize component trace to JSON-compatible format.\"\"\"\n279 |         return {\n    |\n\nF821 Undefined name `datetime`\n   --> canonical_flow/G_aggregation_reporting/audit_logger.py:281:27\n    |\n279 |         return {\n280 |             \"component_name\": trace.component_name,\n281 |             \"start_time\": datetime.fromtimestamp(trace.start_time, timezone.utc).isoformat(),\n    |                           ^^^^^^^^\n282 |             \"end_time\": (\n283 |                 datetime.fromtimestamp(trace.end_time, timezone.utc).isoformat() \n    |\n\nF821 Undefined name `timezone`\n   --> canonical_flow/G_aggregation_reporting/audit_logger.py:281:68\n    |\n279 |         return {\n280 |             \"component_name\": trace.component_name,\n281 |             \"start_time\": datetime.fromtimestamp(trace.start_time, timezone.utc).isoformat(),\n    |                                                                    ^^^^^^^^\n282 |             \"end_time\": (\n283 |                 datetime.fromtimestamp(trace.end_time, timezone.utc).isoformat() \n    |\n\nF821 Undefined name `datetime`\n   --> canonical_flow/G_aggregation_reporting/audit_logger.py:283:17\n    |\n281 |             \"start_time\": datetime.fromtimestamp(trace.start_time, timezone.utc).isoformat(),\n282 |             \"end_time\": (\n283 |                 datetime.fromtimestamp(trace.end_time, timezone.utc).isoformat() \n    |                 ^^^^^^^^\n284 |                 if trace.end_time else None\n285 |             ),\n    |\n\nF821 Undefined name `timezone`\n   --> canonical_flow/G_aggregation_reporting/audit_logger.py:283:56\n    |\n281 |             \"start_time\": datetime.fromtimestamp(trace.start_time, timezone.utc).isoformat(),\n282 |             \"end_time\": (\n283 |                 datetime.fromtimestamp(trace.end_time, timezone.utc).isoformat() \n    |                                                        ^^^^^^^^\n284 |                 if trace.end_time else None\n285 |             ),\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/G_aggregation_reporting/audit_logger.py:295:71\n    |\n293 |         }\n294 |     \n295 |     def _calculate_session_statistics(self, session: AuditSession) -> Dict[str, Any]:\n    |                                                                       ^^^^\n296 |         \"\"\"Calculate summary statistics for the session.\"\"\"\n297 |         components = session.components\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/G_aggregation_reporting/audit_logger.py:295:81\n    |\n293 |         }\n294 |     \n295 |     def _calculate_session_statistics(self, session: AuditSession) -> Dict[str, Any]:\n    |                                                                                 ^^^\n296 |         \"\"\"Calculate summary statistics for the session.\"\"\"\n297 |         components = session.components\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/G_aggregation_reporting/audit_logger.py:319:42\n    |\n317 |         }\n318 |     \n319 |     def _create_data_summary(self, data: Any) -> Dict[str, Any]:\n    |                                          ^^^\n320 |         \"\"\"Create summary of input/output data for audit logging.\"\"\"\n321 |         if data is None:\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/G_aggregation_reporting/audit_logger.py:319:50\n    |\n317 |         }\n318 |     \n319 |     def _create_data_summary(self, data: Any) -> Dict[str, Any]:\n    |                                                  ^^^^\n320 |         \"\"\"Create summary of input/output data for audit logging.\"\"\"\n321 |         if data is None:\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/G_aggregation_reporting/audit_logger.py:319:60\n    |\n317 |         }\n318 |     \n319 |     def _create_data_summary(self, data: Any) -> Dict[str, Any]:\n    |                                                            ^^^\n320 |         \"\"\"Create summary of input/output data for audit logging.\"\"\"\n321 |         if data is None:\n    |\n\nF821 Undefined name `Optional`\n   --> canonical_flow/G_aggregation_reporting/audit_logger.py:352:36\n    |\n350 |             }\n351 |     \n352 |     def _get_memory_usage(self) -> Optional[float]:\n    |                                    ^^^^^^^^\n353 |         \"\"\"Get current memory usage in MB.\"\"\"\n354 |         try:\n    |\n\ninvalid-syntax: Expected an indented block after function definition\n  --> canonical_flow/G_aggregation_reporting/meso_aggregator.py:75:5\n   |\n73 |     def _process_document_stem(stem: str) -> Dict[str, Any]:\n74 |     \n75 |     Args:\n   |     ^^^^\n76 |         document_stem: Document identifier stem (e.g., \"ACANDI-CHOCO\")\n   |\n\ninvalid-syntax: Expected an expression\n  --> canonical_flow/G_aggregation_reporting/meso_aggregator.py:75:10\n   |\n73 |     def _process_document_stem(stem: str) -> Dict[str, Any]:\n74 |     \n75 |     Args:\n   |          ^\n76 |         document_stem: Document identifier stem (e.g., \"ACANDI-CHOCO\")\n   |\n\ninvalid-syntax: Unexpected indentation\n  --> canonical_flow/G_aggregation_reporting/meso_aggregator.py:76:1\n   |\n75 |     Args:\n76 |         document_stem: Document identifier stem (e.g., \"ACANDI-CHOCO\")\n   | ^^^^^^^^\n77 |         \n78 |     Returns:\n   |\n\ninvalid-syntax: Simple statements must be separated by newlines or semicolons\n  --> canonical_flow/G_aggregation_reporting/meso_aggregator.py:76:33\n   |\n75 |     Args:\n76 |         document_stem: Document identifier stem (e.g., \"ACANDI-CHOCO\")\n   |                                 ^^^^^^^^^^\n77 |         \n78 |     Returns:\n   |\n\ninvalid-syntax: Simple statements must be separated by newlines or semicolons\n  --> canonical_flow/G_aggregation_reporting/meso_aggregator.py:76:44\n   |\n75 |     Args:\n76 |         document_stem: Document identifier stem (e.g., \"ACANDI-CHOCO\")\n   |                                            ^^^^\n77 |         \n78 |     Returns:\n   |\n\ninvalid-syntax: Expected an identifier\n  --> canonical_flow/G_aggregation_reporting/meso_aggregator.py:76:54\n   |\n75 |     Args:\n76 |         document_stem: Document identifier stem (e.g., \"ACANDI-CHOCO\")\n   |                                                      ^\n77 |         \n78 |     Returns:\n   |\n\ninvalid-syntax: Expected an expression\n  --> canonical_flow/G_aggregation_reporting/meso_aggregator.py:78:13\n   |\n76 |         document_stem: Document identifier stem (e.g., \"ACANDI-CHOCO\")\n77 |         \n78 |     Returns:\n   |             ^\n79 |         Dict with 'success' boolean and 'artifacts' list\n80 |     \"\"\"\n   |\n\ninvalid-syntax: Unexpected indentation\n  --> canonical_flow/G_aggregation_reporting/meso_aggregator.py:79:1\n   |\n78 |     Returns:\n79 |         Dict with 'success' boolean and 'artifacts' list\n   | ^^^^^^^^\n80 |     \"\"\"\n81 |     try:\n   |\n\ninvalid-syntax: Compound statements are not allowed on the same line as simple statements\n  --> canonical_flow/G_aggregation_reporting/meso_aggregator.py:79:14\n   |\n78 |     Returns:\n79 |         Dict with 'success' boolean and 'artifacts' list\n   |              ^^^^\n80 |     \"\"\"\n81 |     try:\n   |\n\ninvalid-syntax: Expected ',', found name\n  --> canonical_flow/G_aggregation_reporting/meso_aggregator.py:79:29\n   |\n78 |     Returns:\n79 |         Dict with 'success' boolean and 'artifacts' list\n   |                             ^^^^^^^\n80 |     \"\"\"\n81 |     try:\n   |\n\ninvalid-syntax: Expected ',', found name\n  --> canonical_flow/G_aggregation_reporting/meso_aggregator.py:79:53\n   |\n78 |     Returns:\n79 |         Dict with 'success' boolean and 'artifacts' list\n   |                                                     ^^^^\n80 |     \"\"\"\n81 |     try:\n   |\n\ninvalid-syntax: Expected ',', found newline\n  --> canonical_flow/G_aggregation_reporting/meso_aggregator.py:79:57\n   |\n78 |     Returns:\n79 |         Dict with 'success' boolean and 'artifacts' list\n   |                                                         ^\n80 |     \"\"\"\n81 |     try:\n   |\n\ninvalid-syntax: Expected ',', found dedent\n  --> canonical_flow/G_aggregation_reporting/meso_aggregator.py:80:5\n   |\n78 |     Returns:\n79 |         Dict with 'success' boolean and 'artifacts' list\n80 |     \"\"\"\n   |     ^\n81 |     try:\n82 |         logger.info(f\"Starting meso aggregation for document: {document_stem}\")\n   |\n\ninvalid-syntax: Expected ',', found name\n   --> canonical_flow/G_aggregation_reporting/meso_aggregator.py:156:8\n    |\n155 | def _read_analysis_outputs(document_stem: str) -> List[Dict[str, Any]]:\n156 |     \"\"\"Read all analysis outputs related to the document stem.\"\"\"\n    |        ^^^^\n157 |     # Try different possible paths for analysis directory\n158 |     possible_paths = [\n    |\n\ninvalid-syntax: Expected ',', found name\n   --> canonical_flow/G_aggregation_reporting/meso_aggregator.py:156:13\n    |\n155 | def _read_analysis_outputs(document_stem: str) -> List[Dict[str, Any]]:\n156 |     \"\"\"Read all analysis outputs related to the document stem.\"\"\"\n    |             ^^^\n157 |     # Try different possible paths for analysis directory\n158 |     possible_paths = [\n    |\n\ninvalid-syntax: Expected ',', found name\n   --> canonical_flow/G_aggregation_reporting/meso_aggregator.py:156:17\n    |\n155 | def _read_analysis_outputs(document_stem: str) -> List[Dict[str, Any]]:\n156 |     \"\"\"Read all analysis outputs related to the document stem.\"\"\"\n    |                 ^^^^^^^^\n157 |     # Try different possible paths for analysis directory\n158 |     possible_paths = [\n    |\n\ninvalid-syntax: Expected ',', found name\n   --> canonical_flow/G_aggregation_reporting/meso_aggregator.py:156:26\n    |\n155 | def _read_analysis_outputs(document_stem: str) -> List[Dict[str, Any]]:\n156 |     \"\"\"Read all analysis outputs related to the document stem.\"\"\"\n    |                          ^^^^^^^\n157 |     # Try different possible paths for analysis directory\n158 |     possible_paths = [\n    |\n\ninvalid-syntax: Expected ',', found name\n   --> canonical_flow/G_aggregation_reporting/meso_aggregator.py:156:34\n    |\n155 | def _read_analysis_outputs(document_stem: str) -> List[Dict[str, Any]]:\n156 |     \"\"\"Read all analysis outputs related to the document stem.\"\"\"\n    |                                  ^^^^^^^\n157 |     # Try different possible paths for analysis directory\n158 |     possible_paths = [\n    |\n\ninvalid-syntax: Expected ',', found name\n   --> canonical_flow/G_aggregation_reporting/meso_aggregator.py:156:42\n    |\n155 | def _read_analysis_outputs(document_stem: str) -> List[Dict[str, Any]]:\n156 |     \"\"\"Read all analysis outputs related to the document stem.\"\"\"\n    |                                          ^^\n157 |     # Try different possible paths for analysis directory\n158 |     possible_paths = [\n    |\n\ninvalid-syntax: Expected ',', found name\n   --> canonical_flow/G_aggregation_reporting/meso_aggregator.py:156:45\n    |\n155 | def _read_analysis_outputs(document_stem: str) -> List[Dict[str, Any]]:\n156 |     \"\"\"Read all analysis outputs related to the document stem.\"\"\"\n    |                                             ^^^\n157 |     # Try different possible paths for analysis directory\n158 |     possible_paths = [\n    |\n\ninvalid-syntax: Expected ',', found name\n   --> canonical_flow/G_aggregation_reporting/meso_aggregator.py:156:49\n    |\n155 | def _read_analysis_outputs(document_stem: str) -> List[Dict[str, Any]]:\n156 |     \"\"\"Read all analysis outputs related to the document stem.\"\"\"\n    |                                                 ^^^^^^^^\n157 |     # Try different possible paths for analysis directory\n158 |     possible_paths = [\n    |\n\ninvalid-syntax: Expected ',', found name\n   --> canonical_flow/G_aggregation_reporting/meso_aggregator.py:156:58\n    |\n155 | def _read_analysis_outputs(document_stem: str) -> List[Dict[str, Any]]:\n156 |     \"\"\"Read all analysis outputs related to the document stem.\"\"\"\n    |                                                          ^^^^\n157 |     # Try different possible paths for analysis directory\n158 |     possible_paths = [\n    |\n\ninvalid-syntax: Expected an identifier\n   --> canonical_flow/G_aggregation_reporting/meso_aggregator.py:156:63\n    |\n155 |   def _read_analysis_outputs(document_stem: str) -> List[Dict[str, Any]]:\n156 |       \"\"\"Read all analysis outputs related to the document stem.\"\"\"\n    |  _______________________________________________________________^\n157 | |     # Try different possible paths for analysis directory\n158 | |     possible_paths = [\n159 | |         Path(\"canonical_flow/analysis\"),\n160 | |         Path(\"../analysis\"),\n161 | |         Path(\"../../canonical_flow/analysis\")\n162 | |     ]\n163 | |     \n164 | |     analysis_dir = None\n165 | |     for path in possible_paths:\n166 | |         if path.exists():\n167 | |             analysis_dir = path\n168 | |             break\n169 | |     \n170 | |     analysis_data = []\n171 | |     \n172 | |     if not analysis_dir:\n173 | |         logger.warning(\"Analysis directory not found in any expected location\")\n174 | |         return analysis_data\n175 | |     \n176 | |     # Look for files containing the document stem or relevant analysis outputs\n177 | |     for file_path in analysis_dir.glob(\"**/*.json\"):\n178 | |         try:\n179 | |             with open(file_path, 'r', encoding='utf-8') as f:\n180 | |                 data = json.load(f)\n181 | |                 \n182 | |             # Check if this file relates to our document stem\n183 | |             if _is_relevant_analysis_file(data, document_stem, file_path.name):\n184 | |                 analysis_data.append({\n185 | |                     \"source_file\": str(file_path),\n186 | |                     \"data\": data\n187 | |                 })\n188 | |                 logger.info(f\"Loaded analysis file: {file_path}\")\n189 | |                 \n190 | |         except (json.JSONDecodeError, IOError) as e:\n191 | |             logger.warning(f\"Could not read analysis file {file_path}: {str(e)}\")\n192 | |             continue\n193 | |     \n194 | |     return analysis_data\n195 | |\n196 | |\n197 | | def _is_relevant_analysis_file(data: Dict[str, Any], document_stem: str, filename: str) -> bool:\n198 | |     \"\"\"Check if an analysis file is relevant to the document stem.\"\"\"\n    | |_______^\n199 |       # Check if document stem appears in the filename\n200 |       if document_stem.lower() in filename.lower():\n    |\n\ninvalid-syntax: Expected ',', found name\n   --> canonical_flow/G_aggregation_reporting/meso_aggregator.py:198:8\n    |\n197 | def _is_relevant_analysis_file(data: Dict[str, Any], document_stem: str, filename: str) -> bool:\n198 |     \"\"\"Check if an analysis file is relevant to the document stem.\"\"\"\n    |        ^^^^^\n199 |     # Check if document stem appears in the filename\n200 |     if document_stem.lower() in filename.lower():\n    |\n\ninvalid-syntax: Expected 'else', found name\n   --> canonical_flow/G_aggregation_reporting/meso_aggregator.py:198:20\n    |\n197 | def _is_relevant_analysis_file(data: Dict[str, Any], document_stem: str, filename: str) -> bool:\n198 |     \"\"\"Check if an analysis file is relevant to the document stem.\"\"\"\n    |                    ^^^^^^^^\n199 |     # Check if document stem appears in the filename\n200 |     if document_stem.lower() in filename.lower():\n    |\n\ninvalid-syntax: Expected ',', found name\n   --> canonical_flow/G_aggregation_reporting/meso_aggregator.py:198:29\n    |\n197 | def _is_relevant_analysis_file(data: Dict[str, Any], document_stem: str, filename: str) -> bool:\n198 |     \"\"\"Check if an analysis file is relevant to the document stem.\"\"\"\n    |                             ^^^^\n199 |     # Check if document stem appears in the filename\n200 |     if document_stem.lower() in filename.lower():\n    |\n\ninvalid-syntax: Expected ',', found name\n   --> canonical_flow/G_aggregation_reporting/meso_aggregator.py:198:46\n    |\n197 | def _is_relevant_analysis_file(data: Dict[str, Any], document_stem: str, filename: str) -> bool:\n198 |     \"\"\"Check if an analysis file is relevant to the document stem.\"\"\"\n    |                                              ^^\n199 |     # Check if document stem appears in the filename\n200 |     if document_stem.lower() in filename.lower():\n    |\n\ninvalid-syntax: Expected ',', found name\n   --> canonical_flow/G_aggregation_reporting/meso_aggregator.py:198:49\n    |\n197 | def _is_relevant_analysis_file(data: Dict[str, Any], document_stem: str, filename: str) -> bool:\n198 |     \"\"\"Check if an analysis file is relevant to the document stem.\"\"\"\n    |                                                 ^^^\n199 |     # Check if document stem appears in the filename\n200 |     if document_stem.lower() in filename.lower():\n    |\n\ninvalid-syntax: Expected ',', found name\n   --> canonical_flow/G_aggregation_reporting/meso_aggregator.py:198:53\n    |\n197 | def _is_relevant_analysis_file(data: Dict[str, Any], document_stem: str, filename: str) -> bool:\n198 |     \"\"\"Check if an analysis file is relevant to the document stem.\"\"\"\n    |                                                     ^^^^^^^^\n199 |     # Check if document stem appears in the filename\n200 |     if document_stem.lower() in filename.lower():\n    |\n\ninvalid-syntax: Expected ',', found name\n   --> canonical_flow/G_aggregation_reporting/meso_aggregator.py:198:62\n    |\n197 | def _is_relevant_analysis_file(data: Dict[str, Any], document_stem: str, filename: str) -> bool:\n198 |     \"\"\"Check if an analysis file is relevant to the document stem.\"\"\"\n    |                                                              ^^^^\n199 |     # Check if document stem appears in the filename\n200 |     if document_stem.lower() in filename.lower():\n    |\n\ninvalid-syntax: Expected an identifier\n   --> canonical_flow/G_aggregation_reporting/meso_aggregator.py:198:67\n    |\n197 |   def _is_relevant_analysis_file(data: Dict[str, Any], document_stem: str, filename: str) -> bool:\n198 |       \"\"\"Check if an analysis file is relevant to the document stem.\"\"\"\n    |  ___________________________________________________________________^\n199 | |     # Check if document stem appears in the filename\n200 | |     if document_stem.lower() in filename.lower():\n201 | |         return True\n202 | |     \n203 | |     # Check if document stem appears in the data content\n204 | |     data_str = json.dumps(data, default=str).lower()\n205 | |     if document_stem.lower() in data_str:\n206 | |         return True\n207 | |     \n208 | |     # Accept validation and audit files as they contain general evidence\n209 | |     if any(keyword in filename.lower() for keyword in ['validation', 'audit', 'evidence']):\n210 | |         return True\n211 | |     \n212 | |     return False\n213 | |\n214 | |\n215 | | def _aggregate_evidence_by_dimensions(analysis_data: List[Dict[str, Any]]) -> Dict[str, Dict[str, Any]]:\n216 | |     \"\"\"Aggregate evidence by identified dimensions.\"\"\"\n    | |_______^\n217 |       dimensions = defaultdict(lambda: {\n218 |           \"evidence_items\": [],\n    |\n\ninvalid-syntax: Expected ',', found name\n   --> canonical_flow/G_aggregation_reporting/meso_aggregator.py:216:8\n    |\n215 | def _aggregate_evidence_by_dimensions(analysis_data: List[Dict[str, Any]]) -> Dict[str, Dict[str, Any]]:\n216 |     \"\"\"Aggregate evidence by identified dimensions.\"\"\"\n    |        ^^^^^^^^^\n217 |     dimensions = defaultdict(lambda: {\n218 |         \"evidence_items\": [],\n    |\n\ninvalid-syntax: Expected ',', found name\n   --> canonical_flow/G_aggregation_reporting/meso_aggregator.py:216:18\n    |\n215 | def _aggregate_evidence_by_dimensions(analysis_data: List[Dict[str, Any]]) -> Dict[str, Dict[str, Any]]:\n216 |     \"\"\"Aggregate evidence by identified dimensions.\"\"\"\n    |                  ^^^^^^^^\n217 |     dimensions = defaultdict(lambda: {\n218 |         \"evidence_items\": [],\n    |\n\ninvalid-syntax: Expected ',', found name\n   --> canonical_flow/G_aggregation_reporting/meso_aggregator.py:216:27\n    |\n215 | def _aggregate_evidence_by_dimensions(analysis_data: List[Dict[str, Any]]) -> Dict[str, Dict[str, Any]]:\n216 |     \"\"\"Aggregate evidence by identified dimensions.\"\"\"\n    |                           ^^\n217 |     dimensions = defaultdict(lambda: {\n218 |         \"evidence_items\": [],\n    |\n\ninvalid-syntax: Expected ',', found name\n   --> canonical_flow/G_aggregation_reporting/meso_aggregator.py:216:30\n    |\n215 | def _aggregate_evidence_by_dimensions(analysis_data: List[Dict[str, Any]]) -> Dict[str, Dict[str, Any]]:\n216 |     \"\"\"Aggregate evidence by identified dimensions.\"\"\"\n    |                              ^^^^^^^^^^\n217 |     dimensions = defaultdict(lambda: {\n218 |         \"evidence_items\": [],\n    |\n\ninvalid-syntax: Expected ',', found name\n   --> canonical_flow/G_aggregation_reporting/meso_aggregator.py:216:41\n    |\n215 | def _aggregate_evidence_by_dimensions(analysis_data: List[Dict[str, Any]]) -> Dict[str, Dict[str, Any]]:\n216 |     \"\"\"Aggregate evidence by identified dimensions.\"\"\"\n    |                                         ^^^^^^^^^^\n217 |     dimensions = defaultdict(lambda: {\n218 |         \"evidence_items\": [],\n    |\n\ninvalid-syntax: Expected an identifier\n   --> canonical_flow/G_aggregation_reporting/meso_aggregator.py:216:52\n    |\n215 |   def _aggregate_evidence_by_dimensions(analysis_data: List[Dict[str, Any]]) -> Dict[str, Dict[str, Any]]:\n216 |       \"\"\"Aggregate evidence by identified dimensions.\"\"\"\n    |  ____________________________________________________^\n217 | |     dimensions = defaultdict(lambda: {\n218 | |         \"evidence_items\": [],\n219 | |         \"evidence_types\": [],\n220 | |         \"validation_scores\": [],\n221 | |         \"dnp_scores\": [],\n222 | |         \"severity_levels\": [],\n223 | |         \"sources\": []\n224 | |     })\n225 | |     \n226 | |     for analysis_source in analysis_data:\n227 | |         data = analysis_source[\"data\"]\n228 | |         source_file = analysis_source[\"source_file\"]\n229 | |         \n230 | | # # #         # Extract evidence from different analysis file types  # Module not found  # Module not found  # Module not found\n231 | |         if \"results\" in data and isinstance(data[\"results\"], list):\n232 | |             # Evidence validation format\n233 | |             for result in data[\"results\"]:\n234 | |                 dimension = _extract_dimension_from_evidence(result, data)\n235 | |                 dimensions[dimension][\"evidence_items\"].append(result)\n236 | |                 dimensions[dimension][\"evidence_types\"].append(result.get(\"evidence_id\", \"unknown\"))\n237 | |                 dimensions[dimension][\"validation_scores\"].append(result.get(\"validation_score\", 0.0))\n238 | |                 dimensions[dimension][\"dnp_scores\"].append(result.get(\"dnp_compliance_score\", 0.0))\n239 | |                 dimensions[dimension][\"severity_levels\"].append(result.get(\"severity_level\", \"unknown\"))\n240 | |                 dimensions[dimension][\"sources\"].append(source_file)\n241 | |         \n242 | |         elif \"components\" in data:\n243 | | # # #             # Audit format - extract from component outputs  # Module not found  # Module not found  # Module not found\n244 | |             for comp_id, comp_data in data[\"components\"].items():\n245 | |                 dimension = f\"component_{comp_id}\"\n246 | |                 for event in comp_data.get(\"events\", []):\n247 | |                     if event.get(\"event_type\") == \"component_end\":\n248 | |                         output_schema = event.get(\"output_schema\", {})\n249 | |                         sample_data = output_schema.get(\"sample_data\", {})\n250 | |                         \n251 | |                         evidence_item = {\n252 | |                             \"evidence_id\": f\"{comp_id}_{event.get('event_id', 'unknown')}\",\n253 | |                             \"component_name\": comp_data.get(\"component_name\", \"unknown\"),\n254 | |                             \"output_data\": sample_data,\n255 | |                             \"performance_metrics\": event.get(\"performance_metrics\", {}),\n256 | |                             \"success\": event.get(\"context_data\", {}).get(\"success\", False)\n257 | |                         }\n258 | |                         \n259 | |                         dimensions[dimension][\"evidence_items\"].append(evidence_item)\n260 | |                         dimensions[dimension][\"evidence_types\"].append(\"component_output\")\n261 | |                         dimensions[dimension][\"validation_scores\"].append(1.0 if evidence_item[\"success\"] else 0.0)\n262 | |                         dimensions[dimension][\"dnp_scores\"].append(_calculate_dnp_score_from_metrics(\n263 | |                             evidence_item.get(\"performance_metrics\", {})\n264 | |                         ))\n265 | |                         dimensions[dimension][\"severity_levels\"].append(\"info\" if evidence_item[\"success\"] else \"warning\")\n266 | |                         dimensions[dimension][\"sources\"].append(source_file)\n267 | |     \n268 | |     # Convert defaultdict to regular dict and add metadata\n269 | |     result = {}\n270 | |     for dim_name, dim_data in dimensions.items():\n271 | |         result[dim_name] = {\n272 | |             \"dimension_name\": dim_name,\n273 | |             \"total_evidence\": len(dim_data[\"evidence_items\"]),\n274 | |             \"evidence_items\": dim_data[\"evidence_items\"],\n275 | |             \"evidence_types\": list(set(dim_data[\"evidence_types\"])),\n276 | |             \"validation_scores\": dim_data[\"validation_scores\"],\n277 | |             \"dnp_scores\": dim_data[\"dnp_scores\"],\n278 | |             \"severity_distribution\": dict(Counter(dim_data[\"severity_levels\"])),\n279 | |             \"unique_sources\": list(set(dim_data[\"sources\"]))\n280 | |         }\n281 | |     \n282 | |     return result\n283 | |\n284 | |\n285 | | def _extract_dimension_from_evidence(result: Dict[str, Any], data: Dict[str, Any]) -> str:\n286 | | # # #     \"\"\"Extract dimension from evidence result.\"\"\"  # Module not found  # Module not found  # Module not found\n    | |_____________^\n287 |   # # #     # Try to infer dimension from evidence content  # Module not found  # Module not found  # Module not found\n288 |       evidence_id = result.get(\"evidence_id\", \"\")\n    |\n\ninvalid-syntax: Expected ',', found name\n   --> canonical_flow/G_aggregation_reporting/meso_aggregator.py:286:14\n    |\n285 | def _extract_dimension_from_evidence(result: Dict[str, Any], data: Dict[str, Any]) -> str:\n286 | # # #     \"\"\"Extract dimension from evidence result.\"\"\"  # Module not found  # Module not found  # Module not found\n    |              ^^^^^^^\n287 | # # #     # Try to infer dimension from evidence content  # Module not found  # Module not found  # Module not found\n288 |     evidence_id = result.get(\"evidence_id\", \"\")\n    |\n\ninvalid-syntax: Expected ',', found name\n   --> canonical_flow/G_aggregation_reporting/meso_aggregator.py:286:22\n    |\n285 | def _extract_dimension_from_evidence(result: Dict[str, Any], data: Dict[str, Any]) -> str:\n286 | # # #     \"\"\"Extract dimension from evidence result.\"\"\"  # Module not found  # Module not found  # Module not found\n    |                      ^^^^^^^^^\n287 | # # #     # Try to infer dimension from evidence content  # Module not found  # Module not found  # Module not found\n288 |     evidence_id = result.get(\"evidence_id\", \"\")\n    |\n\ninvalid-syntax: Expected ':', found 'from'\n   --> canonical_flow/G_aggregation_reporting/meso_aggregator.py:286:32\n    |\n285 | def _extract_dimension_from_evidence(result: Dict[str, Any], data: Dict[str, Any]) -> str:\n286 | # # #     \"\"\"Extract dimension from evidence result.\"\"\"  # Module not found  # Module not found  # Module not found\n    |                                ^^^^\n287 | # # #     # Try to infer dimension from evidence content  # Module not found  # Module not found  # Module not found\n288 |     evidence_id = result.get(\"evidence_id\", \"\")\n    |\n\ninvalid-syntax: Expected 'import', found name\n   --> canonical_flow/G_aggregation_reporting/meso_aggregator.py:286:46\n    |\n285 | def _extract_dimension_from_evidence(result: Dict[str, Any], data: Dict[str, Any]) -> str:\n286 | # # #     \"\"\"Extract dimension from evidence result.\"\"\"  # Module not found  # Module not found  # Module not found\n    |                                              ^^^^^^\n287 | # # #     # Try to infer dimension from evidence content  # Module not found  # Module not found  # Module not found\n288 |     evidence_id = result.get(\"evidence_id\", \"\")\n    |\n\ninvalid-syntax: Expected ',', found '.'\n   --> canonical_flow/G_aggregation_reporting/meso_aggregator.py:286:52\n    |\n285 | def _extract_dimension_from_evidence(result: Dict[str, Any], data: Dict[str, Any]) -> str:\n286 | # # #     \"\"\"Extract dimension from evidence result.\"\"\"  # Module not found  # Module not found  # Module not found\n    |                                                    ^\n287 | # # #     # Try to infer dimension from evidence content  # Module not found  # Module not found  # Module not found\n288 |     evidence_id = result.get(\"evidence_id\", \"\")\n    |\n\ninvalid-syntax: Simple statements must be separated by newlines or semicolons\n   --> canonical_flow/G_aggregation_reporting/meso_aggregator.py:286:53\n    |\n285 |   def _extract_dimension_from_evidence(result: Dict[str, Any], data: Dict[str, Any]) -> str:\n286 |   # # #     \"\"\"Extract dimension from evidence result.\"\"\"  # Module not found  # Module not found  # Module not found\n    |  _____________________________________________________^\n287 | | # # #     # Try to infer dimension from evidence content  # Module not found  # Module not found  # Module not found\n288 | |     evidence_id = result.get(\"evidence_id\", \"\")\n289 | |     \n290 | |     # Check for common dimension keywords\n291 | |     dimension_keywords = {\n292 | |         \"transparency\": [\"transparent\", \"disclosure\", \"public\", \"open\"],\n293 | |         \"governance\": [\"govern\", \"administration\", \"management\", \"leadership\"],\n294 | |         \"compliance\": [\"compliance\", \"regulation\", \"standard\", \"rule\"],\n295 | |         \"performance\": [\"performance\", \"efficiency\", \"effectiveness\", \"metric\"],\n296 | |         \"accountability\": [\"account\", \"responsible\", \"oversight\", \"audit\"]\n297 | |     }\n298 | |     \n299 | |     evidence_text = json.dumps(result, default=str).lower()\n300 | |     \n301 | |     for dimension, keywords in dimension_keywords.items():\n302 | |         if any(keyword in evidence_text for keyword in keywords):\n303 | |             return dimension\n304 | |     \n305 | |     # Default dimension based on evidence type or ID\n306 | |     if \"validation\" in evidence_id.lower():\n307 | |         return \"validation_compliance\"\n308 | |     elif \"component\" in evidence_id.lower():\n309 | |         return \"system_performance\"\n310 | |     else:\n311 | |         return \"general_evidence\"\n312 | |\n313 | |\n314 | | def _calculate_coverage_metrics(aggregated_evidence: Dict[str, Dict[str, Any]], \n315 | |                                analysis_data: List[Dict[str, Any]]) -> Dict[str, float]:\n316 | |     \"\"\"Calculate coverage percentage for each dimension.\"\"\"\n    | |_______^\n317 |       coverage_metrics = {}\n    |\n\ninvalid-syntax: Simple statements must be separated by newlines or semicolons\n   --> canonical_flow/G_aggregation_reporting/meso_aggregator.py:316:8\n    |\n314 | def _calculate_coverage_metrics(aggregated_evidence: Dict[str, Dict[str, Any]], \n315 |                                analysis_data: List[Dict[str, Any]]) -> Dict[str, float]:\n316 |     \"\"\"Calculate coverage percentage for each dimension.\"\"\"\n    |        ^^^^^^^^^\n317 |     coverage_metrics = {}\n    |\n\ninvalid-syntax: Simple statements must be separated by newlines or semicolons\n   --> canonical_flow/G_aggregation_reporting/meso_aggregator.py:316:18\n    |\n314 | def _calculate_coverage_metrics(aggregated_evidence: Dict[str, Dict[str, Any]], \n315 |                                analysis_data: List[Dict[str, Any]]) -> Dict[str, float]:\n316 |     \"\"\"Calculate coverage percentage for each dimension.\"\"\"\n    |                  ^^^^^^^^\n317 |     coverage_metrics = {}\n    |\n\ninvalid-syntax: Simple statements must be separated by newlines or semicolons\n   --> canonical_flow/G_aggregation_reporting/meso_aggregator.py:316:27\n    |\n314 | def _calculate_coverage_metrics(aggregated_evidence: Dict[str, Dict[str, Any]], \n315 |                                analysis_data: List[Dict[str, Any]]) -> Dict[str, float]:\n316 |     \"\"\"Calculate coverage percentage for each dimension.\"\"\"\n    |                           ^^^^^^^^^^\n317 |     coverage_metrics = {}\n    |\n\ninvalid-syntax: Compound statements are not allowed on the same line as simple statements\n   --> canonical_flow/G_aggregation_reporting/meso_aggregator.py:316:38\n    |\n314 | def _calculate_coverage_metrics(aggregated_evidence: Dict[str, Dict[str, Any]], \n315 |                                analysis_data: List[Dict[str, Any]]) -> Dict[str, float]:\n316 |     \"\"\"Calculate coverage percentage for each dimension.\"\"\"\n    |                                      ^^^\n317 |     coverage_metrics = {}\n    |\n\ninvalid-syntax: Expected 'in', found name\n   --> canonical_flow/G_aggregation_reporting/meso_aggregator.py:316:47\n    |\n314 | def _calculate_coverage_metrics(aggregated_evidence: Dict[str, Dict[str, Any]], \n315 |                                analysis_data: List[Dict[str, Any]]) -> Dict[str, float]:\n316 |     \"\"\"Calculate coverage percentage for each dimension.\"\"\"\n    |                                               ^^^^^^^^^\n317 |     coverage_metrics = {}\n    |\n\ninvalid-syntax: Expected an identifier\n   --> canonical_flow/G_aggregation_reporting/meso_aggregator.py:316:57\n    |\n314 |   def _calculate_coverage_metrics(aggregated_evidence: Dict[str, Dict[str, Any]], \n315 |                                  analysis_data: List[Dict[str, Any]]) -> Dict[str, float]:\n316 |       \"\"\"Calculate coverage percentage for each dimension.\"\"\"\n    |  _________________________________________________________^\n317 | |     coverage_metrics = {}\n318 | |     \n319 | |     total_sources = len(analysis_data)\n320 | |     if total_sources == 0:\n321 | |         return coverage_metrics\n322 | |     \n323 | |     for dimension, dim_data in aggregated_evidence.items():\n324 | |         unique_sources = len(dim_data.get(\"unique_sources\", []))\n325 | |         coverage_percentage = (unique_sources / total_sources) * 100.0\n326 | |         coverage_metrics[dimension] = round(coverage_percentage, 2)\n327 | |     \n328 | |     return coverage_metrics\n329 | |\n330 | |\n331 | | def _calculate_divergence_scores(aggregated_evidence: Dict[str, Dict[str, Any]]) -> Dict[str, float]:\n332 | |     \"\"\"Calculate divergence scores between evidence clusters.\"\"\"\n    | |_______^\n333 |       divergence_scores = {}\n334 |       dimensions = list(aggregated_evidence.keys())\n    |\n\ninvalid-syntax: Simple statements must be separated by newlines or semicolons\n   --> canonical_flow/G_aggregation_reporting/meso_aggregator.py:332:8\n    |\n331 | def _calculate_divergence_scores(aggregated_evidence: Dict[str, Dict[str, Any]]) -> Dict[str, float]:\n332 |     \"\"\"Calculate divergence scores between evidence clusters.\"\"\"\n    |        ^^^^^^^^^\n333 |     divergence_scores = {}\n334 |     dimensions = list(aggregated_evidence.keys())\n    |\n\ninvalid-syntax: Simple statements must be separated by newlines or semicolons\n   --> canonical_flow/G_aggregation_reporting/meso_aggregator.py:332:18\n    |\n331 | def _calculate_divergence_scores(aggregated_evidence: Dict[str, Dict[str, Any]]) -> Dict[str, float]:\n332 |     \"\"\"Calculate divergence scores between evidence clusters.\"\"\"\n    |                  ^^^^^^^^^^\n333 |     divergence_scores = {}\n334 |     dimensions = list(aggregated_evidence.keys())\n    |\n\ninvalid-syntax: Simple statements must be separated by newlines or semicolons\n   --> canonical_flow/G_aggregation_reporting/meso_aggregator.py:332:29\n    |\n331 | def _calculate_divergence_scores(aggregated_evidence: Dict[str, Dict[str, Any]]) -> Dict[str, float]:\n332 |     \"\"\"Calculate divergence scores between evidence clusters.\"\"\"\n    |                             ^^^^^^\n333 |     divergence_scores = {}\n334 |     dimensions = list(aggregated_evidence.keys())\n    |\n\ninvalid-syntax: Simple statements must be separated by newlines or semicolons\n   --> canonical_flow/G_aggregation_reporting/meso_aggregator.py:332:36\n    |\n331 | def _calculate_divergence_scores(aggregated_evidence: Dict[str, Dict[str, Any]]) -> Dict[str, float]:\n332 |     \"\"\"Calculate divergence scores between evidence clusters.\"\"\"\n    |                                    ^^^^^^^\n333 |     divergence_scores = {}\n334 |     dimensions = list(aggregated_evidence.keys())\n    |\n\ninvalid-syntax: Simple statements must be separated by newlines or semicolons\n   --> canonical_flow/G_aggregation_reporting/meso_aggregator.py:332:44\n    |\n331 | def _calculate_divergence_scores(aggregated_evidence: Dict[str, Dict[str, Any]]) -> Dict[str, float]:\n332 |     \"\"\"Calculate divergence scores between evidence clusters.\"\"\"\n    |                                            ^^^^^^^^\n333 |     divergence_scores = {}\n334 |     dimensions = list(aggregated_evidence.keys())\n    |\n\ninvalid-syntax: Simple statements must be separated by newlines or semicolons\n   --> canonical_flow/G_aggregation_reporting/meso_aggregator.py:332:53\n    |\n331 | def _calculate_divergence_scores(aggregated_evidence: Dict[str, Dict[str, Any]]) -> Dict[str, float]:\n332 |     \"\"\"Calculate divergence scores between evidence clusters.\"\"\"\n    |                                                     ^^^^^^^^\n333 |     divergence_scores = {}\n334 |     dimensions = list(aggregated_evidence.keys())\n    |\n\ninvalid-syntax: Expected an identifier\n   --> canonical_flow/G_aggregation_reporting/meso_aggregator.py:332:62\n    |\n331 |   def _calculate_divergence_scores(aggregated_evidence: Dict[str, Dict[str, Any]]) -> Dict[str, float]:\n332 |       \"\"\"Calculate divergence scores between evidence clusters.\"\"\"\n    |  ______________________________________________________________^\n333 | |     divergence_scores = {}\n334 | |     dimensions = list(aggregated_evidence.keys())\n335 | |     \n336 | |     for i, dim1 in enumerate(dimensions):\n337 | |         for j, dim2 in enumerate(dimensions[i+1:], i+1):\n338 | |             # Calculate divergence between two dimensions\n339 | |             scores1 = aggregated_evidence[dim1].get(\"validation_scores\", [])\n340 | |             scores2 = aggregated_evidence[dim2].get(\"validation_scores\", [])\n341 | |             \n342 | |             if scores1 and scores2:\n343 | |                 # Use coefficient of variation as divergence measure\n344 | |                 mean1, std1 = np.mean(scores1), np.std(scores1)\n345 | |                 mean2, std2 = np.mean(scores2), np.std(scores2)\n346 | |                 \n347 | |                 # Calculate divergence as difference in coefficient of variation\n348 | |                 cv1 = std1 / mean1 if mean1 != 0 else 0\n349 | |                 cv2 = std2 / mean2 if mean2 != 0 else 0\n350 | |                 divergence = abs(cv1 - cv2)\n351 | |                 \n352 | |                 divergence_scores[f\"{dim1}_vs_{dim2}\"] = round(divergence, 4)\n353 | |     \n354 | |     return divergence_scores\n355 | |\n356 | |\n357 | | def _calculate_participation_metrics(aggregated_evidence: Dict[str, Dict[str, Any]], \n358 | |                                    analysis_data: List[Dict[str, Any]]) -> Dict[str, Dict[str, Any]]:\n359 | |     \"\"\"Calculate participation metrics showing how evidence contributes to thematic groups.\"\"\"\n    | |_______^\n360 |       participation_metrics = {}\n    |\n\ninvalid-syntax: Simple statements must be separated by newlines or semicolons\n   --> canonical_flow/G_aggregation_reporting/meso_aggregator.py:359:8\n    |\n357 | def _calculate_participation_metrics(aggregated_evidence: Dict[str, Dict[str, Any]], \n358 |                                    analysis_data: List[Dict[str, Any]]) -> Dict[str, Dict[str, Any]]:\n359 |     \"\"\"Calculate participation metrics showing how evidence contributes to thematic groups.\"\"\"\n    |        ^^^^^^^^^\n360 |     participation_metrics = {}\n    |\n\ninvalid-syntax: Simple statements must be separated by newlines or semicolons\n   --> canonical_flow/G_aggregation_reporting/meso_aggregator.py:359:18\n    |\n357 | def _calculate_participation_metrics(aggregated_evidence: Dict[str, Dict[str, Any]], \n358 |                                    analysis_data: List[Dict[str, Any]]) -> Dict[str, Dict[str, Any]]:\n359 |     \"\"\"Calculate participation metrics showing how evidence contributes to thematic groups.\"\"\"\n    |                  ^^^^^^^^^^^^^\n360 |     participation_metrics = {}\n    |\n\ninvalid-syntax: Simple statements must be separated by newlines or semicolons\n   --> canonical_flow/G_aggregation_reporting/meso_aggregator.py:359:32\n    |\n357 | def _calculate_participation_metrics(aggregated_evidence: Dict[str, Dict[str, Any]], \n358 |                                    analysis_data: List[Dict[str, Any]]) -> Dict[str, Dict[str, Any]]:\n359 |     \"\"\"Calculate participation metrics showing how evidence contributes to thematic groups.\"\"\"\n    |                                ^^^^^^^\n360 |     participation_metrics = {}\n    |\n\ninvalid-syntax: Simple statements must be separated by newlines or semicolons\n   --> canonical_flow/G_aggregation_reporting/meso_aggregator.py:359:40\n    |\n357 | def _calculate_participation_metrics(aggregated_evidence: Dict[str, Dict[str, Any]], \n358 |                                    analysis_data: List[Dict[str, Any]]) -> Dict[str, Dict[str, Any]]:\n359 |     \"\"\"Calculate participation metrics showing how evidence contributes to thematic groups.\"\"\"\n    |                                        ^^^^^^^\n360 |     participation_metrics = {}\n    |\n\ninvalid-syntax: Simple statements must be separated by newlines or semicolons\n   --> canonical_flow/G_aggregation_reporting/meso_aggregator.py:359:48\n    |\n357 | def _calculate_participation_metrics(aggregated_evidence: Dict[str, Dict[str, Any]], \n358 |                                    analysis_data: List[Dict[str, Any]]) -> Dict[str, Dict[str, Any]]:\n359 |     \"\"\"Calculate participation metrics showing how evidence contributes to thematic groups.\"\"\"\n    |                                                ^^^\n360 |     participation_metrics = {}\n    |\n\ninvalid-syntax: Simple statements must be separated by newlines or semicolons\n   --> canonical_flow/G_aggregation_reporting/meso_aggregator.py:359:52\n    |\n357 | def _calculate_participation_metrics(aggregated_evidence: Dict[str, Dict[str, Any]], \n358 |                                    analysis_data: List[Dict[str, Any]]) -> Dict[str, Dict[str, Any]]:\n359 |     \"\"\"Calculate participation metrics showing how evidence contributes to thematic groups.\"\"\"\n    |                                                    ^^^^^^^^\n360 |     participation_metrics = {}\n    |\n\ninvalid-syntax: Simple statements must be separated by newlines or semicolons\n   --> canonical_flow/G_aggregation_reporting/meso_aggregator.py:359:61\n    |\n357 | def _calculate_participation_metrics(aggregated_evidence: Dict[str, Dict[str, Any]], \n358 |                                    analysis_data: List[Dict[str, Any]]) -> Dict[str, Dict[str, Any]]:\n359 |     \"\"\"Calculate participation metrics showing how evidence contributes to thematic groups.\"\"\"\n    |                                                             ^^^^^^^^^^^\n360 |     participation_metrics = {}\n    |\n\ninvalid-syntax: Simple statements must be separated by newlines or semicolons\n   --> canonical_flow/G_aggregation_reporting/meso_aggregator.py:359:73\n    |\n357 | def _calculate_participation_metrics(aggregated_evidence: Dict[str, Dict[str, Any]], \n358 |                                    analysis_data: List[Dict[str, Any]]) -> Dict[str, Dict[str, Any]]:\n359 |     \"\"\"Calculate participation metrics showing how evidence contributes to thematic groups.\"\"\"\n    |                                                                         ^^\n360 |     participation_metrics = {}\n    |\n\ninvalid-syntax: Simple statements must be separated by newlines or semicolons\n   --> canonical_flow/G_aggregation_reporting/meso_aggregator.py:359:76\n    |\n357 | def _calculate_participation_metrics(aggregated_evidence: Dict[str, Dict[str, Any]], \n358 |                                    analysis_data: List[Dict[str, Any]]) -> Dict[str, Dict[str, Any]]:\n359 |     \"\"\"Calculate participation metrics showing how evidence contributes to thematic groups.\"\"\"\n    |                                                                            ^^^^^^^^\n360 |     participation_metrics = {}\n    |\n\ninvalid-syntax: Simple statements must be separated by newlines or semicolons\n   --> canonical_flow/G_aggregation_reporting/meso_aggregator.py:359:85\n    |\n357 | def _calculate_participation_metrics(aggregated_evidence: Dict[str, Dict[str, Any]], \n358 |                                    analysis_data: List[Dict[str, Any]]) -> Dict[str, Dict[str, Any]]:\n359 |     \"\"\"Calculate participation metrics showing how evidence contributes to thematic groups.\"\"\"\n    |                                                                                     ^^^^^^\n360 |     participation_metrics = {}\n    |\n\ninvalid-syntax: Expected an identifier\n   --> canonical_flow/G_aggregation_reporting/meso_aggregator.py:359:92\n    |\n357 |   def _calculate_participation_metrics(aggregated_evidence: Dict[str, Dict[str, Any]], \n358 |                                      analysis_data: List[Dict[str, Any]]) -> Dict[str, Dict[str, Any]]:\n359 |       \"\"\"Calculate participation metrics showing how evidence contributes to thematic groups.\"\"\"\n    |  ____________________________________________________________________________________________^\n360 | |     participation_metrics = {}\n361 | |     \n362 | |     # Define thematic groups based on evidence characteristics\n363 | |     thematic_groups = {\n364 | |         \"high_quality\": {\"validation_threshold\": 0.7, \"dnp_threshold\": 0.8},\n365 | |         \"moderate_quality\": {\"validation_threshold\": 0.4, \"dnp_threshold\": 0.5},\n366 | |         \"improvement_needed\": {\"validation_threshold\": 0.0, \"dnp_threshold\": 0.0}\n367 | |     }\n368 | |     \n369 | |     for dimension, dim_data in aggregated_evidence.items():\n370 | |         dimension_metrics = {}\n371 | |         validation_scores = dim_data.get(\"validation_scores\", [])\n372 | |         dnp_scores = dim_data.get(\"dnp_scores\", [])\n373 | |         total_evidence = len(validation_scores)\n374 | |         \n375 | |         if total_evidence == 0:\n376 | |             dimension_metrics = {group: {\"count\": 0, \"percentage\": 0.0} for group in thematic_groups}\n377 | |         else:\n378 | |             for group_name, thresholds in thematic_groups.items():\n379 | |                 # Count evidence meeting group criteria\n380 | |                 count = 0\n381 | |                 for val_score, dnp_score in zip(validation_scores, dnp_scores):\n382 | |                     if (val_score >= thresholds[\"validation_threshold\"] and \n383 | |                         dnp_score >= thresholds[\"dnp_threshold\"]):\n384 | |                         count += 1\n385 | |                 \n386 | |                 percentage = (count / total_evidence) * 100.0\n387 | |                 dimension_metrics[group_name] = {\n388 | |                     \"count\": count,\n389 | |                     \"percentage\": round(percentage, 2)\n390 | |                 }\n391 | |         \n392 | |         participation_metrics[dimension] = dimension_metrics\n393 | |     \n394 | |     return participation_metrics\n395 | |\n396 | |\n397 | | def _calculate_dnp_score_from_metrics(performance_metrics: Dict[str, Any]) -> float:\n398 | | # # #     \"\"\"Calculate a DNP-style score from performance metrics.\"\"\"  # Module not found  # Module not found  # Module not found\n    | |_____________^\n399 |       if not performance_metrics:\n400 |           return 0.5  # Neutral score\n    |\n\ninvalid-syntax: Simple statements must be separated by newlines or semicolons\n   --> canonical_flow/G_aggregation_reporting/meso_aggregator.py:398:14\n    |\n397 | def _calculate_dnp_score_from_metrics(performance_metrics: Dict[str, Any]) -> float:\n398 | # # #     \"\"\"Calculate a DNP-style score from performance metrics.\"\"\"  # Module not found  # Module not found  # Module not found\n    |              ^^^^^^^^^\n399 |     if not performance_metrics:\n400 |         return 0.5  # Neutral score\n    |\n\ninvalid-syntax: Simple statements must be separated by newlines or semicolons\n   --> canonical_flow/G_aggregation_reporting/meso_aggregator.py:398:24\n    |\n397 | def _calculate_dnp_score_from_metrics(performance_metrics: Dict[str, Any]) -> float:\n398 | # # #     \"\"\"Calculate a DNP-style score from performance metrics.\"\"\"  # Module not found  # Module not found  # Module not found\n    |                        ^\n399 |     if not performance_metrics:\n400 |         return 0.5  # Neutral score\n    |\n\ninvalid-syntax: Simple statements must be separated by newlines or semicolons\n   --> canonical_flow/G_aggregation_reporting/meso_aggregator.py:398:26\n    |\n397 | def _calculate_dnp_score_from_metrics(performance_metrics: Dict[str, Any]) -> float:\n398 | # # #     \"\"\"Calculate a DNP-style score from performance metrics.\"\"\"  # Module not found  # Module not found  # Module not found\n    |                          ^^^\n399 |     if not performance_metrics:\n400 |         return 0.5  # Neutral score\n    |\n\ninvalid-syntax: Simple statements must be separated by newlines or semicolons\n   --> canonical_flow/G_aggregation_reporting/meso_aggregator.py:398:36\n    |\n397 | def _calculate_dnp_score_from_metrics(performance_metrics: Dict[str, Any]) -> float:\n398 | # # #     \"\"\"Calculate a DNP-style score from performance metrics.\"\"\"  # Module not found  # Module not found  # Module not found\n    |                                    ^^^^^\n399 |     if not performance_metrics:\n400 |         return 0.5  # Neutral score\n    |\n\ninvalid-syntax: Simple statements must be separated by newlines or semicolons\n   --> canonical_flow/G_aggregation_reporting/meso_aggregator.py:398:42\n    |\n397 | def _calculate_dnp_score_from_metrics(performance_metrics: Dict[str, Any]) -> float:\n398 | # # #     \"\"\"Calculate a DNP-style score from performance metrics.\"\"\"  # Module not found  # Module not found  # Module not found\n    |                                          ^^^^\n399 |     if not performance_metrics:\n400 |         return 0.5  # Neutral score\n    |\n\ninvalid-syntax: Expected 'import', found name\n   --> canonical_flow/G_aggregation_reporting/meso_aggregator.py:398:59\n    |\n397 | def _calculate_dnp_score_from_metrics(performance_metrics: Dict[str, Any]) -> float:\n398 | # # #     \"\"\"Calculate a DNP-style score from performance metrics.\"\"\"  # Module not found  # Module not found  # Module not found\n    |                                                           ^^^^^^^\n399 |     if not performance_metrics:\n400 |         return 0.5  # Neutral score\n    |\n\ninvalid-syntax: Expected ',', found '.'\n   --> canonical_flow/G_aggregation_reporting/meso_aggregator.py:398:66\n    |\n397 | def _calculate_dnp_score_from_metrics(performance_metrics: Dict[str, Any]) -> float:\n398 | # # #     \"\"\"Calculate a DNP-style score from performance metrics.\"\"\"  # Module not found  # Module not found  # Module not found\n    |                                                                  ^\n399 |     if not performance_metrics:\n400 |         return 0.5  # Neutral score\n    |\n\ninvalid-syntax: missing closing quote in string literal\n   --> canonical_flow/G_aggregation_reporting/meso_aggregator.py:398:67\n    |\n397 |   def _calculate_dnp_score_from_metrics(performance_metrics: Dict[str, Any]) -> float:\n398 |   # # #     \"\"\"Calculate a DNP-style score from performance metrics.\"\"\"  # Module not found  # Module not found  # Module not found\n    |  ___________________________________________________________________^\n399 | |     if not performance_metrics:\n400 | |         return 0.5  # Neutral score\n401 | |     \n402 | |     # Simple heuristic: good performance = high DNP score\n403 | |     duration = performance_metrics.get(\"duration_seconds\", 1.0)\n404 | |     cpu_percent = performance_metrics.get(\"cpu_percent\", 50.0)\n405 | |     \n406 | |     # Normalize metrics (lower duration and CPU = higher score)\n407 | |     duration_score = max(0, 1.0 - min(duration / 10.0, 1.0))  # Cap at 10 seconds\n408 | |     cpu_score = max(0, 1.0 - min(cpu_percent / 100.0, 1.0))  # Normalize CPU\n409 | |     \n410 | |     return (duration_score + cpu_score) / 2.0\n411 | |\n412 | |\n413 | | if __name__ == \"__main__\":\n414 | |     if len(sys.argv) != 2:\n415 | |         print(\"Usage: python meso_aggregator.py <document_stem>\")\n416 | |         sys.exit(1)\n417 | |     \n418 | |     document_stem = sys.argv[1]\n419 | |     result = process(document_stem)\n420 | |     \n421 | |     print(json.dumps(result, indent=2, ensure_ascii=False))\n    | |___________________________________________________________^\n    |\n\ninvalid-syntax: Expected a statement\n   --> canonical_flow/G_aggregation_reporting/meso_aggregator.py:421:60\n    |\n419 |     result = process(document_stem)\n420 |     \n421 |     print(json.dumps(result, indent=2, ensure_ascii=False))\n    |                                                            ^\n    |\n\nF821 Undefined name `Dict`\n  --> canonical_flow/G_aggregation_reporting/report_compiler.py:52:37\n   |\n52 | def create_deterministic_dict(data: Dict[str, Any]) -> Dict[str, Any]:\n   |                                     ^^^^\n53 |     \"\"\"Create dictionary with deterministic field ordering.\"\"\"\n54 |     if not isinstance(data, dict):\n   |\n\nF821 Undefined name `Any`\n  --> canonical_flow/G_aggregation_reporting/report_compiler.py:52:47\n   |\n52 | def create_deterministic_dict(data: Dict[str, Any]) -> Dict[str, Any]:\n   |                                               ^^^\n53 |     \"\"\"Create dictionary with deterministic field ordering.\"\"\"\n54 |     if not isinstance(data, dict):\n   |\n\nF821 Undefined name `Dict`\n  --> canonical_flow/G_aggregation_reporting/report_compiler.py:52:56\n   |\n52 | def create_deterministic_dict(data: Dict[str, Any]) -> Dict[str, Any]:\n   |                                                        ^^^^\n53 |     \"\"\"Create dictionary with deterministic field ordering.\"\"\"\n54 |     if not isinstance(data, dict):\n   |\n\nF821 Undefined name `Any`\n  --> canonical_flow/G_aggregation_reporting/report_compiler.py:52:66\n   |\n52 | def create_deterministic_dict(data: Dict[str, Any]) -> Dict[str, Any]:\n   |                                                                  ^^^\n53 |     \"\"\"Create dictionary with deterministic field ordering.\"\"\"\n54 |     if not isinstance(data, dict):\n   |\n\nF821 Undefined name `Dict`\n  --> canonical_flow/G_aggregation_reporting/report_compiler.py:61:27\n   |\n61 | def safe_get_nested(data: Dict[str, Any], *keys, default=None) -> Any:\n   |                           ^^^^\n62 |     \"\"\"Safely get nested dictionary values.\"\"\"\n63 |     current = data\n   |\n\nF821 Undefined name `Any`\n  --> canonical_flow/G_aggregation_reporting/report_compiler.py:61:37\n   |\n61 | def safe_get_nested(data: Dict[str, Any], *keys, default=None) -> Any:\n   |                                     ^^^\n62 |     \"\"\"Safely get nested dictionary values.\"\"\"\n63 |     current = data\n   |\n\nF821 Undefined name `Any`\n  --> canonical_flow/G_aggregation_reporting/report_compiler.py:61:67\n   |\n61 | def safe_get_nested(data: Dict[str, Any], *keys, default=None) -> Any:\n   |                                                                   ^^^\n62 |     \"\"\"Safely get nested dictionary values.\"\"\"\n63 |     current = data\n   |\n\nF821 Undefined name `Dict`\n  --> canonical_flow/G_aggregation_reporting/report_compiler.py:72:43\n   |\n72 | def generate_executive_summary(meso_data: Dict[str, Any]) -> Dict[str, Any]:\n   |                                           ^^^^\n73 | # # #     \"\"\"Generate executive summary from meso aggregation data.\"\"\"  # Module not found  # Module not found  # Module not found\n74 |     try:\n   |\n\nF821 Undefined name `Any`\n  --> canonical_flow/G_aggregation_reporting/report_compiler.py:72:53\n   |\n72 | def generate_executive_summary(meso_data: Dict[str, Any]) -> Dict[str, Any]:\n   |                                                     ^^^\n73 | # # #     \"\"\"Generate executive summary from meso aggregation data.\"\"\"  # Module not found  # Module not found  # Module not found\n74 |     try:\n   |\n\nF821 Undefined name `Dict`\n  --> canonical_flow/G_aggregation_reporting/report_compiler.py:72:62\n   |\n72 | def generate_executive_summary(meso_data: Dict[str, Any]) -> Dict[str, Any]:\n   |                                                              ^^^^\n73 | # # #     \"\"\"Generate executive summary from meso aggregation data.\"\"\"  # Module not found  # Module not found  # Module not found\n74 |     try:\n   |\n\nF821 Undefined name `Any`\n  --> canonical_flow/G_aggregation_reporting/report_compiler.py:72:72\n   |\n72 | def generate_executive_summary(meso_data: Dict[str, Any]) -> Dict[str, Any]:\n   |                                                                        ^^^\n73 | # # #     \"\"\"Generate executive summary from meso aggregation data.\"\"\"  # Module not found  # Module not found  # Module not found\n74 |     try:\n   |\n\nF841 Local variable `coverage_matrix` is assigned to but never used\n  --> canonical_flow/G_aggregation_reporting/report_compiler.py:76:9\n   |\n74 |     try:\n75 |         meso_summary = meso_data.get(\"meso_summary\", {})\n76 |         coverage_matrix = meso_data.get(\"coverage_matrix\", {})\n   |         ^^^^^^^^^^^^^^^\n77 |         \n78 |         # Extract key metrics\n   |\nhelp: Remove assignment to unused variable `coverage_matrix`\n\nF821 Undefined name `Dict`\n   --> canonical_flow/G_aggregation_reporting/report_compiler.py:135:42\n    |\n135 | def generate_findings_section(meso_data: Dict[str, Any]) -> Dict[str, Any]:\n    |                                          ^^^^\n136 |     \"\"\"Generate detailed findings with evidence references.\"\"\"\n137 |     try:\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/G_aggregation_reporting/report_compiler.py:135:52\n    |\n135 | def generate_findings_section(meso_data: Dict[str, Any]) -> Dict[str, Any]:\n    |                                                    ^^^\n136 |     \"\"\"Generate detailed findings with evidence references.\"\"\"\n137 |     try:\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/G_aggregation_reporting/report_compiler.py:135:61\n    |\n135 | def generate_findings_section(meso_data: Dict[str, Any]) -> Dict[str, Any]:\n    |                                                             ^^^^\n136 |     \"\"\"Generate detailed findings with evidence references.\"\"\"\n137 |     try:\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/G_aggregation_reporting/report_compiler.py:135:71\n    |\n135 | def generate_findings_section(meso_data: Dict[str, Any]) -> Dict[str, Any]:\n    |                                                                       ^^^\n136 |     \"\"\"Generate detailed findings with evidence references.\"\"\"\n137 |     try:\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/G_aggregation_reporting/report_compiler.py:199:47\n    |\n199 | def generate_compliance_assessment(meso_data: Dict[str, Any]) -> Dict[str, Any]:\n    |                                               ^^^^\n200 |     \"\"\"Generate compliance assessment metrics.\"\"\"\n201 |     try:\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/G_aggregation_reporting/report_compiler.py:199:57\n    |\n199 | def generate_compliance_assessment(meso_data: Dict[str, Any]) -> Dict[str, Any]:\n    |                                                         ^^^\n200 |     \"\"\"Generate compliance assessment metrics.\"\"\"\n201 |     try:\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/G_aggregation_reporting/report_compiler.py:199:66\n    |\n199 | def generate_compliance_assessment(meso_data: Dict[str, Any]) -> Dict[str, Any]:\n    |                                                                  ^^^^\n200 |     \"\"\"Generate compliance assessment metrics.\"\"\"\n201 |     try:\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/G_aggregation_reporting/report_compiler.py:199:76\n    |\n199 | def generate_compliance_assessment(meso_data: Dict[str, Any]) -> Dict[str, Any]:\n    |                                                                            ^^^\n200 |     \"\"\"Generate compliance assessment metrics.\"\"\"\n201 |     try:\n    |\n\nF841 Local variable `coverage_matrix` is assigned to but never used\n   --> canonical_flow/G_aggregation_reporting/report_compiler.py:202:9\n    |\n200 |     \"\"\"Generate compliance assessment metrics.\"\"\"\n201 |     try:\n202 |         coverage_matrix = meso_data.get(\"coverage_matrix\", {})\n    |         ^^^^^^^^^^^^^^^\n203 |         coverage_summary = safe_get_nested(meso_data, \"meso_summary\", \"component_coverage_summary\", default={})\n    |\nhelp: Remove assignment to unused variable `coverage_matrix`\n\nF821 Undefined name `Dict`\n   --> canonical_flow/G_aggregation_reporting/report_compiler.py:267:40\n    |\n267 | def generate_key_highlights(meso_data: Dict[str, Any]) -> Dict[str, Any]:\n    |                                        ^^^^\n268 | # # #     \"\"\"Generate key highlights from the analysis.\"\"\"  # Module not found  # Module not found  # Module not found\n269 |     try:\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/G_aggregation_reporting/report_compiler.py:267:50\n    |\n267 | def generate_key_highlights(meso_data: Dict[str, Any]) -> Dict[str, Any]:\n    |                                                  ^^^\n268 | # # #     \"\"\"Generate key highlights from the analysis.\"\"\"  # Module not found  # Module not found  # Module not found\n269 |     try:\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/G_aggregation_reporting/report_compiler.py:267:59\n    |\n267 | def generate_key_highlights(meso_data: Dict[str, Any]) -> Dict[str, Any]:\n    |                                                           ^^^^\n268 | # # #     \"\"\"Generate key highlights from the analysis.\"\"\"  # Module not found  # Module not found  # Module not found\n269 |     try:\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/G_aggregation_reporting/report_compiler.py:267:69\n    |\n267 | def generate_key_highlights(meso_data: Dict[str, Any]) -> Dict[str, Any]:\n    |                                                                     ^^^\n268 | # # #     \"\"\"Generate key highlights from the analysis.\"\"\"  # Module not found  # Module not found  # Module not found\n269 |     try:\n    |\n\nF841 Local variable `coverage_matrix` is assigned to but never used\n   --> canonical_flow/G_aggregation_reporting/report_compiler.py:271:9\n    |\n269 |     try:\n270 |         meso_summary = meso_data.get(\"meso_summary\", {})\n271 |         coverage_matrix = meso_data.get(\"coverage_matrix\", {})\n    |         ^^^^^^^^^^^^^^^\n272 |         \n273 |         # Identify well-covered components\n    |\nhelp: Remove assignment to unused variable `coverage_matrix`\n\nF821 Undefined name `Optional`\n   --> canonical_flow/G_aggregation_reporting/report_compiler.py:351:42\n    |\n351 | def process(document_stem: str, context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:\n    |                                          ^^^^^^^^\n352 |     \"\"\"\n353 |     Main processing function that generates structured narrative reports with canonicalization.\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/G_aggregation_reporting/report_compiler.py:351:51\n    |\n351 | def process(document_stem: str, context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:\n    |                                                   ^^^^\n352 |     \"\"\"\n353 |     Main processing function that generates structured narrative reports with canonicalization.\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/G_aggregation_reporting/report_compiler.py:351:61\n    |\n351 | def process(document_stem: str, context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:\n    |                                                             ^^^\n352 |     \"\"\"\n353 |     Main processing function that generates structured narrative reports with canonicalization.\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/G_aggregation_reporting/report_compiler.py:351:78\n    |\n351 | def process(document_stem: str, context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:\n    |                                                                              ^^^^\n352 |     \"\"\"\n353 |     Main processing function that generates structured narrative reports with canonicalization.\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/G_aggregation_reporting/report_compiler.py:351:88\n    |\n351 | def process(document_stem: str, context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:\n    |                                                                                        ^^^\n352 |     \"\"\"\n353 |     Main processing function that generates structured narrative reports with canonicalization.\n    |\n\nF821 Undefined name `JSONCanonicalizer`\n   --> canonical_flow/G_aggregation_reporting/report_compiler.py:363:21\n    |\n361 |     \"\"\"\n362 |     # Initialize canonicalizer\n363 |     canonicalizer = JSONCanonicalizer(audit_enabled=True, validation_enabled=True)\n    |                     ^^^^^^^^^^^^^^^^^\n364 |     \n365 |     # Canonicalize inputs\n    |\n\nF821 Undefined name `Path`\n   --> canonical_flow/G_aggregation_reporting/report_compiler.py:379:22\n    |\n377 |     try:\n378 |         # Construct input file path\n379 |         input_path = Path(\"canonical_flow/aggregation\") / f\"{document_stem}_meso.json\"\n    |                      ^^^^\n380 |         \n381 |         # Check if input file exists - fail fast with graceful fallback\n    |\n\nF821 Undefined name `datetime`\n   --> canonical_flow/G_aggregation_reporting/report_compiler.py:392:30\n    |\n390 |                 \"document_stem\": document_stem,\n391 |                 \"warnings\": warnings,\n392 |                 \"timestamp\": datetime.now().isoformat(),\n    |                              ^^^^^^^^\n393 |                 \"processing_id\": str(uuid.uuid4()),\n394 |                 \"report_data\": create_deterministic_dict({\n    |\n\nF841 [*] Local variable `e` is assigned to but never used\n   --> canonical_flow/G_aggregation_reporting/report_compiler.py:407:40\n    |\n405 |             with open(input_path, 'r', encoding='utf-8') as f:\n406 |                 meso_data = json.load(f)\n407 |         except json.JSONDecodeError as e:\n    |                                        ^\n408 |             warnings.append(ReportStatus.WARNINGS[\"MALFORMED_DATA\"])\n409 |             status = ReportStatus.PARTIAL\n    |\nhelp: Remove assignment to unused variable `e`\n\nF821 Undefined name `datetime`\n   --> canonical_flow/G_aggregation_reporting/report_compiler.py:432:21\n    |\n430 |         # Generate report sections with error handling\n431 |         processing_id = str(uuid.uuid4())\n432 |         timestamp = datetime.now().isoformat()\n    |                     ^^^^^^^^\n433 |         \n434 |         try:\n    |\n\nF821 Undefined name `Path`\n   --> canonical_flow/G_aggregation_reporting/report_compiler.py:493:23\n    |\n492 |         # Write output file with UTF-8 encoding\n493 |         output_path = Path(\"canonical_flow/aggregation\") / f\"{document_stem}_report.json\"\n    |                       ^^^^\n494 |         \n495 |         try:\n    |\n\nF821 Undefined name `datetime`\n   --> canonical_flow/G_aggregation_reporting/report_compiler.py:539:26\n    |\n537 |             \"warnings\": [ReportStatus.WARNINGS[\"PROCESSING_ERROR\"]],\n538 |             \"error\": str(e),\n539 |             \"timestamp\": datetime.now().isoformat(),\n    |                          ^^^^^^^^\n540 |             \"processing_id\": str(uuid.uuid4()),\n541 |             \"canonicalization\": {\n    |\n\ninvalid-syntax: Unexpected indentation\n  --> canonical_flow/I_ingestion_preparation/__init__.py:25:1\n   |\n24 | # # # from .gate_validation_system import (  # Module not found  # Module not found  # Module not found\n25 |     IngestionPipelineGatekeeper,\n   | ^^^^\n26 |     ComponentGate,\n27 |     GateValidationReport,\n   |\n\ninvalid-syntax: Expected a statement\n  --> canonical_flow/I_ingestion_preparation/__init__.py:34:1\n   |\n32 |     ArtifactValidator,\n33 |     JSONArtifactValidator\n34 | )\n   | ^\n35 |\n36 | # # # from .ingestion_orchestrator import IngestionPreparationOrchestrator  # Module not found  # Module not found  # Module not found\n   |\n\ninvalid-syntax: Expected a statement\n  --> canonical_flow/I_ingestion_preparation/__init__.py:34:2\n   |\n32 |     ArtifactValidator,\n33 |     JSONArtifactValidator\n34 | )\n   |  ^\n35 |\n36 | # # # from .ingestion_orchestrator import IngestionPreparationOrchestrator  # Module not found  # Module not found  # Module not found\n   |\n\nF821 Undefined name `project_root`\n  --> canonical_flow/I_ingestion_preparation/advanced_loader.py:26:12\n   |\n24 |     # Add project root to path for imports\n25 | # # #     project_root = Path(__file__).resolve().parents[2]  # Go up two levels from canonical_flow/I_ingestion_preparation/  # Modul\u2026\n26 |     if str(project_root) not in sys.path:\n   |            ^^^^^^^^^^^^\n27 |         sys.path.insert(0, str(project_root))\n   |\n\nF821 Undefined name `project_root`\n  --> canonical_flow/I_ingestion_preparation/advanced_loader.py:27:32\n   |\n25 | # # #     project_root = Path(__file__).resolve().parents[2]  # Go up two levels from canonical_flow/I_ingestion_preparation/  # Modul\u2026\n26 |     if str(project_root) not in sys.path:\n27 |         sys.path.insert(0, str(project_root))\n   |                                ^^^^^^^^^^^^\n28 |     \n29 |     # Load original module\n   |\n\nF821 Undefined name `project_root`\n  --> canonical_flow/I_ingestion_preparation/advanced_loader.py:30:21\n   |\n29 |     # Load original module\n30 |     original_file = project_root / \"advanced_loader.py\"\n   |                     ^^^^^^^^^^^^\n31 |     if original_file.exists():\n32 |         spec = importlib_util.spec_from_file_location(\n   |\n\nF821 Undefined name `importlib_util`\n  --> canonical_flow/I_ingestion_preparation/advanced_loader.py:32:16\n   |\n30 |     original_file = project_root / \"advanced_loader.py\"\n31 |     if original_file.exists():\n32 |         spec = importlib_util.spec_from_file_location(\n   |                ^^^^^^^^^^^^^^\n33 |             f\"original_{component_id.lower()}\", \n34 |             str(original_file)\n   |\n\nF821 Undefined name `importlib_util`\n  --> canonical_flow/I_ingestion_preparation/advanced_loader.py:38:31\n   |\n37 |         if spec and spec.loader:\n38 |             original_module = importlib_util.module_from_spec(spec)\n   |                               ^^^^^^^^^^^^^^\n39 |             spec.loader.exec_module(original_module)\n   |\n\nF821 Undefined name `ArtifactManager`\n  --> canonical_flow/I_ingestion_preparation/advanced_loader.py:64:32\n   |\n62 |         \"\"\"\n63 |         try:\n64 |             artifact_manager = ArtifactManager()\n   |                                ^^^^^^^^^^^^^^^\n65 |             \n66 |             # Handle failed module loading case\n   |\n\nF821 Undefined name `e`\n  --> canonical_flow/I_ingestion_preparation/advanced_loader.py:68:68\n   |\n66 |             # Handle failed module loading case\n67 |             error_data = {\n68 |                 \"error\": f\"Module {source_module} failed to load: {e}\",\n   |                                                                    ^\n69 |                 \"component\": \"02I\",\n70 |                 \"status\": \"failed\",\n   |\n\nF821 Undefined name `e`\n  --> canonical_flow/I_ingestion_preparation/advanced_loader.py:84:68\n   |\n82 |             return {\n83 |                 \"success\": False,\n84 |                 \"error\": f\"Module {source_module} failed to load: {e}\",\n   |                                                                    ^\n85 |                 \"output_path\": str(output_path),\n86 |                 \"artifact_type\": \"bundle\"\n   |\n\nF821 Undefined name `project_root`\n  --> canonical_flow/I_ingestion_preparation/feature_extractor.py:26:12\n   |\n24 |     # Add project root to path for imports\n25 | # # #     project_root = Path(__file__).resolve().parents[2]  # Go up two levels from canonical_flow/I_ingestion_preparation/  # Modul\u2026\n26 |     if str(project_root) not in sys.path:\n   |            ^^^^^^^^^^^^\n27 |         sys.path.insert(0, str(project_root))\n   |\n\nF821 Undefined name `project_root`\n  --> canonical_flow/I_ingestion_preparation/feature_extractor.py:27:32\n   |\n25 | # # #     project_root = Path(__file__).resolve().parents[2]  # Go up two levels from canonical_flow/I_ingestion_preparation/  # Modul\u2026\n26 |     if str(project_root) not in sys.path:\n27 |         sys.path.insert(0, str(project_root))\n   |                                ^^^^^^^^^^^^\n28 |     \n29 |     # Load original module\n   |\n\nF821 Undefined name `project_root`\n  --> canonical_flow/I_ingestion_preparation/feature_extractor.py:30:21\n   |\n29 |     # Load original module\n30 |     original_file = project_root / \"feature_extractor.py\"\n   |                     ^^^^^^^^^^^^\n31 |     if original_file.exists():\n32 |         spec = importlib_util.spec_from_file_location(\n   |\n\nF821 Undefined name `importlib_util`\n  --> canonical_flow/I_ingestion_preparation/feature_extractor.py:32:16\n   |\n30 |     original_file = project_root / \"feature_extractor.py\"\n31 |     if original_file.exists():\n32 |         spec = importlib_util.spec_from_file_location(\n   |                ^^^^^^^^^^^^^^\n33 |             f\"original_{component_id.lower()}\", \n34 |             str(original_file)\n   |\n\nF821 Undefined name `importlib_util`\n  --> canonical_flow/I_ingestion_preparation/feature_extractor.py:38:31\n   |\n37 |         if spec and spec.loader:\n38 |             original_module = importlib_util.module_from_spec(spec)\n   |                               ^^^^^^^^^^^^^^\n39 |             spec.loader.exec_module(original_module)\n   |\n\nF821 Undefined name `ArtifactManager`\n  --> canonical_flow/I_ingestion_preparation/feature_extractor.py:64:32\n   |\n62 |         \"\"\"\n63 |         try:\n64 |             artifact_manager = ArtifactManager()\n   |                                ^^^^^^^^^^^^^^^\n65 |             \n66 |             # Handle failed module loading case\n   |\n\nF821 Undefined name `e`\n  --> canonical_flow/I_ingestion_preparation/feature_extractor.py:68:68\n   |\n66 |             # Handle failed module loading case\n67 |             error_data = {\n68 |                 \"error\": f\"Module {source_module} failed to load: {e}\",\n   |                                                                    ^\n69 |                 \"component\": \"03I\",\n70 |                 \"status\": \"failed\",\n   |\n\nF821 Undefined name `e`\n  --> canonical_flow/I_ingestion_preparation/feature_extractor.py:84:68\n   |\n82 |             return {\n83 |                 \"success\": False,\n84 |                 \"error\": f\"Module {source_module} failed to load: {e}\",\n   |                                                                    ^\n85 |                 \"output_path\": str(output_path),\n86 |                 \"artifact_type\": \"features\"\n   |\n\ninvalid-syntax: unindent does not match any outer indentation level\n  --> canonical_flow/I_ingestion_preparation/gate_validation_system.py:36:1\n   |\n34 | # # #     from ..mathematical_enhancers.mathematical_compatibility_matrix import (  # Module not found  # Module not found  # Module n\u2026\n35 |         LibraryStatusReporter, MathematicalCompatibilityMatrix\n36 |     )\n   | ^^^^\n37 |     LIBRARY_STATUS_REPORTER_AVAILABLE = True\n38 | except ImportError:\n   |\n\ninvalid-syntax: Expected a statement\n  --> canonical_flow/I_ingestion_preparation/gate_validation_system.py:36:5\n   |\n34 | # # #     from ..mathematical_enhancers.mathematical_compatibility_matrix import (  # Module not found  # Module not found  # Module n\u2026\n35 |         LibraryStatusReporter, MathematicalCompatibilityMatrix\n36 |     )\n   |     ^\n37 |     LIBRARY_STATUS_REPORTER_AVAILABLE = True\n38 | except ImportError:\n   |\n\ninvalid-syntax: Expected a statement\n  --> canonical_flow/I_ingestion_preparation/gate_validation_system.py:36:6\n   |\n34 | # # #     from ..mathematical_enhancers.mathematical_compatibility_matrix import (  # Module not found  # Module not found  # Module n\u2026\n35 |         LibraryStatusReporter, MathematicalCompatibilityMatrix\n36 |     )\n   |      ^\n37 |     LIBRARY_STATUS_REPORTER_AVAILABLE = True\n38 | except ImportError:\n   |\n\ninvalid-syntax: Unexpected indentation\n  --> canonical_flow/I_ingestion_preparation/gate_validation_system.py:37:1\n   |\n35 |         LibraryStatusReporter, MathematicalCompatibilityMatrix\n36 |     )\n37 |     LIBRARY_STATUS_REPORTER_AVAILABLE = True\n   | ^^^^\n38 | except ImportError:\n39 |     LIBRARY_STATUS_REPORTER_AVAILABLE = False\n   |\n\ninvalid-syntax: Expected ':', found ']'\n   --> canonical_flow/I_ingestion_preparation/gate_validation_system.py:310:9\n    |\n308 |                 min_size_bytes=100\n309 |             )\n310 |         ],\n    |         ^\n311 | # # #         \"03I\": [    # feature_extractor requires bundle files from advanced_loader  # Module not found  # Module not found  # M\u2026\n312 |             ArtifactSpec(\n    |\n\ninvalid-syntax: Expected an expression or a '}'\n   --> canonical_flow/I_ingestion_preparation/gate_validation_system.py:310:10\n    |\n308 |                 min_size_bytes=100\n309 |             )\n310 |         ],\n    |          ^\n311 | # # #         \"03I\": [    # feature_extractor requires bundle files from advanced_loader  # Module not found  # Module not found  # M\u2026\n312 |             ArtifactSpec(\n    |\n\ninvalid-syntax: Expected ',', found newline\n   --> canonical_flow/I_ingestion_preparation/gate_validation_system.py:310:11\n    |\n308 |                 min_size_bytes=100\n309 |             )\n310 |         ],\n    |           ^\n311 | # # #         \"03I\": [    # feature_extractor requires bundle files from advanced_loader  # Module not found  # Module not found  # M\u2026\n312 |             ArtifactSpec(\n    |\n\ninvalid-syntax: Expected ',', found indent\n   --> canonical_flow/I_ingestion_preparation/gate_validation_system.py:312:1\n    |\n310 |         ],\n311 | # # #         \"03I\": [    # feature_extractor requires bundle files from advanced_loader  # Module not found  # Module not found  # M\u2026\n312 |             ArtifactSpec(\n    | ^^^^^^^^^^^^\n313 |                 pattern=\"*_bundle.json\", \n314 | # # #                 description=\"Document bundles from advanced loader\",  # Module not found  # Module not found  # Module not found\n    |\n\ninvalid-syntax: Expected ':', found newline\n   --> canonical_flow/I_ingestion_preparation/gate_validation_system.py:317:14\n    |\n315 |                 required_fields=[\"document_features\", \"structure\", \"content\"],\n316 |                 min_size_bytes=200\n317 |             )\n    |              ^\n318 |         ],\n319 | # # #         \"04I\": [    # normative_validator requires feature files from feature_extractor  # Module not found  # Module not found\u2026\n    |\n\ninvalid-syntax: unindent does not match any outer indentation level\n   --> canonical_flow/I_ingestion_preparation/gate_validation_system.py:318:1\n    |\n316 |                 min_size_bytes=200\n317 |             )\n318 |         ],\n    | ^^^^^^^^\n319 | # # #         \"04I\": [    # normative_validator requires feature files from feature_extractor  # Module not found  # Module not found\u2026\n320 |             ArtifactSpec(\n    |\n\ninvalid-syntax: Expected ',', found ']'\n   --> canonical_flow/I_ingestion_preparation/gate_validation_system.py:318:9\n    |\n316 |                 min_size_bytes=200\n317 |             )\n318 |         ],\n    |         ^\n319 | # # #         \"04I\": [    # normative_validator requires feature files from feature_extractor  # Module not found  # Module not found\u2026\n320 |             ArtifactSpec(\n    |\n\ninvalid-syntax: Expected an expression or a '}'\n   --> canonical_flow/I_ingestion_preparation/gate_validation_system.py:318:10\n    |\n316 |                 min_size_bytes=200\n317 |             )\n318 |         ],\n    |          ^\n319 | # # #         \"04I\": [    # normative_validator requires feature files from feature_extractor  # Module not found  # Module not found\u2026\n320 |             ArtifactSpec(\n    |\n\ninvalid-syntax: Expected ',', found newline\n   --> canonical_flow/I_ingestion_preparation/gate_validation_system.py:318:11\n    |\n316 |                 min_size_bytes=200\n317 |             )\n318 |         ],\n    |           ^\n319 | # # #         \"04I\": [    # normative_validator requires feature files from feature_extractor  # Module not found  # Module not found\u2026\n320 |             ArtifactSpec(\n    |\n\ninvalid-syntax: Expected ',', found indent\n   --> canonical_flow/I_ingestion_preparation/gate_validation_system.py:320:1\n    |\n318 |         ],\n319 | # # #         \"04I\": [    # normative_validator requires feature files from feature_extractor  # Module not found  # Module not found\u2026\n320 |             ArtifactSpec(\n    | ^^^^^^^^^^^^\n321 |                 pattern=\"*_features.json\",\n322 | # # #                 description=\"Extracted features from feature extractor\",   # Module not found  # Module not found  # Module not\u2026\n    |\n\ninvalid-syntax: Expected ':', found newline\n   --> canonical_flow/I_ingestion_preparation/gate_validation_system.py:325:14\n    |\n323 |                 required_fields=[\"textual_features\", \"structural_features\", \"compliance_score\"],\n324 |                 min_size_bytes=150\n325 |             )\n    |              ^\n326 |         ],\n327 | # # #         \"05I\": [    # raw_data_generator requires validation files from normative_validator  # Module not found  # Module not f\u2026\n    |\n\ninvalid-syntax: unindent does not match any outer indentation level\n   --> canonical_flow/I_ingestion_preparation/gate_validation_system.py:326:1\n    |\n324 |                 min_size_bytes=150\n325 |             )\n326 |         ],\n    | ^^^^^^^^\n327 | # # #         \"05I\": [    # raw_data_generator requires validation files from normative_validator  # Module not found  # Module not f\u2026\n328 |             ArtifactSpec(\n    |\n\ninvalid-syntax: Expected ',', found ']'\n   --> canonical_flow/I_ingestion_preparation/gate_validation_system.py:326:9\n    |\n324 |                 min_size_bytes=150\n325 |             )\n326 |         ],\n    |         ^\n327 | # # #         \"05I\": [    # raw_data_generator requires validation files from normative_validator  # Module not found  # Module not f\u2026\n328 |             ArtifactSpec(\n    |\n\ninvalid-syntax: Expected an expression or a '}'\n   --> canonical_flow/I_ingestion_preparation/gate_validation_system.py:326:10\n    |\n324 |                 min_size_bytes=150\n325 |             )\n326 |         ],\n    |          ^\n327 | # # #         \"05I\": [    # raw_data_generator requires validation files from normative_validator  # Module not found  # Module not f\u2026\n328 |             ArtifactSpec(\n    |\n\ninvalid-syntax: Expected ',', found newline\n   --> canonical_flow/I_ingestion_preparation/gate_validation_system.py:326:11\n    |\n324 |                 min_size_bytes=150\n325 |             )\n326 |         ],\n    |           ^\n327 | # # #         \"05I\": [    # raw_data_generator requires validation files from normative_validator  # Module not found  # Module not f\u2026\n328 |             ArtifactSpec(\n    |\n\ninvalid-syntax: Expected ',', found indent\n   --> canonical_flow/I_ingestion_preparation/gate_validation_system.py:328:1\n    |\n326 |         ],\n327 | # # #         \"05I\": [    # raw_data_generator requires validation files from normative_validator  # Module not found  # Module not f\u2026\n328 |             ArtifactSpec(\n    | ^^^^^^^^^^^^\n329 |                 pattern=\"*_validation.json\",\n330 | # # #                 description=\"Validation results from normative validator\",  # Module not found  # Module not found  # Module no\u2026\n    |\n\ninvalid-syntax: Expected ':', found newline\n   --> canonical_flow/I_ingestion_preparation/gate_validation_system.py:333:14\n    |\n331 |                 required_fields=[\"compliance_score\", \"checklist\", \"summary\"], \n332 |                 min_size_bytes=300\n333 |             )\n    |              ^\n334 |         ]\n335 |     }\n    |\n\ninvalid-syntax: unindent does not match any outer indentation level\n   --> canonical_flow/I_ingestion_preparation/gate_validation_system.py:334:1\n    |\n332 |                 min_size_bytes=300\n333 |             )\n334 |         ]\n    | ^^^^^^^^\n335 |     }\n    |\n\ninvalid-syntax: Expected ',', found ']'\n   --> canonical_flow/I_ingestion_preparation/gate_validation_system.py:334:9\n    |\n332 |                 min_size_bytes=300\n333 |             )\n334 |         ]\n    |         ^\n335 |     }\n    |\n\ninvalid-syntax: Expected ',', found newline\n   --> canonical_flow/I_ingestion_preparation/gate_validation_system.py:334:10\n    |\n332 |                 min_size_bytes=300\n333 |             )\n334 |         ]\n    |          ^\n335 |     }\n    |\n\ninvalid-syntax: Unexpected indentation\n  --> canonical_flow/I_ingestion_preparation/ingestion_orchestrator.py:20:1\n   |\n18 | # Import gate validation system\n19 | # # # from .gate_validation_system import (  # Module not found  # Module not found  # Module not found\n20 |     IngestionPipelineGatekeeper,\n   | ^^^^\n21 |     ComponentState, \n22 |     GateStatus\n   |\n\ninvalid-syntax: Expected a statement\n  --> canonical_flow/I_ingestion_preparation/ingestion_orchestrator.py:23:1\n   |\n21 |     ComponentState, \n22 |     GateStatus\n23 | )\n   | ^\n24 |\n25 | # Import component modules\n   |\n\ninvalid-syntax: Expected a statement\n  --> canonical_flow/I_ingestion_preparation/ingestion_orchestrator.py:23:2\n   |\n21 |     ComponentState, \n22 |     GateStatus\n23 | )\n   |  ^\n24 |\n25 | # Import component modules\n   |\n\nF821 Undefined name `project_root`\n  --> canonical_flow/I_ingestion_preparation/normative_validator.py:26:12\n   |\n24 |     # Add project root to path for imports\n25 | # # #     project_root = Path(__file__).resolve().parents[2]  # Go up two levels from canonical_flow/I_ingestion_preparation/  # Modul\u2026\n26 |     if str(project_root) not in sys.path:\n   |            ^^^^^^^^^^^^\n27 |         sys.path.insert(0, str(project_root))\n   |\n\nF821 Undefined name `project_root`\n  --> canonical_flow/I_ingestion_preparation/normative_validator.py:27:32\n   |\n25 | # # #     project_root = Path(__file__).resolve().parents[2]  # Go up two levels from canonical_flow/I_ingestion_preparation/  # Modul\u2026\n26 |     if str(project_root) not in sys.path:\n27 |         sys.path.insert(0, str(project_root))\n   |                                ^^^^^^^^^^^^\n28 |     \n29 |     # Load original module\n   |\n\nF821 Undefined name `project_root`\n  --> canonical_flow/I_ingestion_preparation/normative_validator.py:30:21\n   |\n29 |     # Load original module\n30 |     original_file = project_root / \"normative_validator.py\"\n   |                     ^^^^^^^^^^^^\n31 |     if original_file.exists():\n32 |         spec = importlib_util.spec_from_file_location(\n   |\n\nF821 Undefined name `importlib_util`\n  --> canonical_flow/I_ingestion_preparation/normative_validator.py:32:16\n   |\n30 |     original_file = project_root / \"normative_validator.py\"\n31 |     if original_file.exists():\n32 |         spec = importlib_util.spec_from_file_location(\n   |                ^^^^^^^^^^^^^^\n33 |             f\"original_{component_id.lower()}\", \n34 |             str(original_file)\n   |\n\nF821 Undefined name `importlib_util`\n  --> canonical_flow/I_ingestion_preparation/normative_validator.py:38:31\n   |\n37 |         if spec and spec.loader:\n38 |             original_module = importlib_util.module_from_spec(spec)\n   |                               ^^^^^^^^^^^^^^\n39 |             spec.loader.exec_module(original_module)\n   |\n\nF821 Undefined name `ArtifactManager`\n  --> canonical_flow/I_ingestion_preparation/normative_validator.py:64:32\n   |\n62 |         \"\"\"\n63 |         try:\n64 |             artifact_manager = ArtifactManager()\n   |                                ^^^^^^^^^^^^^^^\n65 |             \n66 |             # Handle failed module loading case\n   |\n\nF821 Undefined name `e`\n  --> canonical_flow/I_ingestion_preparation/normative_validator.py:68:68\n   |\n66 |             # Handle failed module loading case\n67 |             error_data = {\n68 |                 \"error\": f\"Module {source_module} failed to load: {e}\",\n   |                                                                    ^\n69 |                 \"component\": \"04I\",\n70 |                 \"status\": \"failed\",\n   |\n\nF821 Undefined name `e`\n  --> canonical_flow/I_ingestion_preparation/normative_validator.py:84:68\n   |\n82 |             return {\n83 |                 \"success\": False,\n84 |                 \"error\": f\"Module {source_module} failed to load: {e}\",\n   |                                                                    ^\n85 |                 \"output_path\": str(output_path),\n86 |                 \"artifact_type\": \"validation\"\n   |\n\nF821 Undefined name `project_root`\n  --> canonical_flow/I_ingestion_preparation/pdf_reader.py:26:12\n   |\n24 |     # Add project root to path for imports\n25 | # # #     project_root = Path(__file__).resolve().parents[2]  # Go up two levels from canonical_flow/I_ingestion_preparation/  # Modul\u2026\n26 |     if str(project_root) not in sys.path:\n   |            ^^^^^^^^^^^^\n27 |         sys.path.insert(0, str(project_root))\n   |\n\nF821 Undefined name `project_root`\n  --> canonical_flow/I_ingestion_preparation/pdf_reader.py:27:32\n   |\n25 | # # #     project_root = Path(__file__).resolve().parents[2]  # Go up two levels from canonical_flow/I_ingestion_preparation/  # Modul\u2026\n26 |     if str(project_root) not in sys.path:\n27 |         sys.path.insert(0, str(project_root))\n   |                                ^^^^^^^^^^^^\n28 |     \n29 |     # Load original module\n   |\n\nF821 Undefined name `project_root`\n  --> canonical_flow/I_ingestion_preparation/pdf_reader.py:30:21\n   |\n29 |     # Load original module\n30 |     original_file = project_root / \"pdf_reader.py\"\n   |                     ^^^^^^^^^^^^\n31 |     if original_file.exists():\n32 |         spec = importlib_util.spec_from_file_location(\n   |\n\nF821 Undefined name `importlib_util`\n  --> canonical_flow/I_ingestion_preparation/pdf_reader.py:32:16\n   |\n30 |     original_file = project_root / \"pdf_reader.py\"\n31 |     if original_file.exists():\n32 |         spec = importlib_util.spec_from_file_location(\n   |                ^^^^^^^^^^^^^^\n33 |             f\"original_{component_id.lower()}\", \n34 |             str(original_file)\n   |\n\nF821 Undefined name `importlib_util`\n  --> canonical_flow/I_ingestion_preparation/pdf_reader.py:38:31\n   |\n37 |         if spec and spec.loader:\n38 |             original_module = importlib_util.module_from_spec(spec)\n   |                               ^^^^^^^^^^^^^^\n39 |             spec.loader.exec_module(original_module)\n   |\n\nF821 Undefined name `ArtifactManager`\n  --> canonical_flow/I_ingestion_preparation/pdf_reader.py:64:32\n   |\n62 |         \"\"\"\n63 |         try:\n64 |             artifact_manager = ArtifactManager()\n   |                                ^^^^^^^^^^^^^^^\n65 |             \n66 |             # Handle failed module loading case\n   |\n\nF821 Undefined name `e`\n  --> canonical_flow/I_ingestion_preparation/pdf_reader.py:68:68\n   |\n66 |             # Handle failed module loading case\n67 |             error_data = {\n68 |                 \"error\": f\"Module {source_module} failed to load: {e}\",\n   |                                                                    ^\n69 |                 \"component\": \"01I\",\n70 |                 \"status\": \"failed\",\n   |\n\nF821 Undefined name `e`\n  --> canonical_flow/I_ingestion_preparation/pdf_reader.py:84:68\n   |\n82 |             return {\n83 |                 \"success\": False,\n84 |                 \"error\": f\"Module {source_module} failed to load: {e}\",\n   |                                                                    ^\n85 |                 \"output_path\": str(output_path),\n86 |                 \"artifact_type\": \"text\"\n   |\n\nF401 [*] `traceback` imported but unused\n  --> canonical_flow/I_ingestion_preparation/preflight_validation.py:22:8\n   |\n20 | import sys\n21 | import time\n22 | import traceback\n   |        ^^^^^^^^^\n23 | # # # from dataclasses import dataclass, field  # Module not found  # Module not found  # Module not found\n24 | # # # from datetime import datetime  # Module not found  # Module not found  # Module not found\n   |\nhelp: Remove unused import: `traceback`\n\nF821 Undefined name `dataclass`\n  --> canonical_flow/I_ingestion_preparation/preflight_validation.py:34:2\n   |\n34 | @dataclass\n   |  ^^^^^^^^^\n35 | class ValidationResult:\n36 |     \"\"\"Result of validating a single component.\"\"\"\n   |\n\nF821 Undefined name `Optional`\n  --> canonical_flow/I_ingestion_preparation/preflight_validation.py:41:19\n   |\n39 |     component_name: str\n40 |     import_success: bool\n41 |     import_error: Optional[str] = None\n   |                   ^^^^^^^^\n42 |     methods_found: Dict[str, bool] = field(default_factory=dict)\n43 |     method_signatures: Dict[str, Any] = field(default_factory=dict)\n   |\n\nF821 Undefined name `Dict`\n  --> canonical_flow/I_ingestion_preparation/preflight_validation.py:42:20\n   |\n40 |     import_success: bool\n41 |     import_error: Optional[str] = None\n42 |     methods_found: Dict[str, bool] = field(default_factory=dict)\n   |                    ^^^^\n43 |     method_signatures: Dict[str, Any] = field(default_factory=dict)\n44 |     is_placeholder: bool = False\n   |\n\nF821 Undefined name `field`\n  --> canonical_flow/I_ingestion_preparation/preflight_validation.py:42:38\n   |\n40 |     import_success: bool\n41 |     import_error: Optional[str] = None\n42 |     methods_found: Dict[str, bool] = field(default_factory=dict)\n   |                                      ^^^^^\n43 |     method_signatures: Dict[str, Any] = field(default_factory=dict)\n44 |     is_placeholder: bool = False\n   |\n\nF821 Undefined name `Dict`\n  --> canonical_flow/I_ingestion_preparation/preflight_validation.py:43:24\n   |\n41 |     import_error: Optional[str] = None\n42 |     methods_found: Dict[str, bool] = field(default_factory=dict)\n43 |     method_signatures: Dict[str, Any] = field(default_factory=dict)\n   |                        ^^^^\n44 |     is_placeholder: bool = False\n45 |     smoke_test_passed: bool = False\n   |\n\nF821 Undefined name `Any`\n  --> canonical_flow/I_ingestion_preparation/preflight_validation.py:43:34\n   |\n41 |     import_error: Optional[str] = None\n42 |     methods_found: Dict[str, bool] = field(default_factory=dict)\n43 |     method_signatures: Dict[str, Any] = field(default_factory=dict)\n   |                                  ^^^\n44 |     is_placeholder: bool = False\n45 |     smoke_test_passed: bool = False\n   |\n\nF821 Undefined name `field`\n  --> canonical_flow/I_ingestion_preparation/preflight_validation.py:43:41\n   |\n41 |     import_error: Optional[str] = None\n42 |     methods_found: Dict[str, bool] = field(default_factory=dict)\n43 |     method_signatures: Dict[str, Any] = field(default_factory=dict)\n   |                                         ^^^^^\n44 |     is_placeholder: bool = False\n45 |     smoke_test_passed: bool = False\n   |\n\nF821 Undefined name `Optional`\n  --> canonical_flow/I_ingestion_preparation/preflight_validation.py:46:23\n   |\n44 |     is_placeholder: bool = False\n45 |     smoke_test_passed: bool = False\n46 |     smoke_test_error: Optional[str] = None\n   |                       ^^^^^^^^\n47 |     smoke_test_result: Any = None\n48 |     production_ready: bool = False\n   |\n\nF821 Undefined name `Any`\n  --> canonical_flow/I_ingestion_preparation/preflight_validation.py:47:24\n   |\n45 |     smoke_test_passed: bool = False\n46 |     smoke_test_error: Optional[str] = None\n47 |     smoke_test_result: Any = None\n   |                        ^^^\n48 |     production_ready: bool = False\n49 |     issues: List[str] = field(default_factory=list)\n   |\n\nF821 Undefined name `List`\n  --> canonical_flow/I_ingestion_preparation/preflight_validation.py:49:13\n   |\n47 |     smoke_test_result: Any = None\n48 |     production_ready: bool = False\n49 |     issues: List[str] = field(default_factory=list)\n   |             ^^^^\n50 |     warnings: List[str] = field(default_factory=list)\n   |\n\nF821 Undefined name `field`\n  --> canonical_flow/I_ingestion_preparation/preflight_validation.py:49:25\n   |\n47 |     smoke_test_result: Any = None\n48 |     production_ready: bool = False\n49 |     issues: List[str] = field(default_factory=list)\n   |                         ^^^^^\n50 |     warnings: List[str] = field(default_factory=list)\n   |\n\nF821 Undefined name `List`\n  --> canonical_flow/I_ingestion_preparation/preflight_validation.py:50:15\n   |\n48 |     production_ready: bool = False\n49 |     issues: List[str] = field(default_factory=list)\n50 |     warnings: List[str] = field(default_factory=list)\n   |               ^^^^\n   |\n\nF821 Undefined name `field`\n  --> canonical_flow/I_ingestion_preparation/preflight_validation.py:50:27\n   |\n48 |     production_ready: bool = False\n49 |     issues: List[str] = field(default_factory=list)\n50 |     warnings: List[str] = field(default_factory=list)\n   |                           ^^^^^\n   |\n\nF821 Undefined name `dataclass`\n  --> canonical_flow/I_ingestion_preparation/preflight_validation.py:53:2\n   |\n53 | @dataclass\n   |  ^^^^^^^^^\n54 | class PreflightReport:\n55 |     \"\"\"Complete preflight validation report.\"\"\"\n   |\n\nF821 Undefined name `datetime`\n  --> canonical_flow/I_ingestion_preparation/preflight_validation.py:57:16\n   |\n55 |     \"\"\"Complete preflight validation report.\"\"\"\n56 |     \n57 |     timestamp: datetime\n   |                ^^^^^^^^\n58 |     total_components: int\n59 |     components_ready: int\n   |\n\nF821 Undefined name `Dict`\n  --> canonical_flow/I_ingestion_preparation/preflight_validation.py:62:24\n   |\n60 |     components_with_issues: int\n61 |     overall_status: str  # \"READY\", \"PARTIAL\", \"FAILED\"\n62 |     component_results: Dict[str, ValidationResult] = field(default_factory=dict)\n   |                        ^^^^\n63 |     pipeline_recommendations: List[str] = field(default_factory=list)\n64 |     critical_issues: List[str] = field(default_factory=list)\n   |\n\nF821 Undefined name `field`\n  --> canonical_flow/I_ingestion_preparation/preflight_validation.py:62:54\n   |\n60 |     components_with_issues: int\n61 |     overall_status: str  # \"READY\", \"PARTIAL\", \"FAILED\"\n62 |     component_results: Dict[str, ValidationResult] = field(default_factory=dict)\n   |                                                      ^^^^^\n63 |     pipeline_recommendations: List[str] = field(default_factory=list)\n64 |     critical_issues: List[str] = field(default_factory=list)\n   |\n\nF821 Undefined name `List`\n  --> canonical_flow/I_ingestion_preparation/preflight_validation.py:63:31\n   |\n61 |     overall_status: str  # \"READY\", \"PARTIAL\", \"FAILED\"\n62 |     component_results: Dict[str, ValidationResult] = field(default_factory=dict)\n63 |     pipeline_recommendations: List[str] = field(default_factory=list)\n   |                               ^^^^\n64 |     critical_issues: List[str] = field(default_factory=list)\n   |\n\nF821 Undefined name `field`\n  --> canonical_flow/I_ingestion_preparation/preflight_validation.py:63:43\n   |\n61 |     overall_status: str  # \"READY\", \"PARTIAL\", \"FAILED\"\n62 |     component_results: Dict[str, ValidationResult] = field(default_factory=dict)\n63 |     pipeline_recommendations: List[str] = field(default_factory=list)\n   |                                           ^^^^^\n64 |     critical_issues: List[str] = field(default_factory=list)\n   |\n\nF821 Undefined name `List`\n  --> canonical_flow/I_ingestion_preparation/preflight_validation.py:64:22\n   |\n62 |     component_results: Dict[str, ValidationResult] = field(default_factory=dict)\n63 |     pipeline_recommendations: List[str] = field(default_factory=list)\n64 |     critical_issues: List[str] = field(default_factory=list)\n   |                      ^^^^\n   |\n\nF821 Undefined name `field`\n  --> canonical_flow/I_ingestion_preparation/preflight_validation.py:64:34\n   |\n62 |     component_results: Dict[str, ValidationResult] = field(default_factory=dict)\n63 |     pipeline_recommendations: List[str] = field(default_factory=list)\n64 |     critical_issues: List[str] = field(default_factory=list)\n   |                                  ^^^^^\n   |\n\nF821 Undefined name `Path`\n  --> canonical_flow/I_ingestion_preparation/preflight_validation.py:88:38\n   |\n86 |     }\n87 |     \n88 |     def __init__(self, project_root: Path):\n   |                                      ^^^^\n89 |         \"\"\"Initialize validator with project root path.\"\"\"\n90 |         self.project_root = project_root\n   |\n\nF821 Undefined name `Optional`\n   --> canonical_flow/I_ingestion_preparation/preflight_validation.py:136:75\n    |\n134 |         return result\n135 |     \n136 |     def _test_import(self, module_path: str, result: ValidationResult) -> Optional[Any]:\n    |                                                                           ^^^^^^^^\n137 |         \"\"\"Test if module can be imported successfully.\"\"\"\n138 |         try:\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/I_ingestion_preparation/preflight_validation.py:136:84\n    |\n134 |         return result\n135 |     \n136 |     def _test_import(self, module_path: str, result: ValidationResult) -> Optional[Any]:\n    |                                                                                    ^^^\n137 |         \"\"\"Test if module can be imported successfully.\"\"\"\n138 |         try:\n    |\n\nF541 [*] f-string without any placeholders\n   --> canonical_flow/I_ingestion_preparation/preflight_validation.py:146:19\n    |\n144 |             result.import_success = True\n145 |             \n146 |             print(f\"  \u2713 Import successful\")\n    |                   ^^^^^^^^^^^^^^^^^^^^^^^^\n147 |             return module\n    |\nhelp: Remove extraneous `f` prefix\n\nF821 Undefined name `Any`\n   --> canonical_flow/I_ingestion_preparation/preflight_validation.py:163:41\n    |\n161 |             return None\n162 |     \n163 |     def _validate_methods(self, module: Any, component_type: str, result: ValidationResult):\n    |                                         ^^^\n164 |         \"\"\"Validate that required methods exist and are callable.\"\"\"\n165 |         expected_methods = self.EXPECTED_METHODS.get(component_type, [])\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/I_ingestion_preparation/preflight_validation.py:190:44\n    |\n188 |                 print(f\"  \u2717 Method '{method_name}' missing\")\n189 |     \n190 |     def _detect_placeholders(self, module: Any, result: ValidationResult):\n    |                                            ^^^\n191 |         \"\"\"Detect if module contains placeholder implementations.\"\"\"\n192 |         placeholder_indicators = [\n    |\n\nF821 Undefined name `re`\n   --> canonical_flow/I_ingestion_preparation/preflight_validation.py:205:20\n    |\n203 |             source = inspect.getsource(module)\n204 |             for indicator in placeholder_indicators:\n205 |                 if re.search(indicator, source, re.IGNORECASE):\n    |                    ^^\n206 |                     result.is_placeholder = True\n207 |                     result.issues.append(f\"Placeholder code detected: contains '{indicator}'\")\n    |\n\nF821 Undefined name `re`\n   --> canonical_flow/I_ingestion_preparation/preflight_validation.py:205:49\n    |\n203 |             source = inspect.getsource(module)\n204 |             for indicator in placeholder_indicators:\n205 |                 if re.search(indicator, source, re.IGNORECASE):\n    |                                                 ^^\n206 |                     result.is_placeholder = True\n207 |                     result.issues.append(f\"Placeholder code detected: contains '{indicator}'\")\n    |\n\nE722 Do not use bare `except`\n   --> canonical_flow/I_ingestion_preparation/preflight_validation.py:210:9\n    |\n208 |                     print(f\"  \u26a0 Placeholder detected: {indicator}\")\n209 |                     break\n210 |         except:\n    |         ^^^^^^\n211 |             # Can't get source, check docstring and common attributes\n212 |             if hasattr(module, '__doc__') and module.__doc__:\n    |\n\nF541 [*] f-string without any placeholders\n   --> canonical_flow/I_ingestion_preparation/preflight_validation.py:217:46\n    |\n215 |                     if indicator.lower() in doc:\n216 |                         result.is_placeholder = True\n217 |                         result.issues.append(f\"Placeholder detected in docstring\")\n    |                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n218 |                         print(f\"  \u26a0 Placeholder detected in documentation\")\n219 |                         break\n    |\nhelp: Remove extraneous `f` prefix\n\nF541 [*] f-string without any placeholders\n   --> canonical_flow/I_ingestion_preparation/preflight_validation.py:218:31\n    |\n216 |                         result.is_placeholder = True\n217 |                         result.issues.append(f\"Placeholder detected in docstring\")\n218 |                         print(f\"  \u26a0 Placeholder detected in documentation\")\n    |                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n219 |                         break\n    |\nhelp: Remove extraneous `f` prefix\n\nF541 [*] f-string without any placeholders\n   --> canonical_flow/I_ingestion_preparation/preflight_validation.py:229:27\n    |\n227 |                     result.is_placeholder = True\n228 |                     result.issues.append(\"Process method returns error dict (placeholder pattern)\")\n229 |                     print(f\"  \u26a0 Process method appears to be placeholder\")\n    |                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n230 |             except:\n231 |                 # Expected to fail with real implementations due to missing args\n    |\nhelp: Remove extraneous `f` prefix\n\nE722 Do not use bare `except`\n   --> canonical_flow/I_ingestion_preparation/preflight_validation.py:230:13\n    |\n228 |                     result.issues.append(\"Process method returns error dict (placeholder pattern)\")\n229 |                     print(f\"  \u26a0 Process method appears to be placeholder\")\n230 |             except:\n    |             ^^^^^^\n231 |                 # Expected to fail with real implementations due to missing args\n232 |                 pass\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/I_ingestion_preparation/preflight_validation.py:234:39\n    |\n232 |                 pass\n233 |     \n234 |     def _run_smoke_test(self, module: Any, component_type: str, result: ValidationResult):\n    |                                       ^^^\n235 |         \"\"\"Run smoke test with real PDF document.\"\"\"\n236 |         if not hasattr(module, 'process'):\n    |\n\nF541 [*] f-string without any placeholders\n   --> canonical_flow/I_ingestion_preparation/preflight_validation.py:281:23\n    |\n279 |                 result.smoke_test_passed = False\n280 |                 result.smoke_test_error = \"Process method returned None\"\n281 |                 print(f\"  \u2717 Smoke test failed: returned None\")\n    |                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n282 |                 \n283 |         except Exception as e:\n    |\nhelp: Remove extraneous `f` prefix\n\nF821 Undefined name `Any`\n   --> canonical_flow/I_ingestion_preparation/preflight_validation.py:288:40\n    |\n286 |             print(f\"  \u2717 Smoke test exception: {str(e)}\")\n287 |     \n288 |     def _test_pdf_reader(self, module: Any, pdf_path: Path) -> Any:\n    |                                        ^^^\n289 |         \"\"\"Test PDF reader component.\"\"\"\n290 |         if hasattr(module, 'PDFPageIterator'):\n    |\n\nF821 Undefined name `Path`\n   --> canonical_flow/I_ingestion_preparation/preflight_validation.py:288:55\n    |\n286 |             print(f\"  \u2717 Smoke test exception: {str(e)}\")\n287 |     \n288 |     def _test_pdf_reader(self, module: Any, pdf_path: Path) -> Any:\n    |                                                       ^^^^\n289 |         \"\"\"Test PDF reader component.\"\"\"\n290 |         if hasattr(module, 'PDFPageIterator'):\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/I_ingestion_preparation/preflight_validation.py:288:64\n    |\n286 |             print(f\"  \u2717 Smoke test exception: {str(e)}\")\n287 |     \n288 |     def _test_pdf_reader(self, module: Any, pdf_path: Path) -> Any:\n    |                                                                ^^^\n289 |         \"\"\"Test PDF reader component.\"\"\"\n290 |         if hasattr(module, 'PDFPageIterator'):\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/I_ingestion_preparation/preflight_validation.py:304:45\n    |\n302 |             return module.process(str(pdf_path))\n303 |     \n304 |     def _test_advanced_loader(self, module: Any, pdf_path: Path) -> Any:\n    |                                             ^^^\n305 |         \"\"\"Test advanced loader component.\"\"\"\n306 |         return module.process(str(pdf_path))\n    |\n\nF821 Undefined name `Path`\n   --> canonical_flow/I_ingestion_preparation/preflight_validation.py:304:60\n    |\n302 |             return module.process(str(pdf_path))\n303 |     \n304 |     def _test_advanced_loader(self, module: Any, pdf_path: Path) -> Any:\n    |                                                            ^^^^\n305 |         \"\"\"Test advanced loader component.\"\"\"\n306 |         return module.process(str(pdf_path))\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/I_ingestion_preparation/preflight_validation.py:304:69\n    |\n302 |             return module.process(str(pdf_path))\n303 |     \n304 |     def _test_advanced_loader(self, module: Any, pdf_path: Path) -> Any:\n    |                                                                     ^^^\n305 |         \"\"\"Test advanced loader component.\"\"\"\n306 |         return module.process(str(pdf_path))\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/I_ingestion_preparation/preflight_validation.py:308:47\n    |\n306 |         return module.process(str(pdf_path))\n307 |     \n308 |     def _test_feature_extractor(self, module: Any, pdf_path: Path) -> Any:\n    |                                               ^^^\n309 |         \"\"\"Test feature extractor component.\"\"\"\n310 |         # Create minimal test data\n    |\n\nF821 Undefined name `Path`\n   --> canonical_flow/I_ingestion_preparation/preflight_validation.py:308:62\n    |\n306 |         return module.process(str(pdf_path))\n307 |     \n308 |     def _test_feature_extractor(self, module: Any, pdf_path: Path) -> Any:\n    |                                                              ^^^^\n309 |         \"\"\"Test feature extractor component.\"\"\"\n310 |         # Create minimal test data\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/I_ingestion_preparation/preflight_validation.py:308:71\n    |\n306 |         return module.process(str(pdf_path))\n307 |     \n308 |     def _test_feature_extractor(self, module: Any, pdf_path: Path) -> Any:\n    |                                                                       ^^^\n309 |         \"\"\"Test feature extractor component.\"\"\"\n310 |         # Create minimal test data\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/I_ingestion_preparation/preflight_validation.py:321:49\n    |\n319 |             return module.process(test_text, test_structure, test_metadata)\n320 |     \n321 |     def _test_normative_validator(self, module: Any) -> Any:\n    |                                                 ^^^\n322 |         \"\"\"Test normative validator component.\"\"\"\n323 |         # Create minimal test document\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/I_ingestion_preparation/preflight_validation.py:321:57\n    |\n319 |             return module.process(test_text, test_structure, test_metadata)\n320 |     \n321 |     def _test_normative_validator(self, module: Any) -> Any:\n    |                                                         ^^^\n322 |         \"\"\"Test normative validator component.\"\"\"\n323 |         # Create minimal test document\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/I_ingestion_preparation/preflight_validation.py:339:48\n    |\n337 |             return module.process(test_document)\n338 |     \n339 |     def _test_raw_data_generator(self, module: Any, pdf_path: Path) -> Any:\n    |                                                ^^^\n340 |         \"\"\"Test raw data generator component.\"\"\"\n341 |         test_documents = [\"Sample document 1 for testing.\", \"Sample document 2 for testing.\"]\n    |\n\nF821 Undefined name `Path`\n   --> canonical_flow/I_ingestion_preparation/preflight_validation.py:339:63\n    |\n337 |             return module.process(test_document)\n338 |     \n339 |     def _test_raw_data_generator(self, module: Any, pdf_path: Path) -> Any:\n    |                                                               ^^^^\n340 |         \"\"\"Test raw data generator component.\"\"\"\n341 |         test_documents = [\"Sample document 1 for testing.\", \"Sample document 2 for testing.\"]\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/I_ingestion_preparation/preflight_validation.py:339:72\n    |\n337 |             return module.process(test_document)\n338 |     \n339 |     def _test_raw_data_generator(self, module: Any, pdf_path: Path) -> Any:\n    |                                                                        ^^^\n340 |         \"\"\"Test raw data generator component.\"\"\"\n341 |         test_documents = [\"Sample document 1 for testing.\", \"Sample document 2 for testing.\"]\n    |\n\nF841 Local variable `generator` is assigned to but never used\n   --> canonical_flow/I_ingestion_preparation/preflight_validation.py:344:13\n    |\n343 |         if hasattr(module, 'RawDataArtifactGenerator'):\n344 |             generator = module.RawDataArtifactGenerator(\"test_output\")\n    |             ^^^^^^^^^\n345 |             # Mock the generate_all_artifacts method to avoid heavy processing\n346 |             return {\n    |\nhelp: Remove assignment to unused variable `generator`\n\nF821 Undefined name `Optional`\n   --> canonical_flow/I_ingestion_preparation/preflight_validation.py:354:34\n    |\n352 |             return module.process(test_documents)\n353 |     \n354 |     def _get_sample_pdf(self) -> Optional[Path]:\n    |                                  ^^^^^^^^\n355 |         \"\"\"Get a sample PDF file for testing.\"\"\"\n356 |         # Try project root first\n    |\n\nF821 Undefined name `Path`\n   --> canonical_flow/I_ingestion_preparation/preflight_validation.py:354:43\n    |\n352 |             return module.process(test_documents)\n353 |     \n354 |     def _get_sample_pdf(self) -> Optional[Path]:\n    |                                           ^^^^\n355 |         \"\"\"Get a sample PDF file for testing.\"\"\"\n356 |         # Try project root first\n    |\n\nF821 Undefined name `Path`\n   --> canonical_flow/I_ingestion_preparation/preflight_validation.py:365:26\n    |\n364 |         # Try planes_input in current directory\n365 |         current_planes = Path(\"planes_input\")\n    |                          ^^^^\n366 |         if current_planes.exists():\n367 |             pdf_files = list(current_planes.glob(\"*.pdf\"))\n    |\n\nF821 Undefined name `Optional`\n   --> canonical_flow/I_ingestion_preparation/preflight_validation.py:404:38\n    |\n402 |     \"\"\"Main preflight validation orchestrator.\"\"\"\n403 |     \n404 |     def __init__(self, project_root: Optional[Path] = None):\n    |                                      ^^^^^^^^\n405 |         \"\"\"Initialize preflight validator.\"\"\"\n406 |         if project_root is None:\n    |\n\nF821 Undefined name `Path`\n   --> canonical_flow/I_ingestion_preparation/preflight_validation.py:404:47\n    |\n402 |     \"\"\"Main preflight validation orchestrator.\"\"\"\n403 |     \n404 |     def __init__(self, project_root: Optional[Path] = None):\n    |                                               ^^^^\n405 |         \"\"\"Initialize preflight validator.\"\"\"\n406 |         if project_root is None:\n    |\n\nF821 Undefined name `Path`\n   --> canonical_flow/I_ingestion_preparation/preflight_validation.py:407:28\n    |\n405 |         \"\"\"Initialize preflight validator.\"\"\"\n406 |         if project_root is None:\n407 |             project_root = Path(__file__).resolve().parents[2]  # Go up to project root\n    |                            ^^^^\n408 |         \n409 |         self.project_root = project_root\n    |\n\nF821 Undefined name `Optional`\n   --> canonical_flow/I_ingestion_preparation/preflight_validation.py:414:52\n    |\n412 |         self.report_dir.mkdir(exist_ok=True)\n413 |     \n414 |     def run_preflight_validation(self, components: Optional[List[str]] = None) -> PreflightReport:\n    |                                                    ^^^^^^^^\n415 |         \"\"\"Run comprehensive preflight validation on all or specified components.\"\"\"\n416 |         print(\"\ud83d\ude80 Starting Preflight Validation for I_ingestion_preparation Pipeline\")\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/I_ingestion_preparation/preflight_validation.py:414:61\n    |\n412 |         self.report_dir.mkdir(exist_ok=True)\n413 |     \n414 |     def run_preflight_validation(self, components: Optional[List[str]] = None) -> PreflightReport:\n    |                                                             ^^^^\n415 |         \"\"\"Run comprehensive preflight validation on all or specified components.\"\"\"\n416 |         print(\"\ud83d\ude80 Starting Preflight Validation for I_ingestion_preparation Pipeline\")\n    |\n\nF821 Undefined name `datetime`\n   --> canonical_flow/I_ingestion_preparation/preflight_validation.py:423:23\n    |\n422 |         report = PreflightReport(\n423 |             timestamp=datetime.now(),\n    |                       ^^^^^^^^\n424 |             total_components=len(components),\n425 |             components_ready=0,\n    |\n\nF401 [*] `uuid` imported but unused\n  --> canonical_flow/K_knowledge_extraction/advanced_knowledge_graph_builder.py:13:8\n   |\n12 | import re\n13 | import uuid\n   |        ^^^^\n14 | import hashlib\n15 | import json\n   |\nhelp: Remove unused import: `uuid`\n\nF401 [*] `threading` imported but unused\n  --> canonical_flow/K_knowledge_extraction/advanced_knowledge_graph_builder.py:24:8\n   |\n22 | import logging\n23 | # # # from abc import ABC, abstractmethod  # Module not found  # Module not found  # Module not found\n24 | import threading\n   |        ^^^^^^^^^\n25 | # # # from concurrent.futures import ThreadPoolExecutor, as_completed  # Module not found  # Module not found  # Module not found\n   |\nhelp: Remove unused import: `threading`\n\nF821 Undefined name `dataclass`\n  --> canonical_flow/K_knowledge_extraction/advanced_knowledge_graph_builder.py:54:6\n   |\n52 |         return logging.getLogger(__name__)\n53 |     \n54 |     @dataclass\n   |      ^^^^^^^^^\n55 |     class EvidenceChunk:\n56 |         chunk_id: str\n   |\n\nF821 Undefined name `datetime`\n  --> canonical_flow/K_knowledge_extraction/advanced_knowledge_graph_builder.py:62:31\n   |\n60 |         start_position: int = 0\n61 |         end_position: int = 0\n62 |         processing_timestamp: datetime = field(default_factory=datetime.now)\n   |                               ^^^^^^^^\n63 |         raw_text: str = \"\"\n   |\n\nF821 Undefined name `field`\n  --> canonical_flow/K_knowledge_extraction/advanced_knowledge_graph_builder.py:62:42\n   |\n60 |         start_position: int = 0\n61 |         end_position: int = 0\n62 |         processing_timestamp: datetime = field(default_factory=datetime.now)\n   |                                          ^^^^^\n63 |         raw_text: str = \"\"\n   |\n\nF821 Undefined name `datetime`\n  --> canonical_flow/K_knowledge_extraction/advanced_knowledge_graph_builder.py:62:64\n   |\n60 |         start_position: int = 0\n61 |         end_position: int = 0\n62 |         processing_timestamp: datetime = field(default_factory=datetime.now)\n   |                                                                ^^^^^^^^\n63 |         raw_text: str = \"\"\n   |\n\nF821 Undefined name `dataclass`\n  --> canonical_flow/K_knowledge_extraction/advanced_knowledge_graph_builder.py:65:6\n   |\n63 |         raw_text: str = \"\"\n64 |         \n65 |     @dataclass\n   |      ^^^^^^^^^\n66 |     class SourceMetadata:\n67 |         document_id: str\n   |\n\nF821 Undefined name `Optional`\n  --> canonical_flow/K_knowledge_extraction/advanced_knowledge_graph_builder.py:70:27\n   |\n68 |         title: str\n69 |         author: str = \"\"\n70 |         publication_date: Optional[datetime] = None\n   |                           ^^^^^^^^\n71 |         page_number: Optional[int] = None\n72 |         section_header: Optional[str] = None\n   |\n\nF821 Undefined name `datetime`\n  --> canonical_flow/K_knowledge_extraction/advanced_knowledge_graph_builder.py:70:36\n   |\n68 |         title: str\n69 |         author: str = \"\"\n70 |         publication_date: Optional[datetime] = None\n   |                                    ^^^^^^^^\n71 |         page_number: Optional[int] = None\n72 |         section_header: Optional[str] = None\n   |\n\nF821 Undefined name `Optional`\n  --> canonical_flow/K_knowledge_extraction/advanced_knowledge_graph_builder.py:71:22\n   |\n69 |         author: str = \"\"\n70 |         publication_date: Optional[datetime] = None\n71 |         page_number: Optional[int] = None\n   |                      ^^^^^^^^\n72 |         section_header: Optional[str] = None\n   |\n\nF821 Undefined name `Optional`\n  --> canonical_flow/K_knowledge_extraction/advanced_knowledge_graph_builder.py:72:25\n   |\n70 |         publication_date: Optional[datetime] = None\n71 |         page_number: Optional[int] = None\n72 |         section_header: Optional[str] = None\n   |                         ^^^^^^^^\n73 |         \n74 |     @dataclass\n   |\n\nF821 Undefined name `dataclass`\n  --> canonical_flow/K_knowledge_extraction/advanced_knowledge_graph_builder.py:74:6\n   |\n72 |         section_header: Optional[str] = None\n73 |         \n74 |     @dataclass\n   |      ^^^^^^^^^\n75 |     class DataModel:\n76 |         id: str\n   |\n\nF821 Undefined name `dataclass`\n   --> canonical_flow/K_knowledge_extraction/advanced_knowledge_graph_builder.py:104:2\n    |\n102 | }\n103 |\n104 | @dataclass\n    |  ^^^^^^^^^\n105 | class KGNode:\n106 |     \"\"\"Knowledge Graph Node with comprehensive metadata.\"\"\"\n    |\n\nF821 Undefined name `Optional`\n   --> canonical_flow/K_knowledge_extraction/advanced_knowledge_graph_builder.py:112:18\n    |\n110 |     type: str\n111 |     confidence: float\n112 |     page_number: Optional[int]\n    |                  ^^^^^^^^\n113 |     chunk_id: str\n114 |     document_id: str\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/K_knowledge_extraction/advanced_knowledge_graph_builder.py:118:14\n    |\n116 |     end_position: int\n117 |     context: str\n118 |     aliases: List[str] = field(default_factory=list)\n    |              ^^^^\n119 |     properties: Dict[str, Any] = field(default_factory=dict)\n120 |     provenance: Dict[str, Any] = field(default_factory=dict)\n    |\n\nF821 Undefined name `field`\n   --> canonical_flow/K_knowledge_extraction/advanced_knowledge_graph_builder.py:118:26\n    |\n116 |     end_position: int\n117 |     context: str\n118 |     aliases: List[str] = field(default_factory=list)\n    |                          ^^^^^\n119 |     properties: Dict[str, Any] = field(default_factory=dict)\n120 |     provenance: Dict[str, Any] = field(default_factory=dict)\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/K_knowledge_extraction/advanced_knowledge_graph_builder.py:119:17\n    |\n117 |     context: str\n118 |     aliases: List[str] = field(default_factory=list)\n119 |     properties: Dict[str, Any] = field(default_factory=dict)\n    |                 ^^^^\n120 |     provenance: Dict[str, Any] = field(default_factory=dict)\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/K_knowledge_extraction/advanced_knowledge_graph_builder.py:119:27\n    |\n117 |     context: str\n118 |     aliases: List[str] = field(default_factory=list)\n119 |     properties: Dict[str, Any] = field(default_factory=dict)\n    |                           ^^^\n120 |     provenance: Dict[str, Any] = field(default_factory=dict)\n    |\n\nF821 Undefined name `field`\n   --> canonical_flow/K_knowledge_extraction/advanced_knowledge_graph_builder.py:119:34\n    |\n117 |     context: str\n118 |     aliases: List[str] = field(default_factory=list)\n119 |     properties: Dict[str, Any] = field(default_factory=dict)\n    |                                  ^^^^^\n120 |     provenance: Dict[str, Any] = field(default_factory=dict)\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/K_knowledge_extraction/advanced_knowledge_graph_builder.py:120:17\n    |\n118 |     aliases: List[str] = field(default_factory=list)\n119 |     properties: Dict[str, Any] = field(default_factory=dict)\n120 |     provenance: Dict[str, Any] = field(default_factory=dict)\n    |                 ^^^^\n121 |     \n122 |     def to_dict(self) -> Dict[str, Any]:\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/K_knowledge_extraction/advanced_knowledge_graph_builder.py:120:27\n    |\n118 |     aliases: List[str] = field(default_factory=list)\n119 |     properties: Dict[str, Any] = field(default_factory=dict)\n120 |     provenance: Dict[str, Any] = field(default_factory=dict)\n    |                           ^^^\n121 |     \n122 |     def to_dict(self) -> Dict[str, Any]:\n    |\n\nF821 Undefined name `field`\n   --> canonical_flow/K_knowledge_extraction/advanced_knowledge_graph_builder.py:120:34\n    |\n118 |     aliases: List[str] = field(default_factory=list)\n119 |     properties: Dict[str, Any] = field(default_factory=dict)\n120 |     provenance: Dict[str, Any] = field(default_factory=dict)\n    |                                  ^^^^^\n121 |     \n122 |     def to_dict(self) -> Dict[str, Any]:\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/K_knowledge_extraction/advanced_knowledge_graph_builder.py:122:26\n    |\n120 |     provenance: Dict[str, Any] = field(default_factory=dict)\n121 |     \n122 |     def to_dict(self) -> Dict[str, Any]:\n    |                          ^^^^\n123 |         \"\"\"Convert to dictionary for JSON serialization.\"\"\"\n124 |         return {\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/K_knowledge_extraction/advanced_knowledge_graph_builder.py:122:36\n    |\n120 |     provenance: Dict[str, Any] = field(default_factory=dict)\n121 |     \n122 |     def to_dict(self) -> Dict[str, Any]:\n    |                                    ^^^\n123 |         \"\"\"Convert to dictionary for JSON serialization.\"\"\"\n124 |         return {\n    |\n\nF821 Undefined name `dataclass`\n   --> canonical_flow/K_knowledge_extraction/advanced_knowledge_graph_builder.py:140:2\n    |\n138 |         }\n139 |\n140 | @dataclass \n    |  ^^^^^^^^^\n141 | class KGEdge:\n142 |     \"\"\"Knowledge Graph Edge with provenance tracking.\"\"\"\n    |\n\nF821 Undefined name `Optional`\n   --> canonical_flow/K_knowledge_extraction/advanced_knowledge_graph_builder.py:149:18\n    |\n147 |     type: str\n148 |     confidence: float\n149 |     page_number: Optional[int]\n    |                  ^^^^^^^^\n150 |     chunk_id: str\n151 |     document_id: str\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/K_knowledge_extraction/advanced_knowledge_graph_builder.py:154:17\n    |\n152 |     context: str\n153 |     extraction_method: str\n154 |     properties: Dict[str, Any] = field(default_factory=dict)\n    |                 ^^^^\n155 |     provenance: Dict[str, Any] = field(default_factory=dict)\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/K_knowledge_extraction/advanced_knowledge_graph_builder.py:154:27\n    |\n152 |     context: str\n153 |     extraction_method: str\n154 |     properties: Dict[str, Any] = field(default_factory=dict)\n    |                           ^^^\n155 |     provenance: Dict[str, Any] = field(default_factory=dict)\n    |\n\nF821 Undefined name `field`\n   --> canonical_flow/K_knowledge_extraction/advanced_knowledge_graph_builder.py:154:34\n    |\n152 |     context: str\n153 |     extraction_method: str\n154 |     properties: Dict[str, Any] = field(default_factory=dict)\n    |                                  ^^^^^\n155 |     provenance: Dict[str, Any] = field(default_factory=dict)\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/K_knowledge_extraction/advanced_knowledge_graph_builder.py:155:17\n    |\n153 |     extraction_method: str\n154 |     properties: Dict[str, Any] = field(default_factory=dict)\n155 |     provenance: Dict[str, Any] = field(default_factory=dict)\n    |                 ^^^^\n156 |     \n157 |     def to_dict(self) -> Dict[str, Any]:\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/K_knowledge_extraction/advanced_knowledge_graph_builder.py:155:27\n    |\n153 |     extraction_method: str\n154 |     properties: Dict[str, Any] = field(default_factory=dict)\n155 |     provenance: Dict[str, Any] = field(default_factory=dict)\n    |                           ^^^\n156 |     \n157 |     def to_dict(self) -> Dict[str, Any]:\n    |\n\nF821 Undefined name `field`\n   --> canonical_flow/K_knowledge_extraction/advanced_knowledge_graph_builder.py:155:34\n    |\n153 |     extraction_method: str\n154 |     properties: Dict[str, Any] = field(default_factory=dict)\n155 |     provenance: Dict[str, Any] = field(default_factory=dict)\n    |                                  ^^^^^\n156 |     \n157 |     def to_dict(self) -> Dict[str, Any]:\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/K_knowledge_extraction/advanced_knowledge_graph_builder.py:157:26\n    |\n155 |     provenance: Dict[str, Any] = field(default_factory=dict)\n156 |     \n157 |     def to_dict(self) -> Dict[str, Any]:\n    |                          ^^^^\n158 |         \"\"\"Convert to dictionary for JSON serialization.\"\"\"\n159 |         return {\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/K_knowledge_extraction/advanced_knowledge_graph_builder.py:157:36\n    |\n155 |     provenance: Dict[str, Any] = field(default_factory=dict)\n156 |     \n157 |     def to_dict(self) -> Dict[str, Any]:\n    |                                    ^^^\n158 |         \"\"\"Convert to dictionary for JSON serialization.\"\"\"\n159 |         return {\n    |\n\nF821 Undefined name `dataclass`\n   --> canonical_flow/K_knowledge_extraction/advanced_knowledge_graph_builder.py:174:2\n    |\n172 |         }\n173 |\n174 | @dataclass\n    |  ^^^^^^^^^\n175 | class ValidationError:\n176 |     \"\"\"Validation error with details.\"\"\"\n    |\n\nF821 Undefined name `Optional`\n   --> canonical_flow/K_knowledge_extraction/advanced_knowledge_graph_builder.py:180:16\n    |\n178 |     error_type: str\n179 |     message: str\n180 |     entity_id: Optional[str] = None\n    |                ^^^^^^^^\n181 |     severity: str = \"ERROR\"\n182 |     context: Dict[str, Any] = field(default_factory=dict)\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/K_knowledge_extraction/advanced_knowledge_graph_builder.py:182:14\n    |\n180 |     entity_id: Optional[str] = None\n181 |     severity: str = \"ERROR\"\n182 |     context: Dict[str, Any] = field(default_factory=dict)\n    |              ^^^^\n183 |\n184 | class KnowledgeGraphBuilder:\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/K_knowledge_extraction/advanced_knowledge_graph_builder.py:182:24\n    |\n180 |     entity_id: Optional[str] = None\n181 |     severity: str = \"ERROR\"\n182 |     context: Dict[str, Any] = field(default_factory=dict)\n    |                        ^^^\n183 |\n184 | class KnowledgeGraphBuilder:\n    |\n\nF821 Undefined name `field`\n   --> canonical_flow/K_knowledge_extraction/advanced_knowledge_graph_builder.py:182:31\n    |\n180 |     entity_id: Optional[str] = None\n181 |     severity: str = \"ERROR\"\n182 |     context: Dict[str, Any] = field(default_factory=dict)\n    |                               ^^^^^\n183 |\n184 | class KnowledgeGraphBuilder:\n    |\n\nF821 Undefined name `Optional`\n   --> canonical_flow/K_knowledge_extraction/advanced_knowledge_graph_builder.py:187:32\n    |\n185 |     \"\"\"Advanced Knowledge Graph Builder with standardized process() API.\"\"\"\n186 |     \n187 |     def __init__(self, config: Optional[Dict[str, Any]] = None):\n    |                                ^^^^^^^^\n188 |         \"\"\"Initialize the knowledge graph builder.\"\"\"\n189 |         self.config = config or {}\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/K_knowledge_extraction/advanced_knowledge_graph_builder.py:187:41\n    |\n185 |     \"\"\"Advanced Knowledge Graph Builder with standardized process() API.\"\"\"\n186 |     \n187 |     def __init__(self, config: Optional[Dict[str, Any]] = None):\n    |                                         ^^^^\n188 |         \"\"\"Initialize the knowledge graph builder.\"\"\"\n189 |         self.config = config or {}\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/K_knowledge_extraction/advanced_knowledge_graph_builder.py:187:51\n    |\n185 |     \"\"\"Advanced Knowledge Graph Builder with standardized process() API.\"\"\"\n186 |     \n187 |     def __init__(self, config: Optional[Dict[str, Any]] = None):\n    |                                                   ^^^\n188 |         \"\"\"Initialize the knowledge graph builder.\"\"\"\n189 |         self.config = config or {}\n    |\n\nF821 Undefined name `Optional`\n   --> canonical_flow/K_knowledge_extraction/advanced_knowledge_graph_builder.py:293:71\n    |\n292 |     def extract_entities_with_provenance(self, text: str, chunk_id: str, \n293 |                                        document_id: str, page_number: Optional[int],\n    |                                                                       ^^^^^^^^\n294 |                                        metadata: Optional[Dict[str, Any]] = None) -> List[KGNode]:\n295 |         \"\"\"Extract entities with page-anchored provenance tracking.\"\"\"\n    |\n\nF821 Undefined name `Optional`\n   --> canonical_flow/K_knowledge_extraction/advanced_knowledge_graph_builder.py:294:50\n    |\n292 |     def extract_entities_with_provenance(self, text: str, chunk_id: str, \n293 |                                        document_id: str, page_number: Optional[int],\n294 |                                        metadata: Optional[Dict[str, Any]] = None) -> List[KGNode]:\n    |                                                  ^^^^^^^^\n295 |         \"\"\"Extract entities with page-anchored provenance tracking.\"\"\"\n296 |         entities = []\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/K_knowledge_extraction/advanced_knowledge_graph_builder.py:294:59\n    |\n292 |     def extract_entities_with_provenance(self, text: str, chunk_id: str, \n293 |                                        document_id: str, page_number: Optional[int],\n294 |                                        metadata: Optional[Dict[str, Any]] = None) -> List[KGNode]:\n    |                                                           ^^^^\n295 |         \"\"\"Extract entities with page-anchored provenance tracking.\"\"\"\n296 |         entities = []\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/K_knowledge_extraction/advanced_knowledge_graph_builder.py:294:69\n    |\n292 |     def extract_entities_with_provenance(self, text: str, chunk_id: str, \n293 |                                        document_id: str, page_number: Optional[int],\n294 |                                        metadata: Optional[Dict[str, Any]] = None) -> List[KGNode]:\n    |                                                                     ^^^\n295 |         \"\"\"Extract entities with page-anchored provenance tracking.\"\"\"\n296 |         entities = []\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/K_knowledge_extraction/advanced_knowledge_graph_builder.py:294:86\n    |\n292 |     def extract_entities_with_provenance(self, text: str, chunk_id: str, \n293 |                                        document_id: str, page_number: Optional[int],\n294 |                                        metadata: Optional[Dict[str, Any]] = None) -> List[KGNode]:\n    |                                                                                      ^^^^\n295 |         \"\"\"Extract entities with page-anchored provenance tracking.\"\"\"\n296 |         entities = []\n    |\n\nF821 Undefined name `datetime`\n   --> canonical_flow/K_knowledge_extraction/advanced_knowledge_graph_builder.py:318:53\n    |\n316 |                         # Create provenance tracking\n317 |                         provenance = {\n318 |                             'extraction_timestamp': datetime.now().isoformat(),\n    |                                                     ^^^^^^^^\n319 |                             'extraction_method': 'spacy_ner',\n320 |                             'spacy_label': ent.label_,\n    |\n\nF821 Undefined name `datetime`\n   --> canonical_flow/K_knowledge_extraction/advanced_knowledge_graph_builder.py:360:53\n    |\n359 |                         provenance = {\n360 |                             'extraction_timestamp': datetime.now().isoformat(),\n    |                                                     ^^^^^^^^\n361 |                             'extraction_method': 'regex_pattern',\n362 |                             'pattern_used': pattern,\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/K_knowledge_extraction/advanced_knowledge_graph_builder.py:384:70\n    |\n382 |         return entities\n383 |     \n384 |     def extract_relations_with_provenance(self, text: str, entities: List[KGNode],\n    |                                                                      ^^^^\n385 |                                         chunk_id: str, document_id: str,\n386 |                                         page_number: Optional[int]) -> List[KGEdge]:\n    |\n\nF821 Undefined name `Optional`\n   --> canonical_flow/K_knowledge_extraction/advanced_knowledge_graph_builder.py:386:54\n    |\n384 |     def extract_relations_with_provenance(self, text: str, entities: List[KGNode],\n385 |                                         chunk_id: str, document_id: str,\n386 |                                         page_number: Optional[int]) -> List[KGEdge]:\n    |                                                      ^^^^^^^^\n387 |         \"\"\"Extract relations with provenance tracking.\"\"\"\n388 |         relations = []\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/K_knowledge_extraction/advanced_knowledge_graph_builder.py:386:72\n    |\n384 |     def extract_relations_with_provenance(self, text: str, entities: List[KGNode],\n385 |                                         chunk_id: str, document_id: str,\n386 |                                         page_number: Optional[int]) -> List[KGEdge]:\n    |                                                                        ^^^^\n387 |         \"\"\"Extract relations with provenance tracking.\"\"\"\n388 |         relations = []\n    |\n\nF821 Undefined name `datetime`\n   --> canonical_flow/K_knowledge_extraction/advanced_knowledge_graph_builder.py:416:57\n    |\n415 | \u2026                     provenance = {\n416 | \u2026                         'extraction_timestamp': datetime.now().isoformat(),\n    |                                                   ^^^^^^^^\n417 | \u2026                         'extraction_method': 'regex_pattern',\n418 | \u2026                         'pattern_used': pattern,\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/K_knowledge_extraction/advanced_knowledge_graph_builder.py:451:40\n    |\n449 |         return mapping.get(spacy_label, 'CONCEPT')\n450 |     \n451 |     def validate_ontology(self, nodes: List[KGNode], edges: List[KGEdge]) -> List[ValidationError]:\n    |                                        ^^^^\n452 |         \"\"\"Validate nodes and edges against predefined ontology.\"\"\"\n453 |         errors = []\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/K_knowledge_extraction/advanced_knowledge_graph_builder.py:451:61\n    |\n449 |         return mapping.get(spacy_label, 'CONCEPT')\n450 |     \n451 |     def validate_ontology(self, nodes: List[KGNode], edges: List[KGEdge]) -> List[ValidationError]:\n    |                                                             ^^^^\n452 |         \"\"\"Validate nodes and edges against predefined ontology.\"\"\"\n453 |         errors = []\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/K_knowledge_extraction/advanced_knowledge_graph_builder.py:451:78\n    |\n449 |         return mapping.get(spacy_label, 'CONCEPT')\n450 |     \n451 |     def validate_ontology(self, nodes: List[KGNode], edges: List[KGEdge]) -> List[ValidationError]:\n    |                                                                              ^^^^\n452 |         \"\"\"Validate nodes and edges against predefined ontology.\"\"\"\n453 |         errors = []\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/K_knowledge_extraction/advanced_knowledge_graph_builder.py:502:44\n    |\n500 |         return errors\n501 |     \n502 |     def process(self, document_id: str) -> Dict[str, Any]:\n    |                                            ^^^^\n503 |         \"\"\"\n504 |         Standardized process() API that consumes chunk and entity data \n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/K_knowledge_extraction/advanced_knowledge_graph_builder.py:502:54\n    |\n500 |         return errors\n501 |     \n502 |     def process(self, document_id: str) -> Dict[str, Any]:\n    |                                                      ^^^\n503 |         \"\"\"\n504 |         Standardized process() API that consumes chunk and entity data \n    |\n\nF821 Undefined name `datetime`\n   --> canonical_flow/K_knowledge_extraction/advanced_knowledge_graph_builder.py:514:26\n    |\n512 |         \"\"\"\n513 |         try:\n514 |             start_time = datetime.now()\n    |                          ^^^^^^^^\n515 |             \n516 |             # Extract document stem for directory structure\n    |\n\nF821 Undefined name `Path`\n   --> canonical_flow/K_knowledge_extraction/advanced_knowledge_graph_builder.py:518:26\n    |\n516 |             # Extract document stem for directory structure\n517 |             document_stem = document_id.replace('/', '_').replace('\\\\', '_')\n518 |             output_dir = Path(f\"canonical_flow/knowledge/{document_stem}\")\n    |                          ^^^^\n519 |             output_dir.mkdir(parents=True, exist_ok=True)\n    |\n\nF841 Local variable `entities_data` is assigned to but never used\n   --> canonical_flow/K_knowledge_extraction/advanced_knowledge_graph_builder.py:523:13\n    |\n521 |             # Load existing chunk data (mock implementation)\n522 |             chunks_data = self._load_chunk_data(document_id)\n523 |             entities_data = self._load_entities_data(document_id)\n    |             ^^^^^^^^^^^^^\n524 |             \n525 |             all_nodes = []\n    |\nhelp: Remove assignment to unused variable `entities_data`\n\nF821 Undefined name `datetime`\n   --> canonical_flow/K_knowledge_extraction/advanced_knowledge_graph_builder.py:561:24\n    |\n560 |             # Generate metadata\n561 |             end_time = datetime.now()\n    |                        ^^^^^^^^\n562 |             processing_duration = (end_time - start_time).total_seconds()\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/K_knowledge_extraction/advanced_knowledge_graph_builder.py:617:53\n    |\n615 |             }\n616 |     \n617 |     def _load_chunk_data(self, document_id: str) -> List[Dict[str, Any]]:\n    |                                                     ^^^^\n618 |         \"\"\"Load chunk data for the document (mock implementation).\"\"\"\n619 | # # #         # In a real implementation, this would load from storage  # Module not found  # Module not found  # Module not found\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/K_knowledge_extraction/advanced_knowledge_graph_builder.py:617:58\n    |\n615 |             }\n616 |     \n617 |     def _load_chunk_data(self, document_id: str) -> List[Dict[str, Any]]:\n    |                                                          ^^^^\n618 |         \"\"\"Load chunk data for the document (mock implementation).\"\"\"\n619 | # # #         # In a real implementation, this would load from storage  # Module not found  # Module not found  # Module not found\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/K_knowledge_extraction/advanced_knowledge_graph_builder.py:617:68\n    |\n615 |             }\n616 |     \n617 |     def _load_chunk_data(self, document_id: str) -> List[Dict[str, Any]]:\n    |                                                                    ^^^\n618 |         \"\"\"Load chunk data for the document (mock implementation).\"\"\"\n619 | # # #         # In a real implementation, this would load from storage  # Module not found  # Module not found  # Module not found\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/K_knowledge_extraction/advanced_knowledge_graph_builder.py:633:56\n    |\n631 |         ]\n632 |     \n633 |     def _load_entities_data(self, document_id: str) -> List[Dict[str, Any]]:\n    |                                                        ^^^^\n634 |         \"\"\"Load existing entities data for the document (mock implementation).\"\"\"\n635 | # # #         # In a real implementation, this would load from storage  # Module not found  # Module not found  # Module not found\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/K_knowledge_extraction/advanced_knowledge_graph_builder.py:633:61\n    |\n631 |         ]\n632 |     \n633 |     def _load_entities_data(self, document_id: str) -> List[Dict[str, Any]]:\n    |                                                             ^^^^\n634 |         \"\"\"Load existing entities data for the document (mock implementation).\"\"\"\n635 | # # #         # In a real implementation, this would load from storage  # Module not found  # Module not found  # Module not found\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/K_knowledge_extraction/advanced_knowledge_graph_builder.py:633:71\n    |\n631 |         ]\n632 |     \n633 |     def _load_entities_data(self, document_id: str) -> List[Dict[str, Any]]:\n    |                                                                       ^^^\n634 |         \"\"\"Load existing entities data for the document (mock implementation).\"\"\"\n635 | # # #         # In a real implementation, this would load from storage  # Module not found  # Module not found  # Module not found\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/K_knowledge_extraction/advanced_knowledge_graph_builder.py:638:57\n    |\n636 |         return []\n637 |     \n638 |     def _calculate_confidence_distribution(self, nodes: List[KGNode], \n    |                                                         ^^^^\n639 |                                          edges: List[KGEdge]) -> Dict[str, float]:\n640 |         \"\"\"Calculate distribution of confidence scores.\"\"\"\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/K_knowledge_extraction/advanced_knowledge_graph_builder.py:639:49\n    |\n638 |     def _calculate_confidence_distribution(self, nodes: List[KGNode], \n639 |                                          edges: List[KGEdge]) -> Dict[str, float]:\n    |                                                 ^^^^\n640 |         \"\"\"Calculate distribution of confidence scores.\"\"\"\n641 |         all_confidences = [n.confidence for n in nodes] + [e.confidence for e in edges]\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/K_knowledge_extraction/advanced_knowledge_graph_builder.py:639:66\n    |\n638 |     def _calculate_confidence_distribution(self, nodes: List[KGNode], \n639 |                                          edges: List[KGEdge]) -> Dict[str, float]:\n    |                                                                  ^^^^\n640 |         \"\"\"Calculate distribution of confidence scores.\"\"\"\n641 |         all_confidences = [n.confidence for n in nodes] + [e.confidence for e in edges]\n    |\n\nF821 Undefined name `Path`\n   --> canonical_flow/K_knowledge_extraction/advanced_knowledge_graph_builder.py:652:47\n    |\n650 |         }\n651 |     \n652 |     def _write_kg_artifacts(self, output_dir: Path, nodes: List[KGNode],\n    |                                               ^^^^\n653 |                           edges: List[KGEdge], validation_errors: List[ValidationError],\n654 |                           metadata: Dict[str, Any]):\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/K_knowledge_extraction/advanced_knowledge_graph_builder.py:652:60\n    |\n650 |         }\n651 |     \n652 |     def _write_kg_artifacts(self, output_dir: Path, nodes: List[KGNode],\n    |                                                            ^^^^\n653 |                           edges: List[KGEdge], validation_errors: List[ValidationError],\n654 |                           metadata: Dict[str, Any]):\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/K_knowledge_extraction/advanced_knowledge_graph_builder.py:653:34\n    |\n652 |     def _write_kg_artifacts(self, output_dir: Path, nodes: List[KGNode],\n653 |                           edges: List[KGEdge], validation_errors: List[ValidationError],\n    |                                  ^^^^\n654 |                           metadata: Dict[str, Any]):\n655 |         \"\"\"Write knowledge graph artifacts to canonical directory structure.\"\"\"\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/K_knowledge_extraction/advanced_knowledge_graph_builder.py:653:67\n    |\n652 |     def _write_kg_artifacts(self, output_dir: Path, nodes: List[KGNode],\n653 |                           edges: List[KGEdge], validation_errors: List[ValidationError],\n    |                                                                   ^^^^\n654 |                           metadata: Dict[str, Any]):\n655 |         \"\"\"Write knowledge graph artifacts to canonical directory structure.\"\"\"\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/K_knowledge_extraction/advanced_knowledge_graph_builder.py:654:37\n    |\n652 |     def _write_kg_artifacts(self, output_dir: Path, nodes: List[KGNode],\n653 |                           edges: List[KGEdge], validation_errors: List[ValidationError],\n654 |                           metadata: Dict[str, Any]):\n    |                                     ^^^^\n655 |         \"\"\"Write knowledge graph artifacts to canonical directory structure.\"\"\"\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/K_knowledge_extraction/advanced_knowledge_graph_builder.py:654:47\n    |\n652 |     def _write_kg_artifacts(self, output_dir: Path, nodes: List[KGNode],\n653 |                           edges: List[KGEdge], validation_errors: List[ValidationError],\n654 |                           metadata: Dict[str, Any]):\n    |                                               ^^^\n655 |         \"\"\"Write knowledge graph artifacts to canonical directory structure.\"\"\"\n    |\n\nF821 Undefined name `Optional`\n   --> canonical_flow/K_knowledge_extraction/advanced_knowledge_graph_builder.py:699:41\n    |\n697 | _kg_builder = None\n698 |\n699 | def get_knowledge_graph_builder(config: Optional[Dict[str, Any]] = None) -> KnowledgeGraphBuilder:\n    |                                         ^^^^^^^^\n700 |     \"\"\"Get singleton knowledge graph builder instance.\"\"\"\n701 |     global _kg_builder\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/K_knowledge_extraction/advanced_knowledge_graph_builder.py:699:50\n    |\n697 | _kg_builder = None\n698 |\n699 | def get_knowledge_graph_builder(config: Optional[Dict[str, Any]] = None) -> KnowledgeGraphBuilder:\n    |                                                  ^^^^\n700 |     \"\"\"Get singleton knowledge graph builder instance.\"\"\"\n701 |     global _kg_builder\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/K_knowledge_extraction/advanced_knowledge_graph_builder.py:699:60\n    |\n697 | _kg_builder = None\n698 |\n699 | def get_knowledge_graph_builder(config: Optional[Dict[str, Any]] = None) -> KnowledgeGraphBuilder:\n    |                                                            ^^^\n700 |     \"\"\"Get singleton knowledge graph builder instance.\"\"\"\n701 |     global _kg_builder\n    |\n\nF821 Undefined name `Optional`\n   --> canonical_flow/K_knowledge_extraction/advanced_knowledge_graph_builder.py:706:39\n    |\n704 |     return _kg_builder\n705 |\n706 | def process(document_id: str, config: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:\n    |                                       ^^^^^^^^\n707 |     \"\"\"Standardized process function for canonical flow integration.\"\"\"\n708 |     builder = get_knowledge_graph_builder(config)\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/K_knowledge_extraction/advanced_knowledge_graph_builder.py:706:48\n    |\n704 |     return _kg_builder\n705 |\n706 | def process(document_id: str, config: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:\n    |                                                ^^^^\n707 |     \"\"\"Standardized process function for canonical flow integration.\"\"\"\n708 |     builder = get_knowledge_graph_builder(config)\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/K_knowledge_extraction/advanced_knowledge_graph_builder.py:706:58\n    |\n704 |     return _kg_builder\n705 |\n706 | def process(document_id: str, config: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:\n    |                                                          ^^^\n707 |     \"\"\"Standardized process function for canonical flow integration.\"\"\"\n708 |     builder = get_knowledge_graph_builder(config)\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/K_knowledge_extraction/advanced_knowledge_graph_builder.py:706:75\n    |\n704 |     return _kg_builder\n705 |\n706 | def process(document_id: str, config: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:\n    |                                                                           ^^^^\n707 |     \"\"\"Standardized process function for canonical flow integration.\"\"\"\n708 |     builder = get_knowledge_graph_builder(config)\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/K_knowledge_extraction/advanced_knowledge_graph_builder.py:706:85\n    |\n704 |     return _kg_builder\n705 |\n706 | def process(document_id: str, config: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:\n    |                                                                                     ^^^\n707 |     \"\"\"Standardized process function for canonical flow integration.\"\"\"\n708 |     builder = get_knowledge_graph_builder(config)\n    |\n\nF821 Undefined name `Path`\n  --> canonical_flow/K_knowledge_extraction/causal_dnp_framework.py:24:20\n   |\n22 | try:\n23 |     # Add project root to path for imports\n24 |     project_root = Path(__file__).resolve().parents[1]\n   |                    ^^^^\n25 |     if str(project_root) not in sys.path:\n26 |         sys.path.insert(0, str(project_root))\n   |\n\nF821 Undefined name `importlib_util`\n  --> canonical_flow/K_knowledge_extraction/causal_dnp_framework.py:31:16\n   |\n29 |     original_file = project_root / \"causal_dnp_framework.py\"\n30 |     if original_file.exists():\n31 |         spec = importlib_util.spec_from_file_location(\n   |                ^^^^^^^^^^^^^^\n32 |             f\"original_{alias_code.lower()}\", \n33 |             str(original_file)\n   |\n\nF821 Undefined name `importlib_util`\n  --> canonical_flow/K_knowledge_extraction/causal_dnp_framework.py:37:31\n   |\n36 |         if spec and spec.loader:\n37 |             original_module = importlib_util.module_from_spec(spec)\n   |                               ^^^^^^^^^^^^^^\n38 |             spec.loader.exec_module(original_module)\n   |\n\nF821 Undefined name `e`\n  --> canonical_flow/K_knowledge_extraction/causal_dnp_framework.py:56:67\n   |\n54 |     def process(data=None, context=None):\n55 |         \"\"\"Placeholder process function for failed import.\"\"\"\n56 |         return {\"error\": f\"Module {alias_source} failed to load: {e}\"}\n   |                                                                   ^\n   |\n\nF821 Undefined name `Path`\n  --> canonical_flow/K_knowledge_extraction/causal_graph.py:24:20\n   |\n22 | try:\n23 |     # Add project root to path for imports\n24 |     project_root = Path(__file__).resolve().parents[1]\n   |                    ^^^^\n25 |     if str(project_root) not in sys.path:\n26 |         sys.path.insert(0, str(project_root))\n   |\n\nF821 Undefined name `importlib_util`\n  --> canonical_flow/K_knowledge_extraction/causal_graph.py:31:16\n   |\n29 |     original_file = project_root / \"causal_graph.py\"\n30 |     if original_file.exists():\n31 |         spec = importlib_util.spec_from_file_location(\n   |                ^^^^^^^^^^^^^^\n32 |             f\"original_{alias_code.lower()}\", \n33 |             str(original_file)\n   |\n\nF821 Undefined name `importlib_util`\n  --> canonical_flow/K_knowledge_extraction/causal_graph.py:37:31\n   |\n36 |         if spec and spec.loader:\n37 |             original_module = importlib_util.module_from_spec(spec)\n   |                               ^^^^^^^^^^^^^^\n38 |             spec.loader.exec_module(original_module)\n   |\n\nF821 Undefined name `e`\n  --> canonical_flow/K_knowledge_extraction/causal_graph.py:56:67\n   |\n54 |     def process(data=None, context=None):\n55 |         \"\"\"Placeholder process function for failed import.\"\"\"\n56 |         return {\"error\": f\"Module {alias_source} failed to load: {e}\"}\n   |                                                                   ^\n   |\n\nF401 [*] `json` imported but unused\n  --> canonical_flow/K_knowledge_extraction/causal_graph_constructor.py:14:8\n   |\n12 | \"\"\"\n13 |\n14 | import json\n   |        ^^^^\n15 | import logging\n16 | # # # from collections import defaultdict, OrderedDict  # Module not found  # Module not found  # Module not found\n   |\nhelp: Remove unused import: `json`\n\nF821 Undefined name `Path`\n  --> canonical_flow/K_knowledge_extraction/causal_graph_constructor.py:25:21\n   |\n23 | # # # # Import TotalOrderingBase from project root  # Module not found  # Module not found  # Module not found\n24 | import sys\n25 | sys.path.append(str(Path(__file__).resolve().parents[2]))\n   |                     ^^^^\n26 | # # # from total_ordering_base import TotalOrderingBase  # Module not found  # Module not found  # Module not found\n   |\n\nF821 Undefined name `Enum`\n  --> canonical_flow/K_knowledge_extraction/causal_graph_constructor.py:31:21\n   |\n31 | class DimensionType(Enum):\n   |                     ^^^^\n32 |     \"\"\"Enumeration of dimension types for causal graph construction.\"\"\"\n33 |     DE1 = \"DE-1\"\n   |\n\nF821 Undefined name `dataclass`\n  --> canonical_flow/K_knowledge_extraction/causal_graph_constructor.py:39:2\n   |\n39 | @dataclass(frozen=True)\n   |  ^^^^^^^^^\n40 | class EvidenceReference:\n41 |     \"\"\"Immutable reference to supporting evidence with page numbers.\"\"\"\n   |\n\nF821 Undefined name `List`\n  --> canonical_flow/K_knowledge_extraction/causal_graph_constructor.py:43:19\n   |\n41 |     \"\"\"Immutable reference to supporting evidence with page numbers.\"\"\"\n42 |     source_id: str\n43 |     page_numbers: List[int] = field(default_factory=list)\n   |                   ^^^^\n44 |     text_snippet: str = \"\"\n45 |     confidence: float = 0.0\n   |\n\nF821 Undefined name `field`\n  --> canonical_flow/K_knowledge_extraction/causal_graph_constructor.py:43:31\n   |\n41 |     \"\"\"Immutable reference to supporting evidence with page numbers.\"\"\"\n42 |     source_id: str\n43 |     page_numbers: List[int] = field(default_factory=list)\n   |                               ^^^^^\n44 |     text_snippet: str = \"\"\n45 |     confidence: float = 0.0\n   |\n\nF821 Undefined name `dataclass`\n  --> canonical_flow/K_knowledge_extraction/causal_graph_constructor.py:51:2\n   |\n51 | @dataclass(frozen=True)  \n   |  ^^^^^^^^^\n52 | class CausalRelationship:\n53 |     \"\"\"Immutable causal relationship with evidence anchoring.\"\"\"\n   |\n\nF821 Undefined name `Tuple`\n  --> canonical_flow/K_knowledge_extraction/causal_graph_constructor.py:59:26\n   |\n57 |     confidence: float\n58 |     evidence_strength: float\n59 |     evidence_references: Tuple[EvidenceReference, ...] = field(default_factory=tuple)\n   |                          ^^^^^\n60 |     validity_score: float = 0.0\n   |\n\nF821 Undefined name `field`\n  --> canonical_flow/K_knowledge_extraction/causal_graph_constructor.py:59:58\n   |\n57 |     confidence: float\n58 |     evidence_strength: float\n59 |     evidence_references: Tuple[EvidenceReference, ...] = field(default_factory=tuple)\n   |                                                          ^^^^^\n60 |     validity_score: float = 0.0\n   |\n\nF821 Undefined name `dataclass`\n  --> canonical_flow/K_knowledge_extraction/causal_graph_constructor.py:66:2\n   |\n66 | @dataclass\n   |  ^^^^^^^^^\n67 | class CausalGraphArtifact:\n68 |     \"\"\"Structured output artifact for causal graphs.\"\"\"\n   |\n\nF821 Undefined name `List`\n  --> canonical_flow/K_knowledge_extraction/causal_graph_constructor.py:70:12\n   |\n68 |     \"\"\"Structured output artifact for causal graphs.\"\"\"\n69 |     dimension: str\n70 |     nodes: List[Dict[str, Any]]\n   |            ^^^^\n71 |     edges: List[Dict[str, Any]]\n72 |     metadata: Dict[str, Any]\n   |\n\nF821 Undefined name `Dict`\n  --> canonical_flow/K_knowledge_extraction/causal_graph_constructor.py:70:17\n   |\n68 |     \"\"\"Structured output artifact for causal graphs.\"\"\"\n69 |     dimension: str\n70 |     nodes: List[Dict[str, Any]]\n   |                 ^^^^\n71 |     edges: List[Dict[str, Any]]\n72 |     metadata: Dict[str, Any]\n   |\n\nF821 Undefined name `Any`\n  --> canonical_flow/K_knowledge_extraction/causal_graph_constructor.py:70:27\n   |\n68 |     \"\"\"Structured output artifact for causal graphs.\"\"\"\n69 |     dimension: str\n70 |     nodes: List[Dict[str, Any]]\n   |                           ^^^\n71 |     edges: List[Dict[str, Any]]\n72 |     metadata: Dict[str, Any]\n   |\n\nF821 Undefined name `List`\n  --> canonical_flow/K_knowledge_extraction/causal_graph_constructor.py:71:12\n   |\n69 |     dimension: str\n70 |     nodes: List[Dict[str, Any]]\n71 |     edges: List[Dict[str, Any]]\n   |            ^^^^\n72 |     metadata: Dict[str, Any]\n73 |     validity_statistics: Dict[str, float]\n   |\n\nF821 Undefined name `Dict`\n  --> canonical_flow/K_knowledge_extraction/causal_graph_constructor.py:71:17\n   |\n69 |     dimension: str\n70 |     nodes: List[Dict[str, Any]]\n71 |     edges: List[Dict[str, Any]]\n   |                 ^^^^\n72 |     metadata: Dict[str, Any]\n73 |     validity_statistics: Dict[str, float]\n   |\n\nF821 Undefined name `Any`\n  --> canonical_flow/K_knowledge_extraction/causal_graph_constructor.py:71:27\n   |\n69 |     dimension: str\n70 |     nodes: List[Dict[str, Any]]\n71 |     edges: List[Dict[str, Any]]\n   |                           ^^^\n72 |     metadata: Dict[str, Any]\n73 |     validity_statistics: Dict[str, float]\n   |\n\nF821 Undefined name `Dict`\n  --> canonical_flow/K_knowledge_extraction/causal_graph_constructor.py:72:15\n   |\n70 |     nodes: List[Dict[str, Any]]\n71 |     edges: List[Dict[str, Any]]\n72 |     metadata: Dict[str, Any]\n   |               ^^^^\n73 |     validity_statistics: Dict[str, float]\n   |\n\nF821 Undefined name `Any`\n  --> canonical_flow/K_knowledge_extraction/causal_graph_constructor.py:72:25\n   |\n70 |     nodes: List[Dict[str, Any]]\n71 |     edges: List[Dict[str, Any]]\n72 |     metadata: Dict[str, Any]\n   |                         ^^^\n73 |     validity_statistics: Dict[str, float]\n   |\n\nF821 Undefined name `Dict`\n  --> canonical_flow/K_knowledge_extraction/causal_graph_constructor.py:73:26\n   |\n71 |     edges: List[Dict[str, Any]]\n72 |     metadata: Dict[str, Any]\n73 |     validity_statistics: Dict[str, float]\n   |                          ^^^^\n   |\n\nF821 Undefined name `TotalOrderingBase`\n  --> canonical_flow/K_knowledge_extraction/causal_graph_constructor.py:76:30\n   |\n76 | class CausalGraphConstructor(TotalOrderingBase):\n   |                              ^^^^^^^^^^^^^^^^^\n77 |     \"\"\"\n78 |     09K Causal Graph Constructor that builds dimension-specific directed acyclic graphs\n   |\n\nF821 Undefined name `Path`\n  --> canonical_flow/K_knowledge_extraction/causal_graph_constructor.py:84:33\n   |\n82 |     def __init__(self):\n83 |         super().__init__(component_name=\"CausalGraphConstructor_09K\")\n84 |         self.output_directory = Path(\"canonical_flow/knowledge\")\n   |                                 ^^^^\n85 |         self.output_directory.mkdir(parents=True, exist_ok=True)\n   |\n\nF821 Undefined name `Dict`\n  --> canonical_flow/K_knowledge_extraction/causal_graph_constructor.py:88:32\n   |\n87 |         # Initialize dimension graphs\n88 |         self.dimension_graphs: Dict[DimensionType, nx.DiGraph] = {\n   |                                ^^^^\n89 |             dim: nx.DiGraph() for dim in DimensionType\n90 |         }\n   |\n\nF821 Undefined name `Dict`\n  --> canonical_flow/K_knowledge_extraction/causal_graph_constructor.py:93:33\n   |\n92 |         # Evidence and relationship storage\n93 |         self.evidence_registry: Dict[str, Dict[str, Any]] = {}\n   |                                 ^^^^\n94 |         self.causal_relationships: Dict[DimensionType, List[CausalRelationship]] = {\n95 |             dim: [] for dim in DimensionType\n   |\n\nF821 Undefined name `Dict`\n  --> canonical_flow/K_knowledge_extraction/causal_graph_constructor.py:93:43\n   |\n92 |         # Evidence and relationship storage\n93 |         self.evidence_registry: Dict[str, Dict[str, Any]] = {}\n   |                                           ^^^^\n94 |         self.causal_relationships: Dict[DimensionType, List[CausalRelationship]] = {\n95 |             dim: [] for dim in DimensionType\n   |\n\nF821 Undefined name `Any`\n  --> canonical_flow/K_knowledge_extraction/causal_graph_constructor.py:93:53\n   |\n92 |         # Evidence and relationship storage\n93 |         self.evidence_registry: Dict[str, Dict[str, Any]] = {}\n   |                                                     ^^^\n94 |         self.causal_relationships: Dict[DimensionType, List[CausalRelationship]] = {\n95 |             dim: [] for dim in DimensionType\n   |\n\nF821 Undefined name `Dict`\n  --> canonical_flow/K_knowledge_extraction/causal_graph_constructor.py:94:36\n   |\n92 |         # Evidence and relationship storage\n93 |         self.evidence_registry: Dict[str, Dict[str, Any]] = {}\n94 |         self.causal_relationships: Dict[DimensionType, List[CausalRelationship]] = {\n   |                                    ^^^^\n95 |             dim: [] for dim in DimensionType\n96 |         }\n   |\n\nF821 Undefined name `List`\n  --> canonical_flow/K_knowledge_extraction/causal_graph_constructor.py:94:56\n   |\n92 |         # Evidence and relationship storage\n93 |         self.evidence_registry: Dict[str, Dict[str, Any]] = {}\n94 |         self.causal_relationships: Dict[DimensionType, List[CausalRelationship]] = {\n   |                                                        ^^^^\n95 |             dim: [] for dim in DimensionType\n96 |         }\n   |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/K_knowledge_extraction/causal_graph_constructor.py:110:43\n    |\n108 |         logger.info(f\"Initialized {self.component_name} with output directory: {self.output_directory}\")\n109 |     \n110 |     def process(self, evidence_artifacts: Dict[str, Any], knowledge_graph_artifacts: Dict[str, Any], \n    |                                           ^^^^\n111 |                context: Optional[Dict[str, Any]] = None) -> Dict[str, CausalGraphArtifact]:\n112 |         \"\"\"\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/K_knowledge_extraction/causal_graph_constructor.py:110:53\n    |\n108 |         logger.info(f\"Initialized {self.component_name} with output directory: {self.output_directory}\")\n109 |     \n110 |     def process(self, evidence_artifacts: Dict[str, Any], knowledge_graph_artifacts: Dict[str, Any], \n    |                                                     ^^^\n111 |                context: Optional[Dict[str, Any]] = None) -> Dict[str, CausalGraphArtifact]:\n112 |         \"\"\"\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/K_knowledge_extraction/causal_graph_constructor.py:110:86\n    |\n108 |         logger.info(f\"Initialized {self.component_name} with output directory: {self.output_directory}\")\n109 |     \n110 |     def process(self, evidence_artifacts: Dict[str, Any], knowledge_graph_artifacts: Dict[str, Any], \n    |                                                                                      ^^^^\n111 |                context: Optional[Dict[str, Any]] = None) -> Dict[str, CausalGraphArtifact]:\n112 |         \"\"\"\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/K_knowledge_extraction/causal_graph_constructor.py:110:96\n    |\n108 |         logger.info(f\"Initialized {self.component_name} with output directory: {self.output_directory}\")\n109 |     \n110 |     def process(self, evidence_artifacts: Dict[str, Any], knowledge_graph_artifacts: Dict[str, Any], \n    |                                                                                                ^^^\n111 |                context: Optional[Dict[str, Any]] = None) -> Dict[str, CausalGraphArtifact]:\n112 |         \"\"\"\n    |\n\nF821 Undefined name `Optional`\n   --> canonical_flow/K_knowledge_extraction/causal_graph_constructor.py:111:25\n    |\n110 |     def process(self, evidence_artifacts: Dict[str, Any], knowledge_graph_artifacts: Dict[str, Any], \n111 |                context: Optional[Dict[str, Any]] = None) -> Dict[str, CausalGraphArtifact]:\n    |                         ^^^^^^^^\n112 |         \"\"\"\n113 |         Standardized process() API implementation.\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/K_knowledge_extraction/causal_graph_constructor.py:111:34\n    |\n110 |     def process(self, evidence_artifacts: Dict[str, Any], knowledge_graph_artifacts: Dict[str, Any], \n111 |                context: Optional[Dict[str, Any]] = None) -> Dict[str, CausalGraphArtifact]:\n    |                                  ^^^^\n112 |         \"\"\"\n113 |         Standardized process() API implementation.\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/K_knowledge_extraction/causal_graph_constructor.py:111:44\n    |\n110 |     def process(self, evidence_artifacts: Dict[str, Any], knowledge_graph_artifacts: Dict[str, Any], \n111 |                context: Optional[Dict[str, Any]] = None) -> Dict[str, CausalGraphArtifact]:\n    |                                            ^^^\n112 |         \"\"\"\n113 |         Standardized process() API implementation.\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/K_knowledge_extraction/causal_graph_constructor.py:111:61\n    |\n110 |     def process(self, evidence_artifacts: Dict[str, Any], knowledge_graph_artifacts: Dict[str, Any], \n111 |                context: Optional[Dict[str, Any]] = None) -> Dict[str, CausalGraphArtifact]:\n    |                                                             ^^^^\n112 |         \"\"\"\n113 |         Standardized process() API implementation.\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/K_knowledge_extraction/causal_graph_constructor.py:156:60\n    |\n154 |             return self._generate_sparse_artifacts()\n155 |     \n156 |     def _load_evidence_artifacts(self, evidence_artifacts: Dict[str, Any]) -> None:\n    |                                                            ^^^^\n157 | # # #         \"\"\"Load and register evidence from ingestion artifacts.\"\"\"  # Module not found  # Module not found  # Module not found\n158 |         if not evidence_artifacts:\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/K_knowledge_extraction/causal_graph_constructor.py:156:70\n    |\n154 |             return self._generate_sparse_artifacts()\n155 |     \n156 |     def _load_evidence_artifacts(self, evidence_artifacts: Dict[str, Any]) -> None:\n    |                                                                      ^^^\n157 | # # #         \"\"\"Load and register evidence from ingestion artifacts.\"\"\"  # Module not found  # Module not found  # Module not found\n158 |         if not evidence_artifacts:\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/K_knowledge_extraction/causal_graph_constructor.py:187:72\n    |\n185 |         logger.info(f\"Loaded {evidence_count} evidence artifacts\")\n186 |     \n187 |     def _extract_causal_relationships(self, knowledge_graph_artifacts: Dict[str, Any]) -> None:\n    |                                                                        ^^^^\n188 | # # #         \"\"\"Extract causal relationships from knowledge graph artifacts.\"\"\"  # Module not found  # Module not found  # Module no\u2026\n189 |         if not knowledge_graph_artifacts:\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/K_knowledge_extraction/causal_graph_constructor.py:187:82\n    |\n185 |         logger.info(f\"Loaded {evidence_count} evidence artifacts\")\n186 |     \n187 |     def _extract_causal_relationships(self, knowledge_graph_artifacts: Dict[str, Any]) -> None:\n    |                                                                                  ^^^\n188 | # # #         \"\"\"Extract causal relationships from knowledge graph artifacts.\"\"\"  # Module not found  # Module not found  # Module no\u2026\n189 |         if not knowledge_graph_artifacts:\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/K_knowledge_extraction/causal_graph_constructor.py:217:57\n    |\n215 |         logger.info(f\"Extracted {relationship_count} causal relationships\")\n216 |     \n217 |     def _extract_from_graph_structure(self, graph_data: Dict[str, Any]) -> List[CausalRelationship]:\n    |                                                         ^^^^\n218 | # # #         \"\"\"Extract causal relationships from graph structure data.\"\"\"  # Module not found  # Module not found  # Module not fou\u2026\n219 |         relationships = []\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/K_knowledge_extraction/causal_graph_constructor.py:217:67\n    |\n215 |         logger.info(f\"Extracted {relationship_count} causal relationships\")\n216 |     \n217 |     def _extract_from_graph_structure(self, graph_data: Dict[str, Any]) -> List[CausalRelationship]:\n    |                                                                   ^^^\n218 | # # #         \"\"\"Extract causal relationships from graph structure data.\"\"\"  # Module not found  # Module not found  # Module not fou\u2026\n219 |         relationships = []\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/K_knowledge_extraction/causal_graph_constructor.py:217:76\n    |\n215 |         logger.info(f\"Extracted {relationship_count} causal relationships\")\n216 |     \n217 |     def _extract_from_graph_structure(self, graph_data: Dict[str, Any]) -> List[CausalRelationship]:\n    |                                                                            ^^^^\n218 | # # #         \"\"\"Extract causal relationships from graph structure data.\"\"\"  # Module not found  # Module not found  # Module not fou\u2026\n219 |         relationships = []\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/K_knowledge_extraction/causal_graph_constructor.py:232:60\n    |\n230 |         return relationships\n231 |     \n232 |     def _extract_from_entity_relationships(self, rel_data: Dict[str, Any]) -> List[CausalRelationship]:\n    |                                                            ^^^^\n233 | # # #         \"\"\"Extract causal relationships from entity relationship data.\"\"\"  # Module not found  # Module not found  # Module not\u2026\n234 |         relationships = []\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/K_knowledge_extraction/causal_graph_constructor.py:232:70\n    |\n230 |         return relationships\n231 |     \n232 |     def _extract_from_entity_relationships(self, rel_data: Dict[str, Any]) -> List[CausalRelationship]:\n    |                                                                      ^^^\n233 | # # #         \"\"\"Extract causal relationships from entity relationship data.\"\"\"  # Module not found  # Module not found  # Module not\u2026\n234 |         relationships = []\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/K_knowledge_extraction/causal_graph_constructor.py:232:79\n    |\n230 |         return relationships\n231 |     \n232 |     def _extract_from_entity_relationships(self, rel_data: Dict[str, Any]) -> List[CausalRelationship]:\n    |                                                                               ^^^^\n233 | # # #         \"\"\"Extract causal relationships from entity relationship data.\"\"\"  # Module not found  # Module not found  # Module not\u2026\n234 |         relationships = []\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/K_knowledge_extraction/causal_graph_constructor.py:256:37\n    |\n254 |         return relationships\n255 |     \n256 |     def _is_causal_edge(self, edge: Dict[str, Any]) -> bool:\n    |                                     ^^^^\n257 |         \"\"\"Determine if an edge represents a causal relationship.\"\"\"\n258 |         causal_indicators = [\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/K_knowledge_extraction/causal_graph_constructor.py:256:47\n    |\n254 |         return relationships\n255 |     \n256 |     def _is_causal_edge(self, edge: Dict[str, Any]) -> bool:\n    |                                               ^^^\n257 |         \"\"\"Determine if an edge represents a causal relationship.\"\"\"\n258 |         causal_indicators = [\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/K_knowledge_extraction/causal_graph_constructor.py:269:59\n    |\n267 |                   for indicator in causal_indicators)\n268 |     \n269 |     def _create_causal_relationship_from_edge(self, edge: Dict[str, Any]) -> Optional[CausalRelationship]:\n    |                                                           ^^^^\n270 | # # #         \"\"\"Create a causal relationship from an edge structure.\"\"\"  # Module not found  # Module not found  # Module not found\n271 |         try:\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/K_knowledge_extraction/causal_graph_constructor.py:269:69\n    |\n267 |                   for indicator in causal_indicators)\n268 |     \n269 |     def _create_causal_relationship_from_edge(self, edge: Dict[str, Any]) -> Optional[CausalRelationship]:\n    |                                                                     ^^^\n270 | # # #         \"\"\"Create a causal relationship from an edge structure.\"\"\"  # Module not found  # Module not found  # Module not found\n271 |         try:\n    |\n\nF821 Undefined name `Optional`\n   --> canonical_flow/K_knowledge_extraction/causal_graph_constructor.py:269:78\n    |\n267 |                   for indicator in causal_indicators)\n268 |     \n269 |     def _create_causal_relationship_from_edge(self, edge: Dict[str, Any]) -> Optional[CausalRelationship]:\n    |                                                                              ^^^^^^^^\n270 | # # #         \"\"\"Create a causal relationship from an edge structure.\"\"\"  # Module not found  # Module not found  # Module not found\n271 |         try:\n    |\n\nF841 [*] Local variable `e` is assigned to but never used\n   --> canonical_flow/K_knowledge_extraction/causal_graph_constructor.py:283:53\n    |\n281 |                 validity_score=0.0\n282 |             )\n283 |         except (KeyError, ValueError, TypeError) as e:\n    |                                                     ^\n284 | # # #             logger.warning(f\"Failed to create causal relationship from edge: {e}\")  # Module not found  # Module not found  # M\u2026\n285 |             return None\n    |\nhelp: Remove assignment to unused variable `e`\n\nF821 Undefined name `List`\n   --> canonical_flow/K_knowledge_extraction/causal_graph_constructor.py:287:58\n    |\n285 |             return None\n286 |     \n287 |     def _create_evidence_references(self, evidence_list: List[Dict[str, Any]]) -> List[EvidenceReference]:\n    |                                                          ^^^^\n288 | # # #         \"\"\"Create evidence references from evidence data.\"\"\"  # Module not found  # Module not found  # Module not found\n289 |         references = []\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/K_knowledge_extraction/causal_graph_constructor.py:287:63\n    |\n285 |             return None\n286 |     \n287 |     def _create_evidence_references(self, evidence_list: List[Dict[str, Any]]) -> List[EvidenceReference]:\n    |                                                               ^^^^\n288 | # # #         \"\"\"Create evidence references from evidence data.\"\"\"  # Module not found  # Module not found  # Module not found\n289 |         references = []\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/K_knowledge_extraction/causal_graph_constructor.py:287:73\n    |\n285 |             return None\n286 |     \n287 |     def _create_evidence_references(self, evidence_list: List[Dict[str, Any]]) -> List[EvidenceReference]:\n    |                                                                         ^^^\n288 | # # #         \"\"\"Create evidence references from evidence data.\"\"\"  # Module not found  # Module not found  # Module not found\n289 |         references = []\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/K_knowledge_extraction/causal_graph_constructor.py:287:83\n    |\n285 |             return None\n286 |     \n287 |     def _create_evidence_references(self, evidence_list: List[Dict[str, Any]]) -> List[EvidenceReference]:\n    |                                                                                   ^^^^\n288 | # # #         \"\"\"Create evidence references from evidence data.\"\"\"  # Module not found  # Module not found  # Module not found\n289 |         references = []\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/K_knowledge_extraction/causal_graph_constructor.py:306:59\n    |\n304 |         return references\n305 |     \n306 |     def _calculate_evidence_strength(self, evidence_refs: List[EvidenceReference]) -> float:\n    |                                                           ^^^^\n307 | # # #         \"\"\"Calculate aggregate evidence strength from references.\"\"\"  # Module not found  # Module not found  # Module not found\n308 |         if not evidence_refs:\n    |\n\nF821 Undefined name `Optional`\n   --> canonical_flow/K_knowledge_extraction/causal_graph_constructor.py:321:85\n    |\n319 |         return min(1.0, avg_confidence + diversity_bonus)\n320 |     \n321 |     def _classify_relationship_dimension(self, relationship: CausalRelationship) -> Optional[DimensionType]:\n    |                                                                                     ^^^^^^^^\n322 |         \"\"\"Classify causal relationship into appropriate dimension.\"\"\"\n323 |         # Simple heuristic classification - can be enhanced with ML models\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/K_knowledge_extraction/causal_graph_constructor.py:442:45\n    |\n441 |     def _create_graph_artifact(self, dimension: DimensionType, graph: nx.DiGraph, \n442 |                              relationships: List[CausalRelationship]) -> CausalGraphArtifact:\n    |                                             ^^^^\n443 | # # #         \"\"\"Create structured artifact from dimension graph.\"\"\"  # Module not found  # Module not found  # Module not found\n444 |         # Generate nodes with deterministic ordering\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/K_knowledge_extraction/causal_graph_constructor.py:540:45\n    |\n538 |         )\n539 |     \n540 |     def _generate_sparse_artifacts(self) -> Dict[str, CausalGraphArtifact]:\n    |                                             ^^^^\n541 |         \"\"\"Generate sparse artifacts for all dimensions when processing fails.\"\"\"\n542 |         artifacts = {}\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/K_knowledge_extraction/causal_graph_constructor.py:547:49\n    |\n545 |         return artifacts\n546 |     \n547 |     def _generate_output_files(self, artifacts: Dict[str, CausalGraphArtifact]) -> None:\n    |                                                 ^^^^\n548 |         \"\"\"Generate output JSON files for each dimension.\"\"\"\n549 |         for dimension_key, artifact in artifacts.items():\n    |\n\nF401 [*] `json` imported but unused\n  --> canonical_flow/K_knowledge_extraction/chunking_processor.py:13:8\n   |\n12 | import hashlib\n13 | import json\n   |        ^^^^\n14 | import logging\n15 | import re\n   |\nhelp: Remove unused import: `json`\n\nF821 Undefined name `Path`\n  --> canonical_flow/K_knowledge_extraction/chunking_processor.py:24:24\n   |\n23 | # Import total ordering base\n24 | sys.path.insert(0, str(Path(__file__).resolve().parents[2]))\n   |                        ^^^^\n25 | # # # from total_ordering_base import TotalOrderingBase, DeterministicCollectionMixin  # Module not found  # Module not found  # Modul\u2026\n   |\n\nF821 Undefined name `Enum`\n  --> canonical_flow/K_knowledge_extraction/chunking_processor.py:30:26\n   |\n30 | class PartitioningPolicy(Enum):\n   |                          ^^^^\n31 |     \"\"\"Chunking strategies for text partitioning.\"\"\"\n32 |     SENTENCE_BOUNDARIES = \"sentence_boundaries\"\n   |\n\nF821 Undefined name `dataclass`\n  --> canonical_flow/K_knowledge_extraction/chunking_processor.py:38:2\n   |\n38 | @dataclass\n   |  ^^^^^^^^^\n39 | class ChunkConfig:\n40 |     \"\"\"Configuration for chunking behavior.\"\"\"\n   |\n\nF821 Undefined name `dataclass`\n  --> canonical_flow/K_knowledge_extraction/chunking_processor.py:61:2\n   |\n61 | @dataclass\n   |  ^^^^^^^^^\n62 | class PageAnchor:\n63 |     \"\"\"Page anchor information for provenance tracking.\"\"\"\n   |\n\nF821 Undefined name `Optional`\n  --> canonical_flow/K_knowledge_extraction/chunking_processor.py:67:21\n   |\n65 |     start_char: int\n66 |     end_char: int\n67 |     page_text_hash: Optional[str] = None\n   |                     ^^^^^^^^\n68 |     \n69 |     def __post_init__(self):\n   |\n\nF821 Undefined name `dataclass`\n  --> canonical_flow/K_knowledge_extraction/chunking_processor.py:76:2\n   |\n76 | @dataclass\n   |  ^^^^^^^^^\n77 | class ChunkProvenance:\n78 |     \"\"\"Provenance metadata for chunks.\"\"\"\n   |\n\nF821 Undefined name `List`\n  --> canonical_flow/K_knowledge_extraction/chunking_processor.py:81:19\n   |\n79 |     source_document: str\n80 |     document_stem: str\n81 |     page_anchors: List[PageAnchor] = field(default_factory=list)\n   |                   ^^^^\n82 |     character_offsets: Tuple[int, int] = (0, 0)\n83 |     section_path: List[str] = field(default_factory=list)\n   |\n\nF821 Undefined name `field`\n  --> canonical_flow/K_knowledge_extraction/chunking_processor.py:81:38\n   |\n79 |     source_document: str\n80 |     document_stem: str\n81 |     page_anchors: List[PageAnchor] = field(default_factory=list)\n   |                                      ^^^^^\n82 |     character_offsets: Tuple[int, int] = (0, 0)\n83 |     section_path: List[str] = field(default_factory=list)\n   |\n\nF821 Undefined name `Tuple`\n  --> canonical_flow/K_knowledge_extraction/chunking_processor.py:82:24\n   |\n80 |     document_stem: str\n81 |     page_anchors: List[PageAnchor] = field(default_factory=list)\n82 |     character_offsets: Tuple[int, int] = (0, 0)\n   |                        ^^^^^\n83 |     section_path: List[str] = field(default_factory=list)\n84 |     processing_timestamp: str = \"\"\n   |\n\nF821 Undefined name `List`\n  --> canonical_flow/K_knowledge_extraction/chunking_processor.py:83:19\n   |\n81 |     page_anchors: List[PageAnchor] = field(default_factory=list)\n82 |     character_offsets: Tuple[int, int] = (0, 0)\n83 |     section_path: List[str] = field(default_factory=list)\n   |                   ^^^^\n84 |     processing_timestamp: str = \"\"\n85 |     source_hash: str = \"\"\n   |\n\nF821 Undefined name `field`\n  --> canonical_flow/K_knowledge_extraction/chunking_processor.py:83:31\n   |\n81 |     page_anchors: List[PageAnchor] = field(default_factory=list)\n82 |     character_offsets: Tuple[int, int] = (0, 0)\n83 |     section_path: List[str] = field(default_factory=list)\n   |                               ^^^^^\n84 |     processing_timestamp: str = \"\"\n85 |     source_hash: str = \"\"\n   |\n\nF821 Undefined name `dataclass`\n  --> canonical_flow/K_knowledge_extraction/chunking_processor.py:93:2\n   |\n93 | @dataclass\n   |  ^^^^^^^^^\n94 | class ProcessedChunk:\n95 |     \"\"\"A processed text chunk with full metadata.\"\"\"\n   |\n\nF821 Undefined name `List`\n   --> canonical_flow/K_knowledge_extraction/chunking_processor.py:98:19\n    |\n 96 |     chunk_id: str\n 97 |     content: str\n 98 |     page_anchors: List[PageAnchor]\n    |                   ^^^^\n 99 |     metadata: Dict[str, Any]\n100 |     provenance: ChunkProvenance\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/K_knowledge_extraction/chunking_processor.py:99:15\n    |\n 97 |     content: str\n 98 |     page_anchors: List[PageAnchor]\n 99 |     metadata: Dict[str, Any]\n    |               ^^^^\n100 |     provenance: ChunkProvenance\n101 |     token_count: int = 0\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/K_knowledge_extraction/chunking_processor.py:99:25\n    |\n 97 |     content: str\n 98 |     page_anchors: List[PageAnchor]\n 99 |     metadata: Dict[str, Any]\n    |                         ^^^\n100 |     provenance: ChunkProvenance\n101 |     token_count: int = 0\n    |\n\nF821 Undefined name `TotalOrderingBase`\n   --> canonical_flow/K_knowledge_extraction/chunking_processor.py:132:25\n    |\n132 | class ChunkingProcessor(TotalOrderingBase, DeterministicCollectionMixin):\n    |                         ^^^^^^^^^^^^^^^^^\n133 |     \"\"\"\n134 |     Deterministic text chunking processor with standardized process() API.\n    |\n\nF821 Undefined name `DeterministicCollectionMixin`\n   --> canonical_flow/K_knowledge_extraction/chunking_processor.py:132:44\n    |\n132 | class ChunkingProcessor(TotalOrderingBase, DeterministicCollectionMixin):\n    |                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n133 |     \"\"\"\n134 |     Deterministic text chunking processor with standardized process() API.\n    |\n\nF821 Undefined name `Optional`\n   --> canonical_flow/K_knowledge_extraction/chunking_processor.py:140:32\n    |\n138 |     \"\"\"\n139 |     \n140 |     def __init__(self, config: Optional[ChunkConfig] = None):\n    |                                ^^^^^^^^\n141 |         \"\"\"\n142 |         Initialize chunking processor with configuration.\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/K_knowledge_extraction/chunking_processor.py:166:37\n    |\n164 |         self.update_state_hash(self._get_initial_state())\n165 |     \n166 |     def _get_initial_state(self) -> Dict[str, Any]:\n    |                                     ^^^^\n167 |         \"\"\"Get initial state for hash calculation.\"\"\"\n168 |         return {\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/K_knowledge_extraction/chunking_processor.py:166:47\n    |\n164 |         self.update_state_hash(self._get_initial_state())\n165 |     \n166 |     def _get_initial_state(self) -> Dict[str, Any]:\n    |                                               ^^^\n167 |         \"\"\"Get initial state for hash calculation.\"\"\"\n168 |         return {\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/K_knowledge_extraction/chunking_processor.py:180:29\n    |\n178 |         }\n179 |     \n180 |     def process(self, data: Any = None, context: Any = None) -> Dict[str, Any]:\n    |                             ^^^\n181 |         \"\"\"\n182 |         Main processing function implementing standardized process() API.\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/K_knowledge_extraction/chunking_processor.py:180:50\n    |\n178 |         }\n179 |     \n180 |     def process(self, data: Any = None, context: Any = None) -> Dict[str, Any]:\n    |                                                  ^^^\n181 |         \"\"\"\n182 |         Main processing function implementing standardized process() API.\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/K_knowledge_extraction/chunking_processor.py:180:65\n    |\n178 |         }\n179 |     \n180 |     def process(self, data: Any = None, context: Any = None) -> Dict[str, Any]:\n    |                                                                 ^^^^\n181 |         \"\"\"\n182 |         Main processing function implementing standardized process() API.\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/K_knowledge_extraction/chunking_processor.py:180:75\n    |\n178 |         }\n179 |     \n180 |     def process(self, data: Any = None, context: Any = None) -> Dict[str, Any]:\n    |                                                                           ^^^\n181 |         \"\"\"\n182 |         Main processing function implementing standardized process() API.\n    |\n\nF841 Local variable `canonical_context` is assigned to but never used\n   --> canonical_flow/K_knowledge_extraction/chunking_processor.py:196:13\n    |\n194 |             # Canonicalize inputs for deterministic processing\n195 |             canonical_data = self.canonicalize_data(data) if data else {}\n196 |             canonical_context = self.canonicalize_data(context) if context else {}\n    |             ^^^^^^^^^^^^^^^^^\n197 |             \n198 |             # Extract document information\n    |\nhelp: Remove assignment to unused variable `canonical_context`\n\nF821 Undefined name `Dict`\n   --> canonical_flow/K_knowledge_extraction/chunking_processor.py:231:44\n    |\n229 |             return self._generate_error_result(str(e), operation_id)\n230 |     \n231 |     def _extract_document_info(self, data: Dict[str, Any]) -> Optional[Dict[str, Any]]:\n    |                                            ^^^^\n232 |         \"\"\"\n233 | # # #         Extract document information from input data.  # Module not found  # Module not found  # Module not found\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/K_knowledge_extraction/chunking_processor.py:231:54\n    |\n229 |             return self._generate_error_result(str(e), operation_id)\n230 |     \n231 |     def _extract_document_info(self, data: Dict[str, Any]) -> Optional[Dict[str, Any]]:\n    |                                                      ^^^\n232 |         \"\"\"\n233 | # # #         Extract document information from input data.  # Module not found  # Module not found  # Module not found\n    |\n\nF821 Undefined name `Optional`\n   --> canonical_flow/K_knowledge_extraction/chunking_processor.py:231:63\n    |\n229 |             return self._generate_error_result(str(e), operation_id)\n230 |     \n231 |     def _extract_document_info(self, data: Dict[str, Any]) -> Optional[Dict[str, Any]]:\n    |                                                               ^^^^^^^^\n232 |         \"\"\"\n233 | # # #         Extract document information from input data.  # Module not found  # Module not found  # Module not found\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/K_knowledge_extraction/chunking_processor.py:231:72\n    |\n229 |             return self._generate_error_result(str(e), operation_id)\n230 |     \n231 |     def _extract_document_info(self, data: Dict[str, Any]) -> Optional[Dict[str, Any]]:\n    |                                                                        ^^^^\n232 |         \"\"\"\n233 | # # #         Extract document information from input data.  # Module not found  # Module not found  # Module not found\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/K_knowledge_extraction/chunking_processor.py:231:82\n    |\n229 |             return self._generate_error_result(str(e), operation_id)\n230 |     \n231 |     def _extract_document_info(self, data: Dict[str, Any]) -> Optional[Dict[str, Any]]:\n    |                                                                                  ^^^\n232 |         \"\"\"\n233 | # # #         Extract document information from input data.  # Module not found  # Module not found  # Module not found\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/K_knowledge_extraction/chunking_processor.py:280:55\n    |\n278 |         return None\n279 |     \n280 |     def _extract_text_content(self, document_content: Any) -> str:\n    |                                                       ^^^\n281 |         \"\"\"\n282 | # # #         Extract text content from document content structure.  # Module not found  # Module not found  # Module not found\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/K_knowledge_extraction/chunking_processor.py:319:61\n    |\n317 |         return str(document_content)\n318 |     \n319 |     def _generate_chunks_deterministic(self, document_info: Dict[str, Any]) -> List[ProcessedChunk]:\n    |                                                             ^^^^\n320 |         \"\"\"\n321 |         Generate chunks using deterministic splitting logic.\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/K_knowledge_extraction/chunking_processor.py:319:71\n    |\n317 |         return str(document_content)\n318 |     \n319 |     def _generate_chunks_deterministic(self, document_info: Dict[str, Any]) -> List[ProcessedChunk]:\n    |                                                                       ^^^\n320 |         \"\"\"\n321 |         Generate chunks using deterministic splitting logic.\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/K_knowledge_extraction/chunking_processor.py:319:80\n    |\n317 |         return str(document_content)\n318 |     \n319 |     def _generate_chunks_deterministic(self, document_info: Dict[str, Any]) -> List[ProcessedChunk]:\n    |                                                                                ^^^^\n320 |         \"\"\"\n321 |         Generate chunks using deterministic splitting logic.\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/K_knowledge_extraction/chunking_processor.py:388:54\n    |\n386 |         return processed_chunks\n387 |     \n388 |     def _split_text_by_policy(self, content: str) -> List[str]:\n    |                                                      ^^^^\n389 |         \"\"\"\n390 |         Split text according to the configured partitioning policy.\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/K_knowledge_extraction/chunking_processor.py:410:52\n    |\n408 |             return self._split_by_sentences(content)\n409 |     \n410 |     def _split_by_sentences(self, content: str) -> List[str]:\n    |                                                    ^^^^\n411 |         \"\"\"Split text by sentence boundaries.\"\"\"\n412 |         sentences = self._sentence_pattern.split(content.strip())\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/K_knowledge_extraction/chunking_processor.py:438:53\n    |\n436 |         return self._apply_overlap(chunks)\n437 |     \n438 |     def _split_by_paragraphs(self, content: str) -> List[str]:\n    |                                                     ^^^^\n439 |         \"\"\"Split text by paragraph breaks.\"\"\"\n440 |         paragraphs = self._paragraph_pattern.split(content.strip())\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/K_knowledge_extraction/chunking_processor.py:466:49\n    |\n464 |         return self._apply_overlap(chunks)\n465 |     \n466 |     def _split_by_tokens(self, content: str) -> List[str]:\n    |                                                 ^^^^\n467 |         \"\"\"Split text by fixed token counts.\"\"\"\n468 |         tokens = self._token_pattern.findall(content)\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/K_knowledge_extraction/chunking_processor.py:487:46\n    |\n485 |         return self._apply_overlap(chunks)\n486 |     \n487 |     def _split_hybrid(self, content: str) -> List[str]:\n    |                                              ^^^^\n488 |         \"\"\"Split using hybrid approach combining multiple policies.\"\"\"\n489 |         # First split by paragraphs\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/K_knowledge_extraction/chunking_processor.py:506:38\n    |\n504 |         return refined_chunks\n505 |     \n506 |     def _apply_overlap(self, chunks: List[str]) -> List[str]:\n    |                                      ^^^^\n507 |         \"\"\"Apply overlap between chunks if configured.\"\"\"\n508 |         if self.config.overlap_tokens == 0 and self.config.overlap_ratio == 0:\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/K_knowledge_extraction/chunking_processor.py:506:52\n    |\n504 |         return refined_chunks\n505 |     \n506 |     def _apply_overlap(self, chunks: List[str]) -> List[str]:\n    |                                                    ^^^^\n507 |         \"\"\"Apply overlap between chunks if configured.\"\"\"\n508 |         if self.config.overlap_tokens == 0 and self.config.overlap_ratio == 0:\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/K_knowledge_extraction/chunking_processor.py:536:92\n    |\n534 |         return overlapped_chunks\n535 |     \n536 |     def _generate_page_anchors(self, chunk_text: str, full_content: str, position: int) -> List[PageAnchor]:\n    |                                                                                            ^^^^\n537 |         \"\"\"\n538 |         Generate page anchors for chunk (simplified implementation).\n    |\n\nF841 Local variable `end_pos` is assigned to but never used\n   --> canonical_flow/K_knowledge_extraction/chunking_processor.py:555:9\n    |\n553 |             start_pos = position * chars_per_chunk\n554 |         \n555 |         end_pos = start_pos + len(chunk_text)\n    |         ^^^^^^^\n556 |         \n557 |         # Estimate page number (assuming ~2000 chars per page)\n    |\nhelp: Remove assignment to unused variable `end_pos`\n\nF821 Undefined name `Tuple`\n   --> canonical_flow/K_knowledge_extraction/chunking_processor.py:574:83\n    |\n572 |         return [anchor]\n573 |     \n574 |     def _calculate_character_offsets(self, chunk_text: str, full_content: str) -> Tuple[int, int]:\n    |                                                                                   ^^^^^\n575 |         \"\"\"Calculate character offsets for the chunk in the full content.\"\"\"\n576 |         start_pos = full_content.find(chunk_text)\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/K_knowledge_extraction/chunking_processor.py:582:50\n    |\n580 |         return (start_pos, start_pos + len(chunk_text))\n581 |     \n582 |     def _generate_output_artifacts(self, chunks: List[ProcessedChunk], document_info: Dict[str, Any]) -> Dict[str, Any]:\n    |                                                  ^^^^\n583 |         \"\"\"\n584 |         Generate canonical output artifacts.\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/K_knowledge_extraction/chunking_processor.py:582:87\n    |\n580 |         return (start_pos, start_pos + len(chunk_text))\n581 |     \n582 |     def _generate_output_artifacts(self, chunks: List[ProcessedChunk], document_info: Dict[str, Any]) -> Dict[str, Any]:\n    |                                                                                       ^^^^\n583 |         \"\"\"\n584 |         Generate canonical output artifacts.\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/K_knowledge_extraction/chunking_processor.py:582:97\n    |\n580 |         return (start_pos, start_pos + len(chunk_text))\n581 |     \n582 |     def _generate_output_artifacts(self, chunks: List[ProcessedChunk], document_info: Dict[str, Any]) -> Dict[str, Any]:\n    |                                                                                                 ^^^\n583 |         \"\"\"\n584 |         Generate canonical output artifacts.\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/K_knowledge_extraction/chunking_processor.py:582:106\n    |\n580 |         return (start_pos, start_pos + len(chunk_text))\n581 |     \n582 |     def _generate_output_artifacts(self, chunks: List[ProcessedChunk], document_info: Dict[str, Any]) -> Dict[str, Any]:\n    |                                                                                                          ^^^^\n583 |         \"\"\"\n584 |         Generate canonical output artifacts.\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/K_knowledge_extraction/chunking_processor.py:582:116\n    |\n580 |         return (start_pos, start_pos + len(chunk_text))\n581 |     \n582 |     def _generate_output_artifacts(self, chunks: List[ProcessedChunk], document_info: Dict[str, Any]) -> Dict[str, Any]:\n    |                                                                                                                    ^^^\n583 |         \"\"\"\n584 |         Generate canonical output artifacts.\n    |\n\nF821 Undefined name `Path`\n   --> canonical_flow/K_knowledge_extraction/chunking_processor.py:645:13\n    |\n643 |         try:\n644 |             # Ensure directory exists\n645 |             Path(output_path).parent.mkdir(parents=True, exist_ok=True)\n    |             ^^^^\n646 |             \n647 |             # Write with canonical JSON formatting\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/K_knowledge_extraction/chunking_processor.py:670:48\n    |\n668 |             }\n669 |     \n670 |     def _update_processing_stats(self, chunks: List[ProcessedChunk]) -> Dict[str, Any]:\n    |                                                ^^^^\n671 |         \"\"\"Update and return processing statistics.\"\"\"\n672 |         self.processing_stats[\"documents_processed\"] += 1\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/K_knowledge_extraction/chunking_processor.py:670:73\n    |\n668 |             }\n669 |     \n670 |     def _update_processing_stats(self, chunks: List[ProcessedChunk]) -> Dict[str, Any]:\n    |                                                                         ^^^^\n671 |         \"\"\"Update and return processing statistics.\"\"\"\n672 |         self.processing_stats[\"documents_processed\"] += 1\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/K_knowledge_extraction/chunking_processor.py:670:83\n    |\n668 |             }\n669 |     \n670 |     def _update_processing_stats(self, chunks: List[ProcessedChunk]) -> Dict[str, Any]:\n    |                                                                                   ^^^\n671 |         \"\"\"Update and return processing statistics.\"\"\"\n672 |         self.processing_stats[\"documents_processed\"] += 1\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/K_knowledge_extraction/chunking_processor.py:686:80\n    |\n684 |         return dict(self.processing_stats)\n685 |     \n686 |     def _generate_error_result(self, error_message: str, operation_id: str) -> Dict[str, Any]:\n    |                                                                                ^^^^\n687 |         \"\"\"Generate standardized error result.\"\"\"\n688 |         self.processing_stats[\"errors_encountered\"] += 1\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/K_knowledge_extraction/chunking_processor.py:686:90\n    |\n684 |         return dict(self.processing_stats)\n685 |     \n686 |     def _generate_error_result(self, error_message: str, operation_id: str) -> Dict[str, Any]:\n    |                                                                                          ^^^\n687 |         \"\"\"Generate standardized error result.\"\"\"\n688 |         self.processing_stats[\"errors_encountered\"] += 1\n    |\n\nF821 Undefined name `Path`\n  --> canonical_flow/K_knowledge_extraction/dnp_alignment_analyzer.py:21:24\n   |\n20 | # Import total ordering base\n21 | sys.path.insert(0, str(Path(__file__).resolve().parents[2]))\n   |                        ^^^^\n22 | # # # from total_ordering_base import TotalOrderingBase, DeterministicCollectionMixin  # Module not found  # Module not found  # Modul\u2026\n   |\n\nF821 Undefined name `dataclass`\n  --> canonical_flow/K_knowledge_extraction/dnp_alignment_analyzer.py:27:2\n   |\n27 | @dataclass\n   |  ^^^^^^^^^\n28 | class DNPAlignmentEvidence:\n29 |     \"\"\"Evidence structure for DNP alignment analysis\"\"\"\n   |\n\nF821 Undefined name `dataclass`\n  --> canonical_flow/K_knowledge_extraction/dnp_alignment_analyzer.py:42:2\n   |\n42 | @dataclass\n   |  ^^^^^^^^^\n43 | class DeviationScore:\n44 |     \"\"\"Compliance deviation score for a Dec\u00e1logo point\"\"\"\n   |\n\nF821 Undefined name `dataclass`\n  --> canonical_flow/K_knowledge_extraction/dnp_alignment_analyzer.py:54:2\n   |\n54 | @dataclass\n   |  ^^^^^^^^^\n55 | class DNPBaselineValidation:\n56 |     \"\"\"Validation results for DNP baseline standards\"\"\"\n   |\n\nF821 Undefined name `List`\n  --> canonical_flow/K_knowledge_extraction/dnp_alignment_analyzer.py:60:24\n   |\n58 |     baseline_version: str\n59 |     validation_status: str  # \"valid\", \"outdated\", \"missing\", \"malformed\"\n60 |     validation_issues: List[str] = field(default_factory=list)\n   |                        ^^^^\n61 |     fallback_applied: bool = False\n   |\n\nF821 Undefined name `field`\n  --> canonical_flow/K_knowledge_extraction/dnp_alignment_analyzer.py:60:36\n   |\n58 |     baseline_version: str\n59 |     validation_status: str  # \"valid\", \"outdated\", \"missing\", \"malformed\"\n60 |     validation_issues: List[str] = field(default_factory=list)\n   |                                    ^^^^^\n61 |     fallback_applied: bool = False\n   |\n\nF821 Undefined name `dataclass`\n  --> canonical_flow/K_knowledge_extraction/dnp_alignment_analyzer.py:64:2\n   |\n64 | @dataclass\n   |  ^^^^^^^^^\n65 | class ActionableRecommendation:\n66 |     \"\"\"Actionable recommendation for improving compliance\"\"\"\n   |\n\nF821 Undefined name `List`\n  --> canonical_flow/K_knowledge_extraction/dnp_alignment_analyzer.py:72:25\n   |\n70 |     impact_estimate: float\n71 |     implementation_complexity: str  # \"low\", \"medium\", \"high\"\n72 |     resources_required: List[str] = field(default_factory=list)\n   |                         ^^^^\n   |\n\nF821 Undefined name `field`\n  --> canonical_flow/K_knowledge_extraction/dnp_alignment_analyzer.py:72:37\n   |\n70 |     impact_estimate: float\n71 |     implementation_complexity: str  # \"low\", \"medium\", \"high\"\n72 |     resources_required: List[str] = field(default_factory=list)\n   |                                     ^^^^^\n   |\n\nF821 Undefined name `TotalOrderingBase`\n  --> canonical_flow/K_knowledge_extraction/dnp_alignment_analyzer.py:75:28\n   |\n75 | class DNPAlignmentAnalyzer(TotalOrderingBase, DeterministicCollectionMixin):\n   |                            ^^^^^^^^^^^^^^^^^\n76 |     \"\"\"\n77 |     DNP Alignment Analyzer with deterministic processing and total ordering.\n   |\n\nF821 Undefined name `DeterministicCollectionMixin`\n  --> canonical_flow/K_knowledge_extraction/dnp_alignment_analyzer.py:75:47\n   |\n75 | class DNPAlignmentAnalyzer(TotalOrderingBase, DeterministicCollectionMixin):\n   |                                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n76 |     \"\"\"\n77 |     DNP Alignment Analyzer with deterministic processing and total ordering.\n   |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/K_knowledge_extraction/dnp_alignment_analyzer.py:98:37\n    |\n 96 |         self.update_state_hash(self._get_initial_state())\n 97 |     \n 98 |     def _get_initial_state(self) -> Dict[str, Any]:\n    |                                     ^^^^\n 99 |         \"\"\"Get initial state for hash calculation\"\"\"\n100 |         return {\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/K_knowledge_extraction/dnp_alignment_analyzer.py:98:47\n    |\n 96 |         self.update_state_hash(self._get_initial_state())\n 97 |     \n 98 |     def _get_initial_state(self) -> Dict[str, Any]:\n    |                                               ^^^\n 99 |         \"\"\"Get initial state for hash calculation\"\"\"\n100 |         return {\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/K_knowledge_extraction/dnp_alignment_analyzer.py:106:46\n    |\n104 |         }\n105 |     \n106 |     def _initialize_decalogo_points(self) -> Dict[int, Dict[str, Any]]:\n    |                                              ^^^^\n107 |         \"\"\"Initialize the 10 Dec\u00e1logo points configuration\"\"\"\n108 |         return {\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/K_knowledge_extraction/dnp_alignment_analyzer.py:106:56\n    |\n104 |         }\n105 |     \n106 |     def _initialize_decalogo_points(self) -> Dict[int, Dict[str, Any]]:\n    |                                                        ^^^^\n107 |         \"\"\"Initialize the 10 Dec\u00e1logo points configuration\"\"\"\n108 |         return {\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/K_knowledge_extraction/dnp_alignment_analyzer.py:106:66\n    |\n104 |         }\n105 |     \n106 |     def _initialize_decalogo_points(self) -> Dict[int, Dict[str, Any]]:\n    |                                                                  ^^^\n107 |         \"\"\"Initialize the 10 Dec\u00e1logo points configuration\"\"\"\n108 |         return {\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/K_knowledge_extraction/dnp_alignment_analyzer.py:181:38\n    |\n179 |         }\n180 |     \n181 |     def _load_dnp_baselines(self) -> Dict[str, Any]:\n    |                                      ^^^^\n182 |         \"\"\"Load DNP baseline standards with error handling\"\"\"\n183 |         try:\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/K_knowledge_extraction/dnp_alignment_analyzer.py:181:48\n    |\n179 |         }\n180 |     \n181 |     def _load_dnp_baselines(self) -> Dict[str, Any]:\n    |                                                ^^^\n182 |         \"\"\"Load DNP baseline standards with error handling\"\"\"\n183 |         try:\n    |\n\nF821 Undefined name `Path`\n   --> canonical_flow/K_knowledge_extraction/dnp_alignment_analyzer.py:186:17\n    |\n184 | # # #             # Attempt to load DNP baselines from standard locations  # Module not found  # Module not found  # Module not found\n185 |             baseline_paths = [\n186 |                 Path(\"data/dnp_baselines.json\"),\n    |                 ^^^^\n187 |                 Path(\"standards_alignment/dnp_baselines.json\"),\n188 |                 Path(\"canonical_flow/knowledge/dnp_baselines.json\")\n    |\n\nF821 Undefined name `Path`\n   --> canonical_flow/K_knowledge_extraction/dnp_alignment_analyzer.py:187:17\n    |\n185 |             baseline_paths = [\n186 |                 Path(\"data/dnp_baselines.json\"),\n187 |                 Path(\"standards_alignment/dnp_baselines.json\"),\n    |                 ^^^^\n188 |                 Path(\"canonical_flow/knowledge/dnp_baselines.json\")\n189 |             ]\n    |\n\nF821 Undefined name `Path`\n   --> canonical_flow/K_knowledge_extraction/dnp_alignment_analyzer.py:188:17\n    |\n186 |                 Path(\"data/dnp_baselines.json\"),\n187 |                 Path(\"standards_alignment/dnp_baselines.json\"),\n188 |                 Path(\"canonical_flow/knowledge/dnp_baselines.json\")\n    |                 ^^^^\n189 |             ]\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/K_knowledge_extraction/dnp_alignment_analyzer.py:206:29\n    |\n204 |             return {}\n205 |     \n206 |     def process(self, data: Any = None, context: Any = None) -> Dict[str, Any]:\n    |                             ^^^\n207 |         \"\"\"\n208 |         Main processing function implementing standardized API.\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/K_knowledge_extraction/dnp_alignment_analyzer.py:206:50\n    |\n204 |             return {}\n205 |     \n206 |     def process(self, data: Any = None, context: Any = None) -> Dict[str, Any]:\n    |                                                  ^^^\n207 |         \"\"\"\n208 |         Main processing function implementing standardized API.\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/K_knowledge_extraction/dnp_alignment_analyzer.py:206:65\n    |\n204 |             return {}\n205 |     \n206 |     def process(self, data: Any = None, context: Any = None) -> Dict[str, Any]:\n    |                                                                 ^^^^\n207 |         \"\"\"\n208 |         Main processing function implementing standardized API.\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/K_knowledge_extraction/dnp_alignment_analyzer.py:206:75\n    |\n204 |             return {}\n205 |     \n206 |     def process(self, data: Any = None, context: Any = None) -> Dict[str, Any]:\n    |                                                                           ^^^\n207 |         \"\"\"\n208 |         Main processing function implementing standardized API.\n    |\n\nF821 Undefined name `datetime`\n   --> canonical_flow/K_knowledge_extraction/dnp_alignment_analyzer.py:244:30\n    |\n242 |                 \"recommendations\": recommendations,\n243 |                 \"processing_stats\": self._get_processing_stats(artifacts),\n244 |                 \"timestamp\": datetime.now().isoformat()\n    |                              ^^^^^^^^\n245 |             }\n    |\n\nF821 Undefined name `datetime`\n   --> canonical_flow/K_knowledge_extraction/dnp_alignment_analyzer.py:263:30\n    |\n261 |                 \"error\": str(e),\n262 |                 \"component_metadata\": self.get_deterministic_metadata(),\n263 |                 \"timestamp\": datetime.now().isoformat()\n    |                              ^^^^^^^^\n264 |             }\n265 |             return error_result\n    |\n\nE722 Do not use bare `except`\n   --> canonical_flow/K_knowledge_extraction/dnp_alignment_analyzer.py:298:17\n    |\n296 |                         validation_issues.append(f\"Baseline version {baseline_version} may be outdated\")\n297 |                         validation_status = \"outdated\"\n298 |                 except:\n    |                 ^^^^^^\n299 |                     validation_issues.append(\"Cannot parse baseline version\")\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/K_knowledge_extraction/dnp_alignment_analyzer.py:309:50\n    |\n307 |         )\n308 |     \n309 |     def _extract_knowledge_artifacts(self, data: Dict[str, Any], context: Dict[str, Any]) -> Dict[str, Any]:\n    |                                                  ^^^^\n310 |         \"\"\"Extract and organize knowledge stage artifacts\"\"\"\n311 |         artifacts = {\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/K_knowledge_extraction/dnp_alignment_analyzer.py:309:60\n    |\n307 |         )\n308 |     \n309 |     def _extract_knowledge_artifacts(self, data: Dict[str, Any], context: Dict[str, Any]) -> Dict[str, Any]:\n    |                                                            ^^^\n310 |         \"\"\"Extract and organize knowledge stage artifacts\"\"\"\n311 |         artifacts = {\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/K_knowledge_extraction/dnp_alignment_analyzer.py:309:75\n    |\n307 |         )\n308 |     \n309 |     def _extract_knowledge_artifacts(self, data: Dict[str, Any], context: Dict[str, Any]) -> Dict[str, Any]:\n    |                                                                           ^^^^\n310 |         \"\"\"Extract and organize knowledge stage artifacts\"\"\"\n311 |         artifacts = {\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/K_knowledge_extraction/dnp_alignment_analyzer.py:309:85\n    |\n307 |         )\n308 |     \n309 |     def _extract_knowledge_artifacts(self, data: Dict[str, Any], context: Dict[str, Any]) -> Dict[str, Any]:\n    |                                                                                     ^^^\n310 |         \"\"\"Extract and organize knowledge stage artifacts\"\"\"\n311 |         artifacts = {\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/K_knowledge_extraction/dnp_alignment_analyzer.py:309:94\n    |\n307 |         )\n308 |     \n309 |     def _extract_knowledge_artifacts(self, data: Dict[str, Any], context: Dict[str, Any]) -> Dict[str, Any]:\n    |                                                                                              ^^^^\n310 |         \"\"\"Extract and organize knowledge stage artifacts\"\"\"\n311 |         artifacts = {\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/K_knowledge_extraction/dnp_alignment_analyzer.py:309:104\n    |\n307 |         )\n308 |     \n309 |     def _extract_knowledge_artifacts(self, data: Dict[str, Any], context: Dict[str, Any]) -> Dict[str, Any]:\n    |                                                                                                        ^^^\n310 |         \"\"\"Extract and organize knowledge stage artifacts\"\"\"\n311 |         artifacts = {\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/K_knowledge_extraction/dnp_alignment_analyzer.py:363:56\n    |\n361 |         return artifacts\n362 |     \n363 |     def _compute_alignment_deviations(self, artifacts: Dict[str, Any]) -> Dict[int, Dict[str, Any]]:\n    |                                                        ^^^^\n364 |         \"\"\"Compute compliance deviations for each Dec\u00e1logo point\"\"\"\n365 |         alignment_results = {}\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/K_knowledge_extraction/dnp_alignment_analyzer.py:363:66\n    |\n361 |         return artifacts\n362 |     \n363 |     def _compute_alignment_deviations(self, artifacts: Dict[str, Any]) -> Dict[int, Dict[str, Any]]:\n    |                                                                  ^^^\n364 |         \"\"\"Compute compliance deviations for each Dec\u00e1logo point\"\"\"\n365 |         alignment_results = {}\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/K_knowledge_extraction/dnp_alignment_analyzer.py:363:75\n    |\n361 |         return artifacts\n362 |     \n363 |     def _compute_alignment_deviations(self, artifacts: Dict[str, Any]) -> Dict[int, Dict[str, Any]]:\n    |                                                                           ^^^^\n364 |         \"\"\"Compute compliance deviations for each Dec\u00e1logo point\"\"\"\n365 |         alignment_results = {}\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/K_knowledge_extraction/dnp_alignment_analyzer.py:363:85\n    |\n361 |         return artifacts\n362 |     \n363 |     def _compute_alignment_deviations(self, artifacts: Dict[str, Any]) -> Dict[int, Dict[str, Any]]:\n    |                                                                                     ^^^^\n364 |         \"\"\"Compute compliance deviations for each Dec\u00e1logo point\"\"\"\n365 |         alignment_results = {}\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/K_knowledge_extraction/dnp_alignment_analyzer.py:363:95\n    |\n361 |         return artifacts\n362 |     \n363 |     def _compute_alignment_deviations(self, artifacts: Dict[str, Any]) -> Dict[int, Dict[str, Any]]:\n    |                                                                                               ^^^\n364 |         \"\"\"Compute compliance deviations for each Dec\u00e1logo point\"\"\"\n365 |         alignment_results = {}\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/K_knowledge_extraction/dnp_alignment_analyzer.py:410:68\n    |\n408 |         return alignment_results\n409 |     \n410 |     def _extract_point_evidence(self, point_id: int, point_config: Dict[str, Any], \n    |                                                                    ^^^^\n411 |                               artifacts: Dict[str, Any]) -> List[DNPAlignmentEvidence]:\n412 |         \"\"\"Extract evidence relevant to a specific Dec\u00e1logo point\"\"\"\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/K_knowledge_extraction/dnp_alignment_analyzer.py:410:78\n    |\n408 |         return alignment_results\n409 |     \n410 |     def _extract_point_evidence(self, point_id: int, point_config: Dict[str, Any], \n    |                                                                              ^^^\n411 |                               artifacts: Dict[str, Any]) -> List[DNPAlignmentEvidence]:\n412 |         \"\"\"Extract evidence relevant to a specific Dec\u00e1logo point\"\"\"\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/K_knowledge_extraction/dnp_alignment_analyzer.py:411:42\n    |\n410 |     def _extract_point_evidence(self, point_id: int, point_config: Dict[str, Any], \n411 |                               artifacts: Dict[str, Any]) -> List[DNPAlignmentEvidence]:\n    |                                          ^^^^\n412 |         \"\"\"Extract evidence relevant to a specific Dec\u00e1logo point\"\"\"\n413 |         evidence_items = []\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/K_knowledge_extraction/dnp_alignment_analyzer.py:411:52\n    |\n410 |     def _extract_point_evidence(self, point_id: int, point_config: Dict[str, Any], \n411 |                               artifacts: Dict[str, Any]) -> List[DNPAlignmentEvidence]:\n    |                                                    ^^^\n412 |         \"\"\"Extract evidence relevant to a specific Dec\u00e1logo point\"\"\"\n413 |         evidence_items = []\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/K_knowledge_extraction/dnp_alignment_analyzer.py:411:61\n    |\n410 |     def _extract_point_evidence(self, point_id: int, point_config: Dict[str, Any], \n411 |                               artifacts: Dict[str, Any]) -> List[DNPAlignmentEvidence]:\n    |                                                             ^^^^\n412 |         \"\"\"Extract evidence relevant to a specific Dec\u00e1logo point\"\"\"\n413 |         evidence_items = []\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/K_knowledge_extraction/dnp_alignment_analyzer.py:463:64\n    |\n461 |         return sorted(evidence_items, key=lambda x: x.confidence, reverse=True)\n462 |     \n463 |     def _get_point_keywords(self, point_id: int, point_config: Dict[str, Any]) -> List[str]:\n    |                                                                ^^^^\n464 |         \"\"\"Get relevant keywords for a Dec\u00e1logo point\"\"\"\n465 |         keyword_map = {\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/K_knowledge_extraction/dnp_alignment_analyzer.py:463:74\n    |\n461 |         return sorted(evidence_items, key=lambda x: x.confidence, reverse=True)\n462 |     \n463 |     def _get_point_keywords(self, point_id: int, point_config: Dict[str, Any]) -> List[str]:\n    |                                                                          ^^^\n464 |         \"\"\"Get relevant keywords for a Dec\u00e1logo point\"\"\"\n465 |         keyword_map = {\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/K_knowledge_extraction/dnp_alignment_analyzer.py:463:83\n    |\n461 |         return sorted(evidence_items, key=lambda x: x.confidence, reverse=True)\n462 |     \n463 |     def _get_point_keywords(self, point_id: int, point_config: Dict[str, Any]) -> List[str]:\n    |                                                                                   ^^^^\n464 |         \"\"\"Get relevant keywords for a Dec\u00e1logo point\"\"\"\n465 |         keyword_map = {\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/K_knowledge_extraction/dnp_alignment_analyzer.py:495:72\n    |\n493 |         return priority_scores.get(point_config.get(\"priority\", \"MEDIA\"), 0.5)\n494 |     \n495 |     def _compute_compliance_score(self, point_id: int, evidence_items: List[DNPAlignmentEvidence], \n    |                                                                        ^^^^\n496 |                                 artifacts: Dict[str, Any]) -> float:\n497 |         \"\"\"Compute compliance score based on evidence\"\"\"\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/K_knowledge_extraction/dnp_alignment_analyzer.py:496:44\n    |\n495 |     def _compute_compliance_score(self, point_id: int, evidence_items: List[DNPAlignmentEvidence], \n496 |                                 artifacts: Dict[str, Any]) -> float:\n    |                                            ^^^^\n497 |         \"\"\"Compute compliance score based on evidence\"\"\"\n498 |         if not evidence_items:\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/K_knowledge_extraction/dnp_alignment_analyzer.py:496:54\n    |\n495 |     def _compute_compliance_score(self, point_id: int, evidence_items: List[DNPAlignmentEvidence], \n496 |                                 artifacts: Dict[str, Any]) -> float:\n    |                                                      ^^^\n497 |         \"\"\"Compute compliance score based on evidence\"\"\"\n498 |         if not evidence_items:\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/K_knowledge_extraction/dnp_alignment_analyzer.py:528:51\n    |\n526 |         return min(1.0, base_score + evidence_boost)\n527 |     \n528 |     def _compute_confidence(self, evidence_items: List[DNPAlignmentEvidence]) -> float:\n    |                                                   ^^^^\n529 |         \"\"\"Compute confidence in the alignment assessment\"\"\"\n530 |         if not evidence_items:\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/K_knowledge_extraction/dnp_alignment_analyzer.py:549:68\n    |\n547 |             return \"critical\"\n548 |     \n549 |     def _create_fallback_result(self, point_id: int, point_config: Dict[str, Any], error: str) -> Dict[str, Any]:\n    |                                                                    ^^^^\n550 |         \"\"\"Create fallback result when processing fails\"\"\"\n551 |         baseline_score = self._get_baseline_score(point_id)\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/K_knowledge_extraction/dnp_alignment_analyzer.py:549:78\n    |\n547 |             return \"critical\"\n548 |     \n549 |     def _create_fallback_result(self, point_id: int, point_config: Dict[str, Any], error: str) -> Dict[str, Any]:\n    |                                                                              ^^^\n550 |         \"\"\"Create fallback result when processing fails\"\"\"\n551 |         baseline_score = self._get_baseline_score(point_id)\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/K_knowledge_extraction/dnp_alignment_analyzer.py:549:99\n    |\n547 |             return \"critical\"\n548 |     \n549 |     def _create_fallback_result(self, point_id: int, point_config: Dict[str, Any], error: str) -> Dict[str, Any]:\n    |                                                                                                   ^^^^\n550 |         \"\"\"Create fallback result when processing fails\"\"\"\n551 |         baseline_score = self._get_baseline_score(point_id)\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/K_knowledge_extraction/dnp_alignment_analyzer.py:549:109\n    |\n547 |             return \"critical\"\n548 |     \n549 |     def _create_fallback_result(self, point_id: int, point_config: Dict[str, Any], error: str) -> Dict[str, Any]:\n    |                                                                                                             ^^^\n550 |         \"\"\"Create fallback result when processing fails\"\"\"\n551 |         baseline_score = self._get_baseline_score(point_id)\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/K_knowledge_extraction/dnp_alignment_analyzer.py:574:60\n    |\n572 |         }\n573 |     \n574 |     def _generate_recommendations(self, alignment_results: Dict[int, Dict[str, Any]]) -> List[ActionableRecommendation]:\n    |                                                            ^^^^\n575 |         \"\"\"Generate actionable recommendations based on alignment results\"\"\"\n576 |         recommendations = []\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/K_knowledge_extraction/dnp_alignment_analyzer.py:574:70\n    |\n572 |         }\n573 |     \n574 |     def _generate_recommendations(self, alignment_results: Dict[int, Dict[str, Any]]) -> List[ActionableRecommendation]:\n    |                                                                      ^^^^\n575 |         \"\"\"Generate actionable recommendations based on alignment results\"\"\"\n576 |         recommendations = []\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/K_knowledge_extraction/dnp_alignment_analyzer.py:574:80\n    |\n572 |         }\n573 |     \n574 |     def _generate_recommendations(self, alignment_results: Dict[int, Dict[str, Any]]) -> List[ActionableRecommendation]:\n    |                                                                                ^^^\n575 |         \"\"\"Generate actionable recommendations based on alignment results\"\"\"\n576 |         recommendations = []\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/K_knowledge_extraction/dnp_alignment_analyzer.py:574:90\n    |\n572 |         }\n573 |     \n574 |     def _generate_recommendations(self, alignment_results: Dict[int, Dict[str, Any]]) -> List[ActionableRecommendation]:\n    |                                                                                          ^^^^\n575 |         \"\"\"Generate actionable recommendations based on alignment results\"\"\"\n576 |         recommendations = []\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/K_knowledge_extraction/dnp_alignment_analyzer.py:606:77\n    |\n604 |         return [rec.__dict__ for rec in recommendations]\n605 |     \n606 |     def _generate_recommendation_text(self, point_id: int, deviation_score: Dict[str, Any], \n    |                                                                             ^^^^\n607 |                                     point_config: Dict[str, Any]) -> str:\n608 |         \"\"\"Generate specific recommendation text\"\"\"\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/K_knowledge_extraction/dnp_alignment_analyzer.py:606:87\n    |\n604 |         return [rec.__dict__ for rec in recommendations]\n605 |     \n606 |     def _generate_recommendation_text(self, point_id: int, deviation_score: Dict[str, Any], \n    |                                                                                       ^^^\n607 |                                     point_config: Dict[str, Any]) -> str:\n608 |         \"\"\"Generate specific recommendation text\"\"\"\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/K_knowledge_extraction/dnp_alignment_analyzer.py:607:51\n    |\n606 |     def _generate_recommendation_text(self, point_id: int, deviation_score: Dict[str, Any], \n607 |                                     point_config: Dict[str, Any]) -> str:\n    |                                                   ^^^^\n608 |         \"\"\"Generate specific recommendation text\"\"\"\n609 |         compliance_score = deviation_score[\"compliance_score\"]\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/K_knowledge_extraction/dnp_alignment_analyzer.py:607:61\n    |\n606 |     def _generate_recommendation_text(self, point_id: int, deviation_score: Dict[str, Any], \n607 |                                     point_config: Dict[str, Any]) -> str:\n    |                                                             ^^^\n608 |         \"\"\"Generate specific recommendation text\"\"\"\n609 |         compliance_score = deviation_score[\"compliance_score\"]\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/K_knowledge_extraction/dnp_alignment_analyzer.py:621:78\n    |\n619 |             return f\"Optimizar: Ajustar estrategias para {point_name} y mantener el nivel de cumplimiento actual.\"\n620 |     \n621 |     def _assess_implementation_complexity(self, point_id: int, point_config: Dict[str, Any]) -> str:\n    |                                                                              ^^^^\n622 |         \"\"\"Assess implementation complexity for a recommendation\"\"\"\n623 |         sector = point_config.get(\"sector\", \"\")\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/K_knowledge_extraction/dnp_alignment_analyzer.py:621:88\n    |\n619 |             return f\"Optimizar: Ajustar estrategias para {point_name} y mantener el nivel de cumplimiento actual.\"\n620 |     \n621 |     def _assess_implementation_complexity(self, point_id: int, point_config: Dict[str, Any]) -> str:\n    |                                                                                        ^^^\n622 |         \"\"\"Assess implementation complexity for a recommendation\"\"\"\n623 |         sector = point_config.get(\"sector\", \"\")\n    |\n\nF841 Local variable `sector` is assigned to but never used\n   --> canonical_flow/K_knowledge_extraction/dnp_alignment_analyzer.py:623:9\n    |\n621 |     def _assess_implementation_complexity(self, point_id: int, point_config: Dict[str, Any]) -> str:\n622 |         \"\"\"Assess implementation complexity for a recommendation\"\"\"\n623 |         sector = point_config.get(\"sector\", \"\")\n    |         ^^^^^^\n624 |         priority = point_config.get(\"priority\", \"MEDIA\")\n625 |         budget_percent = point_config.get(\"budget_min_percent\", 0)\n    |\nhelp: Remove assignment to unused variable `sector`\n\nF821 Undefined name `Dict`\n   --> canonical_flow/K_knowledge_extraction/dnp_alignment_analyzer.py:634:73\n    |\n632 |             return \"low\"\n633 |     \n634 |     def _identify_required_resources(self, point_id: int, point_config: Dict[str, Any]) -> List[str]:\n    |                                                                         ^^^^\n635 |         \"\"\"Identify required resources for implementation\"\"\"\n636 |         sector = point_config.get(\"sector\", \"\")\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/K_knowledge_extraction/dnp_alignment_analyzer.py:634:83\n    |\n632 |             return \"low\"\n633 |     \n634 |     def _identify_required_resources(self, point_id: int, point_config: Dict[str, Any]) -> List[str]:\n    |                                                                                   ^^^\n635 |         \"\"\"Identify required resources for implementation\"\"\"\n636 |         sector = point_config.get(\"sector\", \"\")\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/K_knowledge_extraction/dnp_alignment_analyzer.py:634:92\n    |\n632 |             return \"low\"\n633 |     \n634 |     def _identify_required_resources(self, point_id: int, point_config: Dict[str, Any]) -> List[str]:\n    |                                                                                            ^^^^\n635 |         \"\"\"Identify required resources for implementation\"\"\"\n636 |         sector = point_config.get(\"sector\", \"\")\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/K_knowledge_extraction/dnp_alignment_analyzer.py:649:48\n    |\n647 |         return resource_map.get(sector, [\"recursos_generales\", \"personal_capacitado\", \"coordinacion\"])\n648 |     \n649 |     def _get_processing_stats(self, artifacts: Dict[str, Any]) -> Dict[str, Any]:\n    |                                                ^^^^\n650 |         \"\"\"Get processing statistics\"\"\"\n651 |         return {\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/K_knowledge_extraction/dnp_alignment_analyzer.py:649:58\n    |\n647 |         return resource_map.get(sector, [\"recursos_generales\", \"personal_capacitado\", \"coordinacion\"])\n648 |     \n649 |     def _get_processing_stats(self, artifacts: Dict[str, Any]) -> Dict[str, Any]:\n    |                                                          ^^^\n650 |         \"\"\"Get processing statistics\"\"\"\n651 |         return {\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/K_knowledge_extraction/dnp_alignment_analyzer.py:649:67\n    |\n647 |         return resource_map.get(sector, [\"recursos_generales\", \"personal_capacitado\", \"coordinacion\"])\n648 |     \n649 |     def _get_processing_stats(self, artifacts: Dict[str, Any]) -> Dict[str, Any]:\n    |                                                                   ^^^^\n650 |         \"\"\"Get processing statistics\"\"\"\n651 |         return {\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/K_knowledge_extraction/dnp_alignment_analyzer.py:649:77\n    |\n647 |         return resource_map.get(sector, [\"recursos_generales\", \"personal_capacitado\", \"coordinacion\"])\n648 |     \n649 |     def _get_processing_stats(self, artifacts: Dict[str, Any]) -> Dict[str, Any]:\n    |                                                                             ^^^\n650 |         \"\"\"Get processing statistics\"\"\"\n651 |         return {\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/K_knowledge_extraction/dnp_alignment_analyzer.py:660:50\n    |\n658 |         }\n659 |     \n660 |     def _save_alignment_artifacts(self, results: Dict[str, Any], context: Dict[str, Any]) -> None:\n    |                                                  ^^^^\n661 |         \"\"\"Save alignment results to canonical_flow/knowledge/ directory\"\"\"\n662 |         try:\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/K_knowledge_extraction/dnp_alignment_analyzer.py:660:60\n    |\n658 |         }\n659 |     \n660 |     def _save_alignment_artifacts(self, results: Dict[str, Any], context: Dict[str, Any]) -> None:\n    |                                                            ^^^\n661 |         \"\"\"Save alignment results to canonical_flow/knowledge/ directory\"\"\"\n662 |         try:\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/K_knowledge_extraction/dnp_alignment_analyzer.py:660:75\n    |\n658 |         }\n659 |     \n660 |     def _save_alignment_artifacts(self, results: Dict[str, Any], context: Dict[str, Any]) -> None:\n    |                                                                           ^^^^\n661 |         \"\"\"Save alignment results to canonical_flow/knowledge/ directory\"\"\"\n662 |         try:\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/K_knowledge_extraction/dnp_alignment_analyzer.py:660:85\n    |\n658 |         }\n659 |     \n660 |     def _save_alignment_artifacts(self, results: Dict[str, Any], context: Dict[str, Any]) -> None:\n    |                                                                                     ^^^\n661 |         \"\"\"Save alignment results to canonical_flow/knowledge/ directory\"\"\"\n662 |         try:\n    |\n\nF821 Undefined name `Path`\n   --> canonical_flow/K_knowledge_extraction/dnp_alignment_analyzer.py:671:32\n    |\n669 |                 elif \"filename\" in metadata:\n670 |                     filename = metadata[\"filename\"]\n671 |                     doc_stem = Path(filename).stem if filename else doc_stem\n    |                                ^^^^\n672 |             \n673 |             # Create output directory\n    |\n\nF821 Undefined name `Path`\n   --> canonical_flow/K_knowledge_extraction/dnp_alignment_analyzer.py:674:26\n    |\n673 |             # Create output directory\n674 |             output_dir = Path(\"canonical_flow/knowledge\")\n    |                          ^^^^\n675 |             output_dir.mkdir(parents=True, exist_ok=True)\n    |\n\nF821 Undefined name `Path`\n  --> canonical_flow/K_knowledge_extraction/embedding_builder.py:24:20\n   |\n22 | try:\n23 |     # Add project root to path for imports\n24 |     project_root = Path(__file__).resolve().parents[1]\n   |                    ^^^^\n25 |     if str(project_root) not in sys.path:\n26 |         sys.path.insert(0, str(project_root))\n   |\n\nF821 Undefined name `importlib_util`\n  --> canonical_flow/K_knowledge_extraction/embedding_builder.py:31:16\n   |\n29 |     original_file = project_root / \"embedding_builder.py\"\n30 |     if original_file.exists():\n31 |         spec = importlib_util.spec_from_file_location(\n   |                ^^^^^^^^^^^^^^\n32 |             f\"original_{alias_code.lower()}\", \n33 |             str(original_file)\n   |\n\nF821 Undefined name `importlib_util`\n  --> canonical_flow/K_knowledge_extraction/embedding_builder.py:37:31\n   |\n36 |         if spec and spec.loader:\n37 |             original_module = importlib_util.module_from_spec(spec)\n   |                               ^^^^^^^^^^^^^^\n38 |             spec.loader.exec_module(original_module)\n   |\n\nF821 Undefined name `e`\n  --> canonical_flow/K_knowledge_extraction/embedding_builder.py:56:67\n   |\n54 |     def process(data=None, context=None):\n55 |         \"\"\"Placeholder process function for failed import.\"\"\"\n56 |         return {\"error\": f\"Module {alias_source} failed to load: {e}\"}\n   |                                                                   ^\n   |\n\nF821 Undefined name `Path`\n  --> canonical_flow/K_knowledge_extraction/embedding_generator.py:24:20\n   |\n22 | try:\n23 |     # Add project root to path for imports\n24 |     project_root = Path(__file__).resolve().parents[1]\n   |                    ^^^^\n25 |     if str(project_root) not in sys.path:\n26 |         sys.path.insert(0, str(project_root))\n   |\n\nF821 Undefined name `importlib_util`\n  --> canonical_flow/K_knowledge_extraction/embedding_generator.py:31:16\n   |\n29 |     original_file = project_root / \"embedding_generator.py\"\n30 |     if original_file.exists():\n31 |         spec = importlib_util.spec_from_file_location(\n   |                ^^^^^^^^^^^^^^\n32 |             f\"original_{alias_code.lower()}\", \n33 |             str(original_file)\n   |\n\nF821 Undefined name `importlib_util`\n  --> canonical_flow/K_knowledge_extraction/embedding_generator.py:37:31\n   |\n36 |         if spec and spec.loader:\n37 |             original_module = importlib_util.module_from_spec(spec)\n   |                               ^^^^^^^^^^^^^^\n38 |             spec.loader.exec_module(original_module)\n   |\n\nF821 Undefined name `e`\n  --> canonical_flow/K_knowledge_extraction/embedding_generator.py:56:67\n   |\n54 |     def process(data=None, context=None):\n55 |         \"\"\"Placeholder process function for failed import.\"\"\"\n56 |         return {\"error\": f\"Module {alias_source} failed to load: {e}\"}\n   |                                                                   ^\n   |\n\nF401 [*] `os` imported but unused\n  --> canonical_flow/K_knowledge_extraction/entity_concept_extractor.py:21:8\n   |\n19 | # # # from pathlib import Path  # Module not found  # Module not found  # Module not found\n20 | # # # from typing import Dict, List, Optional, Any, Tuple, Set  # Module not found  # Module not found  # Module not found\n21 | import os\n   |        ^^\n22 |\n23 | # # # from total_ordering_base import TotalOrderingBase  # Module not found  # Module not found  # Module not found\n   |\nhelp: Remove unused import: `os`\n\nF821 Undefined name `TotalOrderingBase`\n  --> canonical_flow/K_knowledge_extraction/entity_concept_extractor.py:55:30\n   |\n55 | class EntityConceptExtractor(TotalOrderingBase):\n   |                              ^^^^^^^^^^^^^^^^^\n56 |     \"\"\"\n57 | # # #     Extracts terms, entities, and concepts from document text with deterministic results.  # Module not found  # Module not foun\u2026\n   |\n\nF821 Undefined name `SnowballStemmer`\n   --> canonical_flow/K_knowledge_extraction/entity_concept_extractor.py:162:32\n    |\n160 |             # Initialize stemmer for the language\n161 |             if self.language == 'es':\n162 |                 self.stemmer = SnowballStemmer('spanish')\n    |                                ^^^^^^^^^^^^^^^\n163 |             else:\n164 |                 self.stemmer = SnowballStemmer('english')\n    |\n\nF821 Undefined name `SnowballStemmer`\n   --> canonical_flow/K_knowledge_extraction/entity_concept_extractor.py:164:32\n    |\n162 |                 self.stemmer = SnowballStemmer('spanish')\n163 |             else:\n164 |                 self.stemmer = SnowballStemmer('english')\n    |                                ^^^^^^^^^^^^^^^\n165 |             \n166 |             logger.info(f\"Initialized NLTK stemmer for language: {self.language}\")\n    |\n\nF821 Undefined name `Set`\n   --> canonical_flow/K_knowledge_extraction/entity_concept_extractor.py:170:34\n    |\n168 |             logger.warning(f\"Failed to initialize NLTK stemmer: {e}\")\n169 |     \n170 |     def _get_stop_words(self) -> Set[str]:\n    |                                  ^^^\n171 |         \"\"\"Get stop words for the configured language.\"\"\"\n172 |         stop_words = set()\n    |\n\nF821 Undefined name `stopwords`\n   --> canonical_flow/K_knowledge_extraction/entity_concept_extractor.py:188:38\n    |\n186 |             try:\n187 |                 if self.language == 'es':\n188 |                     stop_words = set(stopwords.words('spanish'))\n    |                                      ^^^^^^^^^\n189 |                 else:\n190 |                     stop_words = set(stopwords.words('english'))\n    |\n\nF821 Undefined name `stopwords`\n   --> canonical_flow/K_knowledge_extraction/entity_concept_extractor.py:190:38\n    |\n188 |                     stop_words = set(stopwords.words('spanish'))\n189 |                 else:\n190 |                     stop_words = set(stopwords.words('english'))\n    |                                      ^^^^^^^^^\n191 |             except:\n192 |                 # Fallback to default\n    |\n\nE722 Do not use bare `except`\n   --> canonical_flow/K_knowledge_extraction/entity_concept_extractor.py:191:13\n    |\n189 |                 else:\n190 |                     stop_words = set(stopwords.words('english'))\n191 |             except:\n    |             ^^^^^^\n192 |                 # Fallback to default\n193 |                 if self.language == 'es':\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/K_knowledge_extraction/entity_concept_extractor.py:201:58\n    |\n199 |         return stop_words\n200 |     \n201 |     def process(self, document_text: str, page_metadata: List[Dict[str, Any]]) -> Dict[str, Any]:\n    |                                                          ^^^^\n202 |         \"\"\"\n203 |         Process document text and extract terms, entities, and concepts.\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/K_knowledge_extraction/entity_concept_extractor.py:201:63\n    |\n199 |         return stop_words\n200 |     \n201 |     def process(self, document_text: str, page_metadata: List[Dict[str, Any]]) -> Dict[str, Any]:\n    |                                                               ^^^^\n202 |         \"\"\"\n203 |         Process document text and extract terms, entities, and concepts.\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/K_knowledge_extraction/entity_concept_extractor.py:201:73\n    |\n199 |         return stop_words\n200 |     \n201 |     def process(self, document_text: str, page_metadata: List[Dict[str, Any]]) -> Dict[str, Any]:\n    |                                                                         ^^^\n202 |         \"\"\"\n203 |         Process document text and extract terms, entities, and concepts.\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/K_knowledge_extraction/entity_concept_extractor.py:201:83\n    |\n199 |         return stop_words\n200 |     \n201 |     def process(self, document_text: str, page_metadata: List[Dict[str, Any]]) -> Dict[str, Any]:\n    |                                                                                   ^^^^\n202 |         \"\"\"\n203 |         Process document text and extract terms, entities, and concepts.\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/K_knowledge_extraction/entity_concept_extractor.py:201:93\n    |\n199 |         return stop_words\n200 |     \n201 |     def process(self, document_text: str, page_metadata: List[Dict[str, Any]]) -> Dict[str, Any]:\n    |                                                                                             ^^^\n202 |         \"\"\"\n203 |         Process document text and extract terms, entities, and concepts.\n    |\n\nF821 Undefined name `Path`\n   --> canonical_flow/K_knowledge_extraction/entity_concept_extractor.py:236:26\n    |\n234 |             # Generate output directory path\n235 |             stem = self._generate_document_stem(document_text, page_metadata)\n236 |             output_dir = Path(\"canonical_flow/knowledge\") / stem\n    |                          ^^^^\n237 |             \n238 |             # Write JSON artifacts\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/K_knowledge_extraction/entity_concept_extractor.py:270:67\n    |\n268 | # # #             raise ValueError(f\"Failed to process document: {str(e)}\") from e  # Module not found  # Module not found  # Module \u2026\n269 |     \n270 |     def _validate_inputs(self, document_text: str, page_metadata: List[Dict[str, Any]]):\n    |                                                                   ^^^^\n271 |         \"\"\"Validate input parameters.\"\"\"\n272 |         if not isinstance(document_text, str) or not document_text.strip():\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/K_knowledge_extraction/entity_concept_extractor.py:270:72\n    |\n268 | # # #             raise ValueError(f\"Failed to process document: {str(e)}\") from e  # Module not found  # Module not found  # Module \u2026\n269 |     \n270 |     def _validate_inputs(self, document_text: str, page_metadata: List[Dict[str, Any]]):\n    |                                                                        ^^^^\n271 |         \"\"\"Validate input parameters.\"\"\"\n272 |         if not isinstance(document_text, str) or not document_text.strip():\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/K_knowledge_extraction/entity_concept_extractor.py:270:82\n    |\n268 | # # #             raise ValueError(f\"Failed to process document: {str(e)}\") from e  # Module not found  # Module not found  # Module \u2026\n269 |     \n270 |     def _validate_inputs(self, document_text: str, page_metadata: List[Dict[str, Any]]):\n    |                                                                                  ^^^\n271 |         \"\"\"Validate input parameters.\"\"\"\n272 |         if not isinstance(document_text, str) or not document_text.strip():\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/K_knowledge_extraction/entity_concept_extractor.py:286:65\n    |\n284 |                 raise ValueError(f\"Page metadata item {i} missing 'text'\")\n285 |     \n286 |     def _extract_terms(self, document_text: str, page_metadata: List[Dict[str, Any]]) -> Dict[str, Any]:\n    |                                                                 ^^^^\n287 |         \"\"\"Extract terms with frequency counts and page anchors.\"\"\"\n288 |         terms_by_page = defaultdict(lambda: defaultdict(list))\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/K_knowledge_extraction/entity_concept_extractor.py:286:70\n    |\n284 |                 raise ValueError(f\"Page metadata item {i} missing 'text'\")\n285 |     \n286 |     def _extract_terms(self, document_text: str, page_metadata: List[Dict[str, Any]]) -> Dict[str, Any]:\n    |                                                                      ^^^^\n287 |         \"\"\"Extract terms with frequency counts and page anchors.\"\"\"\n288 |         terms_by_page = defaultdict(lambda: defaultdict(list))\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/K_knowledge_extraction/entity_concept_extractor.py:286:80\n    |\n284 |                 raise ValueError(f\"Page metadata item {i} missing 'text'\")\n285 |     \n286 |     def _extract_terms(self, document_text: str, page_metadata: List[Dict[str, Any]]) -> Dict[str, Any]:\n    |                                                                                ^^^\n287 |         \"\"\"Extract terms with frequency counts and page anchors.\"\"\"\n288 |         terms_by_page = defaultdict(lambda: defaultdict(list))\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/K_knowledge_extraction/entity_concept_extractor.py:286:90\n    |\n284 |                 raise ValueError(f\"Page metadata item {i} missing 'text'\")\n285 |     \n286 |     def _extract_terms(self, document_text: str, page_metadata: List[Dict[str, Any]]) -> Dict[str, Any]:\n    |                                                                                          ^^^^\n287 |         \"\"\"Extract terms with frequency counts and page anchors.\"\"\"\n288 |         terms_by_page = defaultdict(lambda: defaultdict(list))\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/K_knowledge_extraction/entity_concept_extractor.py:286:100\n    |\n284 |                 raise ValueError(f\"Page metadata item {i} missing 'text'\")\n285 |     \n286 |     def _extract_terms(self, document_text: str, page_metadata: List[Dict[str, Any]]) -> Dict[str, Any]:\n    |                                                                                                    ^^^\n287 |         \"\"\"Extract terms with frequency counts and page anchors.\"\"\"\n288 |         terms_by_page = defaultdict(lambda: defaultdict(list))\n    |\n\nF821 Undefined name `defaultdict`\n   --> canonical_flow/K_knowledge_extraction/entity_concept_extractor.py:288:25\n    |\n286 |     def _extract_terms(self, document_text: str, page_metadata: List[Dict[str, Any]]) -> Dict[str, Any]:\n287 |         \"\"\"Extract terms with frequency counts and page anchors.\"\"\"\n288 |         terms_by_page = defaultdict(lambda: defaultdict(list))\n    |                         ^^^^^^^^^^^\n289 |         global_term_counts = Counter()\n    |\n\nF821 Undefined name `defaultdict`\n   --> canonical_flow/K_knowledge_extraction/entity_concept_extractor.py:288:45\n    |\n286 |     def _extract_terms(self, document_text: str, page_metadata: List[Dict[str, Any]]) -> Dict[str, Any]:\n287 |         \"\"\"Extract terms with frequency counts and page anchors.\"\"\"\n288 |         terms_by_page = defaultdict(lambda: defaultdict(list))\n    |                                             ^^^^^^^^^^^\n289 |         global_term_counts = Counter()\n    |\n\nF821 Undefined name `Counter`\n   --> canonical_flow/K_knowledge_extraction/entity_concept_extractor.py:289:30\n    |\n287 |         \"\"\"Extract terms with frequency counts and page anchors.\"\"\"\n288 |         terms_by_page = defaultdict(lambda: defaultdict(list))\n289 |         global_term_counts = Counter()\n    |                              ^^^^^^^\n290 |         \n291 |         for page_data in page_metadata:\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/K_knowledge_extraction/entity_concept_extractor.py:336:68\n    |\n334 |         }\n335 |     \n336 |     def _extract_entities(self, document_text: str, page_metadata: List[Dict[str, Any]]) -> Dict[str, Any]:\n    |                                                                    ^^^^\n337 |         \"\"\"Extract named entities with location metadata.\"\"\"\n338 |         entities_by_page = defaultdict(lambda: defaultdict(list))\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/K_knowledge_extraction/entity_concept_extractor.py:336:73\n    |\n334 |         }\n335 |     \n336 |     def _extract_entities(self, document_text: str, page_metadata: List[Dict[str, Any]]) -> Dict[str, Any]:\n    |                                                                         ^^^^\n337 |         \"\"\"Extract named entities with location metadata.\"\"\"\n338 |         entities_by_page = defaultdict(lambda: defaultdict(list))\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/K_knowledge_extraction/entity_concept_extractor.py:336:83\n    |\n334 |         }\n335 |     \n336 |     def _extract_entities(self, document_text: str, page_metadata: List[Dict[str, Any]]) -> Dict[str, Any]:\n    |                                                                                   ^^^\n337 |         \"\"\"Extract named entities with location metadata.\"\"\"\n338 |         entities_by_page = defaultdict(lambda: defaultdict(list))\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/K_knowledge_extraction/entity_concept_extractor.py:336:93\n    |\n334 |         }\n335 |     \n336 |     def _extract_entities(self, document_text: str, page_metadata: List[Dict[str, Any]]) -> Dict[str, Any]:\n    |                                                                                             ^^^^\n337 |         \"\"\"Extract named entities with location metadata.\"\"\"\n338 |         entities_by_page = defaultdict(lambda: defaultdict(list))\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/K_knowledge_extraction/entity_concept_extractor.py:336:103\n    |\n334 |         }\n335 |     \n336 |     def _extract_entities(self, document_text: str, page_metadata: List[Dict[str, Any]]) -> Dict[str, Any]:\n    |                                                                                                       ^^^\n337 |         \"\"\"Extract named entities with location metadata.\"\"\"\n338 |         entities_by_page = defaultdict(lambda: defaultdict(list))\n    |\n\nF821 Undefined name `defaultdict`\n   --> canonical_flow/K_knowledge_extraction/entity_concept_extractor.py:338:28\n    |\n336 |     def _extract_entities(self, document_text: str, page_metadata: List[Dict[str, Any]]) -> Dict[str, Any]:\n337 |         \"\"\"Extract named entities with location metadata.\"\"\"\n338 |         entities_by_page = defaultdict(lambda: defaultdict(list))\n    |                            ^^^^^^^^^^^\n339 |         global_entity_counts = Counter()\n    |\n\nF821 Undefined name `defaultdict`\n   --> canonical_flow/K_knowledge_extraction/entity_concept_extractor.py:338:48\n    |\n336 |     def _extract_entities(self, document_text: str, page_metadata: List[Dict[str, Any]]) -> Dict[str, Any]:\n337 |         \"\"\"Extract named entities with location metadata.\"\"\"\n338 |         entities_by_page = defaultdict(lambda: defaultdict(list))\n    |                                                ^^^^^^^^^^^\n339 |         global_entity_counts = Counter()\n    |\n\nF821 Undefined name `Counter`\n   --> canonical_flow/K_knowledge_extraction/entity_concept_extractor.py:339:32\n    |\n337 |         \"\"\"Extract named entities with location metadata.\"\"\"\n338 |         entities_by_page = defaultdict(lambda: defaultdict(list))\n339 |         global_entity_counts = Counter()\n    |                                ^^^^^^^\n340 |         \n341 |         if not self.nlp:\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/K_knowledge_extraction/entity_concept_extractor.py:403:77\n    |\n401 |         }\n402 |     \n403 |     def _extract_entities_fallback(self, document_text: str, page_metadata: List[Dict[str, Any]]) -> Dict[str, Any]:\n    |                                                                             ^^^^\n404 |         \"\"\"Fallback entity extraction using simple patterns.\"\"\"\n405 |         entities_by_page = defaultdict(lambda: defaultdict(list))\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/K_knowledge_extraction/entity_concept_extractor.py:403:82\n    |\n401 |         }\n402 |     \n403 |     def _extract_entities_fallback(self, document_text: str, page_metadata: List[Dict[str, Any]]) -> Dict[str, Any]:\n    |                                                                                  ^^^^\n404 |         \"\"\"Fallback entity extraction using simple patterns.\"\"\"\n405 |         entities_by_page = defaultdict(lambda: defaultdict(list))\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/K_knowledge_extraction/entity_concept_extractor.py:403:92\n    |\n401 |         }\n402 |     \n403 |     def _extract_entities_fallback(self, document_text: str, page_metadata: List[Dict[str, Any]]) -> Dict[str, Any]:\n    |                                                                                            ^^^\n404 |         \"\"\"Fallback entity extraction using simple patterns.\"\"\"\n405 |         entities_by_page = defaultdict(lambda: defaultdict(list))\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/K_knowledge_extraction/entity_concept_extractor.py:403:102\n    |\n401 |         }\n402 |     \n403 |     def _extract_entities_fallback(self, document_text: str, page_metadata: List[Dict[str, Any]]) -> Dict[str, Any]:\n    |                                                                                                      ^^^^\n404 |         \"\"\"Fallback entity extraction using simple patterns.\"\"\"\n405 |         entities_by_page = defaultdict(lambda: defaultdict(list))\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/K_knowledge_extraction/entity_concept_extractor.py:403:112\n    |\n401 |         }\n402 |     \n403 |     def _extract_entities_fallback(self, document_text: str, page_metadata: List[Dict[str, Any]]) -> Dict[str, Any]:\n    |                                                                                                                ^^^\n404 |         \"\"\"Fallback entity extraction using simple patterns.\"\"\"\n405 |         entities_by_page = defaultdict(lambda: defaultdict(list))\n    |\n\nF821 Undefined name `defaultdict`\n   --> canonical_flow/K_knowledge_extraction/entity_concept_extractor.py:405:28\n    |\n403 |     def _extract_entities_fallback(self, document_text: str, page_metadata: List[Dict[str, Any]]) -> Dict[str, Any]:\n404 |         \"\"\"Fallback entity extraction using simple patterns.\"\"\"\n405 |         entities_by_page = defaultdict(lambda: defaultdict(list))\n    |                            ^^^^^^^^^^^\n406 |         global_entity_counts = Counter()\n    |\n\nF821 Undefined name `defaultdict`\n   --> canonical_flow/K_knowledge_extraction/entity_concept_extractor.py:405:48\n    |\n403 |     def _extract_entities_fallback(self, document_text: str, page_metadata: List[Dict[str, Any]]) -> Dict[str, Any]:\n404 |         \"\"\"Fallback entity extraction using simple patterns.\"\"\"\n405 |         entities_by_page = defaultdict(lambda: defaultdict(list))\n    |                                                ^^^^^^^^^^^\n406 |         global_entity_counts = Counter()\n    |\n\nF821 Undefined name `Counter`\n   --> canonical_flow/K_knowledge_extraction/entity_concept_extractor.py:406:32\n    |\n404 |         \"\"\"Fallback entity extraction using simple patterns.\"\"\"\n405 |         entities_by_page = defaultdict(lambda: defaultdict(list))\n406 |         global_entity_counts = Counter()\n    |                                ^^^^^^^\n407 |         \n408 |         # Simple patterns for common entity types\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/K_knowledge_extraction/entity_concept_extractor.py:470:68\n    |\n468 |         }\n469 |     \n470 |     def _extract_concepts(self, document_text: str, page_metadata: List[Dict[str, Any]]) -> Dict[str, Any]:\n    |                                                                    ^^^^\n471 |         \"\"\"Extract conceptual knowledge using pattern matching.\"\"\"\n472 |         concepts_by_page = defaultdict(lambda: defaultdict(list))\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/K_knowledge_extraction/entity_concept_extractor.py:470:73\n    |\n468 |         }\n469 |     \n470 |     def _extract_concepts(self, document_text: str, page_metadata: List[Dict[str, Any]]) -> Dict[str, Any]:\n    |                                                                         ^^^^\n471 |         \"\"\"Extract conceptual knowledge using pattern matching.\"\"\"\n472 |         concepts_by_page = defaultdict(lambda: defaultdict(list))\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/K_knowledge_extraction/entity_concept_extractor.py:470:83\n    |\n468 |         }\n469 |     \n470 |     def _extract_concepts(self, document_text: str, page_metadata: List[Dict[str, Any]]) -> Dict[str, Any]:\n    |                                                                                   ^^^\n471 |         \"\"\"Extract conceptual knowledge using pattern matching.\"\"\"\n472 |         concepts_by_page = defaultdict(lambda: defaultdict(list))\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/K_knowledge_extraction/entity_concept_extractor.py:470:93\n    |\n468 |         }\n469 |     \n470 |     def _extract_concepts(self, document_text: str, page_metadata: List[Dict[str, Any]]) -> Dict[str, Any]:\n    |                                                                                             ^^^^\n471 |         \"\"\"Extract conceptual knowledge using pattern matching.\"\"\"\n472 |         concepts_by_page = defaultdict(lambda: defaultdict(list))\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/K_knowledge_extraction/entity_concept_extractor.py:470:103\n    |\n468 |         }\n469 |     \n470 |     def _extract_concepts(self, document_text: str, page_metadata: List[Dict[str, Any]]) -> Dict[str, Any]:\n    |                                                                                                       ^^^\n471 |         \"\"\"Extract conceptual knowledge using pattern matching.\"\"\"\n472 |         concepts_by_page = defaultdict(lambda: defaultdict(list))\n    |\n\nF821 Undefined name `defaultdict`\n   --> canonical_flow/K_knowledge_extraction/entity_concept_extractor.py:472:28\n    |\n470 |     def _extract_concepts(self, document_text: str, page_metadata: List[Dict[str, Any]]) -> Dict[str, Any]:\n471 |         \"\"\"Extract conceptual knowledge using pattern matching.\"\"\"\n472 |         concepts_by_page = defaultdict(lambda: defaultdict(list))\n    |                            ^^^^^^^^^^^\n473 |         global_concept_counts = Counter()\n    |\n\nF821 Undefined name `defaultdict`\n   --> canonical_flow/K_knowledge_extraction/entity_concept_extractor.py:472:48\n    |\n470 |     def _extract_concepts(self, document_text: str, page_metadata: List[Dict[str, Any]]) -> Dict[str, Any]:\n471 |         \"\"\"Extract conceptual knowledge using pattern matching.\"\"\"\n472 |         concepts_by_page = defaultdict(lambda: defaultdict(list))\n    |                                                ^^^^^^^^^^^\n473 |         global_concept_counts = Counter()\n    |\n\nF821 Undefined name `Counter`\n   --> canonical_flow/K_knowledge_extraction/entity_concept_extractor.py:473:33\n    |\n471 |         \"\"\"Extract conceptual knowledge using pattern matching.\"\"\"\n472 |         concepts_by_page = defaultdict(lambda: defaultdict(list))\n473 |         global_concept_counts = Counter()\n    |                                 ^^^^^^^\n474 |         \n475 |         for page_data in page_metadata:\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/K_knowledge_extraction/entity_concept_extractor.py:533:44\n    |\n531 |         }\n532 |     \n533 |     def _tokenize_text(self, text: str) -> Dict[str, List[int]]:\n    |                                            ^^^^\n534 |         \"\"\"Tokenize text and return tokens with their positions.\"\"\"\n535 |         tokens = defaultdict(list)\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/K_knowledge_extraction/entity_concept_extractor.py:533:54\n    |\n531 |         }\n532 |     \n533 |     def _tokenize_text(self, text: str) -> Dict[str, List[int]]:\n    |                                                      ^^^^\n534 |         \"\"\"Tokenize text and return tokens with their positions.\"\"\"\n535 |         tokens = defaultdict(list)\n    |\n\nF821 Undefined name `defaultdict`\n   --> canonical_flow/K_knowledge_extraction/entity_concept_extractor.py:535:18\n    |\n533 |     def _tokenize_text(self, text: str) -> Dict[str, List[int]]:\n534 |         \"\"\"Tokenize text and return tokens with their positions.\"\"\"\n535 |         tokens = defaultdict(list)\n    |                  ^^^^^^^^^^^\n536 |         \n537 |         if NLTK_AVAILABLE:\n    |\n\nF821 Undefined name `word_tokenize`\n   --> canonical_flow/K_knowledge_extraction/entity_concept_extractor.py:540:31\n    |\n538 |             # Use NLTK tokenization\n539 |             try:\n540 |                 word_tokens = word_tokenize(text, language='spanish' if self.language == 'es' else 'english')\n    |                               ^^^^^^^^^^^^^\n541 |                 \n542 |                 # Find positions of tokens in original text\n    |\n\nE722 Do not use bare `except`\n   --> canonical_flow/K_knowledge_extraction/entity_concept_extractor.py:554:13\n    |\n553 |                 return dict(tokens)\n554 |             except:\n    |             ^^^^^^\n555 |                 pass  # Fall back to regex\n    |\n\nE722 Do not use bare `except`\n   --> canonical_flow/K_knowledge_extraction/entity_concept_extractor.py:602:17\n    |\n600 |                     stemmed = self.stemmer.stem(word)\n601 |                     stemmed_words.append(stemmed)\n602 |                 except:\n    |                 ^^^^^^\n603 |                     stemmed_words.append(word)\n604 |             normalized = ' '.join(stemmed_words)\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/K_knowledge_extraction/entity_concept_extractor.py:614:74\n    |\n612 |         return text[context_start:context_end].strip()\n613 |     \n614 |     def _generate_document_stem(self, document_text: str, page_metadata: List[Dict[str, Any]]) -> str:\n    |                                                                          ^^^^\n615 |         \"\"\"Generate a deterministic stem for the document.\"\"\"\n616 |         # Use document characteristics to create stem\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/K_knowledge_extraction/entity_concept_extractor.py:614:79\n    |\n612 |         return text[context_start:context_end].strip()\n613 |     \n614 |     def _generate_document_stem(self, document_text: str, page_metadata: List[Dict[str, Any]]) -> str:\n    |                                                                               ^^^^\n615 |         \"\"\"Generate a deterministic stem for the document.\"\"\"\n616 |         # Use document characteristics to create stem\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/K_knowledge_extraction/entity_concept_extractor.py:614:89\n    |\n612 |         return text[context_start:context_end].strip()\n613 |     \n614 |     def _generate_document_stem(self, document_text: str, page_metadata: List[Dict[str, Any]]) -> str:\n    |                                                                                         ^^^\n615 |         \"\"\"Generate a deterministic stem for the document.\"\"\"\n616 |         # Use document characteristics to create stem\n    |\n\nF821 Undefined name `Path`\n   --> canonical_flow/K_knowledge_extraction/entity_concept_extractor.py:626:49\n    |\n624 |         return self.generate_stable_id(stem_data, prefix=\"doc\")[:16]\n625 |     \n626 |     def _write_json_artifacts(self, output_dir: Path, terms_result: Dict, \n    |                                                 ^^^^\n627 |                             entities_result: Dict, concepts_result: Dict):\n628 |         \"\"\"Write JSON artifacts to the output directory.\"\"\"\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/K_knowledge_extraction/entity_concept_extractor.py:626:69\n    |\n624 |         return self.generate_stable_id(stem_data, prefix=\"doc\")[:16]\n625 |     \n626 |     def _write_json_artifacts(self, output_dir: Path, terms_result: Dict, \n    |                                                                     ^^^^\n627 |                             entities_result: Dict, concepts_result: Dict):\n628 |         \"\"\"Write JSON artifacts to the output directory.\"\"\"\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/K_knowledge_extraction/entity_concept_extractor.py:627:46\n    |\n626 |     def _write_json_artifacts(self, output_dir: Path, terms_result: Dict, \n627 |                             entities_result: Dict, concepts_result: Dict):\n    |                                              ^^^^\n628 |         \"\"\"Write JSON artifacts to the output directory.\"\"\"\n629 |         # Create output directory\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/K_knowledge_extraction/entity_concept_extractor.py:627:69\n    |\n626 |     def _write_json_artifacts(self, output_dir: Path, terms_result: Dict, \n627 |                             entities_result: Dict, concepts_result: Dict):\n    |                                                                     ^^^^\n628 |         \"\"\"Write JSON artifacts to the output directory.\"\"\"\n629 |         # Create output directory\n    |\n\nF821 Undefined name `Enum`\n  --> canonical_flow/K_knowledge_extraction/gate_validator.py:17:23\n   |\n17 | class ComponentStatus(Enum):\n   |                       ^^^^\n18 |     \"\"\"Component execution status.\"\"\"\n19 |     SUCCESS = \"success\"\n   |\n\nF821 Undefined name `dataclass`\n  --> canonical_flow/K_knowledge_extraction/gate_validator.py:26:2\n   |\n26 | @dataclass\n   |  ^^^^^^^^^\n27 | class ValidationResult:\n28 |     \"\"\"Result of artifact validation.\"\"\"\n   |\n\nF821 Undefined name `List`\n  --> canonical_flow/K_knowledge_extraction/gate_validator.py:30:13\n   |\n28 |     \"\"\"Result of artifact validation.\"\"\"\n29 |     is_valid: bool\n30 |     errors: List[str]\n   |             ^^^^\n31 |     warnings: List[str]\n32 |     artifact_path: Optional[Path] = None\n   |\n\nF821 Undefined name `List`\n  --> canonical_flow/K_knowledge_extraction/gate_validator.py:31:15\n   |\n29 |     is_valid: bool\n30 |     errors: List[str]\n31 |     warnings: List[str]\n   |               ^^^^\n32 |     artifact_path: Optional[Path] = None\n   |\n\nF821 Undefined name `Optional`\n  --> canonical_flow/K_knowledge_extraction/gate_validator.py:32:20\n   |\n30 |     errors: List[str]\n31 |     warnings: List[str]\n32 |     artifact_path: Optional[Path] = None\n   |                    ^^^^^^^^\n33 |     \n34 |     def add_error(self, error: str):\n   |\n\nF821 Undefined name `Path`\n  --> canonical_flow/K_knowledge_extraction/gate_validator.py:32:29\n   |\n30 |     errors: List[str]\n31 |     warnings: List[str]\n32 |     artifact_path: Optional[Path] = None\n   |                             ^^^^\n33 |     \n34 |     def add_error(self, error: str):\n   |\n\nF821 Undefined name `dataclass`\n  --> canonical_flow/K_knowledge_extraction/gate_validator.py:44:2\n   |\n44 | @dataclass \n   |  ^^^^^^^^^\n45 | class ComponentDependency:\n46 |     \"\"\"Defines a component's dependencies.\"\"\"\n   |\n\nF821 Undefined name `List`\n  --> canonical_flow/K_knowledge_extraction/gate_validator.py:48:25\n   |\n46 |     \"\"\"Defines a component's dependencies.\"\"\"\n47 |     component_id: str\n48 |     required_artifacts: List[str]  # List of JSON artifact filenames\n   |                         ^^^^\n49 |     schema_validators: Dict[str, callable]  # Artifact name -> validator function\n   |\n\nF821 Undefined name `Dict`\n  --> canonical_flow/K_knowledge_extraction/gate_validator.py:49:24\n   |\n47 |     component_id: str\n48 |     required_artifacts: List[str]  # List of JSON artifact filenames\n49 |     schema_validators: Dict[str, callable]  # Artifact name -> validator function\n   |                        ^^^^\n   |\n\nF821 Undefined name `Path`\n  --> canonical_flow/K_knowledge_extraction/gate_validator.py:60:45\n   |\n58 |     \"\"\"\n59 |     \n60 |     def __init__(self, canonical_flow_path: Path):\n   |                                             ^^^^\n61 |         \"\"\"Initialize GateValidator.\n   |\n\nF821 Undefined name `Path`\n  --> canonical_flow/K_knowledge_extraction/gate_validator.py:66:36\n   |\n64 |             canonical_flow_path: Path to canonical_flow directory\n65 |         \"\"\"\n66 |         self.canonical_flow_path = Path(canonical_flow_path)\n   |                                    ^^^^\n67 |         self.knowledge_artifacts_path = self.canonical_flow_path / \"knowledge\"\n68 |         self.logger = logging.getLogger(__name__)\n   |\n\nF821 Undefined name `Path`\n   --> canonical_flow/K_knowledge_extraction/gate_validator.py:241:75\n    |\n239 |         return result\n240 |     \n241 |     def _get_artifact_path(self, document_id: str, artifact_name: str) -> Path:\n    |                                                                           ^^^^\n242 |         \"\"\"Get path to artifact for given document.\"\"\"\n243 |         return self.knowledge_artifacts_path / document_id / artifact_name\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/K_knowledge_extraction/gate_validator.py:282:44\n    |\n281 |     # Schema validation methods\n282 |     def _validate_terms_schema(self, data: Dict[str, Any], artifact_path: Path) -> ValidationResult:\n    |                                            ^^^^\n283 |         \"\"\"Validate terms.json schema.\"\"\"\n284 |         result = ValidationResult(is_valid=True, errors=[], warnings=[], artifact_path=artifact_path)\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/K_knowledge_extraction/gate_validator.py:282:54\n    |\n281 |     # Schema validation methods\n282 |     def _validate_terms_schema(self, data: Dict[str, Any], artifact_path: Path) -> ValidationResult:\n    |                                                      ^^^\n283 |         \"\"\"Validate terms.json schema.\"\"\"\n284 |         result = ValidationResult(is_valid=True, errors=[], warnings=[], artifact_path=artifact_path)\n    |\n\nF821 Undefined name `Path`\n   --> canonical_flow/K_knowledge_extraction/gate_validator.py:282:75\n    |\n281 |     # Schema validation methods\n282 |     def _validate_terms_schema(self, data: Dict[str, Any], artifact_path: Path) -> ValidationResult:\n    |                                                                           ^^^^\n283 |         \"\"\"Validate terms.json schema.\"\"\"\n284 |         result = ValidationResult(is_valid=True, errors=[], warnings=[], artifact_path=artifact_path)\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/K_knowledge_extraction/gate_validator.py:303:45\n    |\n301 |         return result\n302 |     \n303 |     def _validate_chunks_schema(self, data: Dict[str, Any], artifact_path: Path) -> ValidationResult:\n    |                                             ^^^^\n304 |         \"\"\"Validate chunks.json schema.\"\"\"\n305 |         result = ValidationResult(is_valid=True, errors=[], warnings=[], artifact_path=artifact_path)\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/K_knowledge_extraction/gate_validator.py:303:55\n    |\n301 |         return result\n302 |     \n303 |     def _validate_chunks_schema(self, data: Dict[str, Any], artifact_path: Path) -> ValidationResult:\n    |                                                       ^^^\n304 |         \"\"\"Validate chunks.json schema.\"\"\"\n305 |         result = ValidationResult(is_valid=True, errors=[], warnings=[], artifact_path=artifact_path)\n    |\n\nF821 Undefined name `Path`\n   --> canonical_flow/K_knowledge_extraction/gate_validator.py:303:76\n    |\n301 |         return result\n302 |     \n303 |     def _validate_chunks_schema(self, data: Dict[str, Any], artifact_path: Path) -> ValidationResult:\n    |                                                                            ^^^^\n304 |         \"\"\"Validate chunks.json schema.\"\"\"\n305 |         result = ValidationResult(is_valid=True, errors=[], warnings=[], artifact_path=artifact_path)\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/K_knowledge_extraction/gate_validator.py:324:47\n    |\n322 |         return result\n323 |     \n324 |     def _validate_concepts_schema(self, data: Dict[str, Any], artifact_path: Path) -> ValidationResult:\n    |                                               ^^^^\n325 |         \"\"\"Validate concepts.json schema.\"\"\"\n326 |         result = ValidationResult(is_valid=True, errors=[], warnings=[], artifact_path=artifact_path)\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/K_knowledge_extraction/gate_validator.py:324:57\n    |\n322 |         return result\n323 |     \n324 |     def _validate_concepts_schema(self, data: Dict[str, Any], artifact_path: Path) -> ValidationResult:\n    |                                                         ^^^\n325 |         \"\"\"Validate concepts.json schema.\"\"\"\n326 |         result = ValidationResult(is_valid=True, errors=[], warnings=[], artifact_path=artifact_path)\n    |\n\nF821 Undefined name `Path`\n   --> canonical_flow/K_knowledge_extraction/gate_validator.py:324:78\n    |\n322 |         return result\n323 |     \n324 |     def _validate_concepts_schema(self, data: Dict[str, Any], artifact_path: Path) -> ValidationResult:\n    |                                                                              ^^^^\n325 |         \"\"\"Validate concepts.json schema.\"\"\"\n326 |         result = ValidationResult(is_valid=True, errors=[], warnings=[], artifact_path=artifact_path)\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/K_knowledge_extraction/gate_validator.py:345:47\n    |\n343 |         return result\n344 |     \n345 |     def _validate_entities_schema(self, data: Dict[str, Any], artifact_path: Path) -> ValidationResult:\n    |                                               ^^^^\n346 |         \"\"\"Validate entities.json schema.\"\"\"\n347 |         result = ValidationResult(is_valid=True, errors=[], warnings=[], artifact_path=artifact_path)\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/K_knowledge_extraction/gate_validator.py:345:57\n    |\n343 |         return result\n344 |     \n345 |     def _validate_entities_schema(self, data: Dict[str, Any], artifact_path: Path) -> ValidationResult:\n    |                                                         ^^^\n346 |         \"\"\"Validate entities.json schema.\"\"\"\n347 |         result = ValidationResult(is_valid=True, errors=[], warnings=[], artifact_path=artifact_path)\n    |\n\nF821 Undefined name `Path`\n   --> canonical_flow/K_knowledge_extraction/gate_validator.py:345:78\n    |\n343 |         return result\n344 |     \n345 |     def _validate_entities_schema(self, data: Dict[str, Any], artifact_path: Path) -> ValidationResult:\n    |                                                                              ^^^^\n346 |         \"\"\"Validate entities.json schema.\"\"\"\n347 |         result = ValidationResult(is_valid=True, errors=[], warnings=[], artifact_path=artifact_path)\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/K_knowledge_extraction/gate_validator.py:366:48\n    |\n364 |         return result\n365 |     \n366 |     def _validate_relations_schema(self, data: Dict[str, Any], artifact_path: Path) -> ValidationResult:\n    |                                                ^^^^\n367 |         \"\"\"Validate relations.json schema.\"\"\"\n368 |         result = ValidationResult(is_valid=True, errors=[], warnings=[], artifact_path=artifact_path)\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/K_knowledge_extraction/gate_validator.py:366:58\n    |\n364 |         return result\n365 |     \n366 |     def _validate_relations_schema(self, data: Dict[str, Any], artifact_path: Path) -> ValidationResult:\n    |                                                          ^^^\n367 |         \"\"\"Validate relations.json schema.\"\"\"\n368 |         result = ValidationResult(is_valid=True, errors=[], warnings=[], artifact_path=artifact_path)\n    |\n\nF821 Undefined name `Path`\n   --> canonical_flow/K_knowledge_extraction/gate_validator.py:366:79\n    |\n364 |         return result\n365 |     \n366 |     def _validate_relations_schema(self, data: Dict[str, Any], artifact_path: Path) -> ValidationResult:\n    |                                                                               ^^^^\n367 |         \"\"\"Validate relations.json schema.\"\"\"\n368 |         result = ValidationResult(is_valid=True, errors=[], warnings=[], artifact_path=artifact_path)\n    |\n\nF821 Undefined name `Path`\n   --> canonical_flow/K_knowledge_extraction/gate_validator.py:395:45\n    |\n393 |     \"\"\"\n394 |     \n395 |     def __init__(self, canonical_flow_path: Path):\n    |                                             ^^^^\n396 |         \"\"\"Initialize gate with validator.\"\"\"\n397 |         self.validator = GateValidator(canonical_flow_path)\n    |\n\nF821 Undefined name `Tuple`\n   --> canonical_flow/K_knowledge_extraction/gate_validator.py:400:67\n    |\n398 |         self.logger = logging.getLogger(__name__)\n399 |     \n400 |     def can_execute(self, component_id: str, document_id: str) -> Tuple[bool, List[str]]:\n    |                                                                   ^^^^^\n401 |         \"\"\"\n402 |         Check if component can execute.\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/K_knowledge_extraction/gate_validator.py:400:79\n    |\n398 |         self.logger = logging.getLogger(__name__)\n399 |     \n400 |     def can_execute(self, component_id: str, document_id: str) -> Tuple[bool, List[str]]:\n    |                                                                               ^^^^\n401 |         \"\"\"\n402 |         Check if component can execute.\n    |\n\ninvalid-syntax: Unexpected indentation\n  --> canonical_flow/K_knowledge_extraction/test_causal_graph_constructor.py:12:1\n   |\n11 | # # # from causal_graph_constructor import (  # Module not found  # Module not found  # Module not found\n12 |     CausalGraphConstructor, \n   | ^^^^\n13 |     DimensionType,\n14 |     EvidenceReference,\n   |\n\ninvalid-syntax: Expected a statement\n  --> canonical_flow/K_knowledge_extraction/test_causal_graph_constructor.py:16:1\n   |\n14 |     EvidenceReference,\n15 |     CausalRelationship\n16 | )\n   | ^\n   |\n\ninvalid-syntax: Expected a statement\n  --> canonical_flow/K_knowledge_extraction/test_causal_graph_constructor.py:16:2\n   |\n14 |     EvidenceReference,\n15 |     CausalRelationship\n16 | )\n   |  ^\n   |\n\ninvalid-syntax: Unexpected indentation\n --> canonical_flow/L_classification_evaluation/__init__.py:4:1\n  |\n3 | # # # from .decalogo_scoring_system import (  # Module not found  # Module not found  # Module not found\n4 |     ScoringSystem,\n  | ^^^^\n5 |     QuestionResponse,\n6 |     DimensionScore,\n  |\n\ninvalid-syntax: Expected a statement\n  --> canonical_flow/L_classification_evaluation/__init__.py:8:1\n   |\n 6 |     DimensionScore,\n 7 |     PointScore\n 8 | )\n   | ^\n 9 |\n10 | # # # from .schemas import (  # Module not found  # Module not found  # Module not found\n   |\n\ninvalid-syntax: Expected a statement\n  --> canonical_flow/L_classification_evaluation/__init__.py:8:2\n   |\n 6 |     DimensionScore,\n 7 |     PointScore\n 8 | )\n   |  ^\n 9 |\n10 | # # # from .schemas import (  # Module not found  # Module not found  # Module not found\n   |\n\ninvalid-syntax: Unexpected indentation\n  --> canonical_flow/L_classification_evaluation/__init__.py:11:1\n   |\n10 | # # # from .schemas import (  # Module not found  # Module not found  # Module not found\n11 |     QuestionEvalInput,\n   | ^^^^\n12 |     DimensionEvalOutput, \n13 |     PointEvalOutput,\n   |\n\ninvalid-syntax: Expected a statement\n  --> canonical_flow/L_classification_evaluation/__init__.py:23:1\n   |\n21 |     reject_unknown_responses,\n22 |     DeterministicSortingMixin\n23 | )\n   | ^\n   |\n\ninvalid-syntax: Expected a statement\n  --> canonical_flow/L_classification_evaluation/__init__.py:23:2\n   |\n21 |     reject_unknown_responses,\n22 |     DeterministicSortingMixin\n23 | )\n   |  ^\n   |\n\nF821 Undefined name `Path`\n  --> canonical_flow/L_classification_evaluation/adaptive_scoring_engine.py:24:20\n   |\n22 | try:\n23 |     # Add project root to path for imports\n24 |     project_root = Path(__file__).resolve().parents[1]\n   |                    ^^^^\n25 |     if str(project_root) not in sys.path:\n26 |         sys.path.insert(0, str(project_root))\n   |\n\nF821 Undefined name `importlib_util`\n  --> canonical_flow/L_classification_evaluation/adaptive_scoring_engine.py:31:16\n   |\n29 |     original_file = project_root / \"adaptive_scoring_engine.py\"\n30 |     if original_file.exists():\n31 |         spec = importlib_util.spec_from_file_location(\n   |                ^^^^^^^^^^^^^^\n32 |             f\"original_{alias_code.lower()}\", \n33 |             str(original_file)\n   |\n\nF821 Undefined name `importlib_util`\n  --> canonical_flow/L_classification_evaluation/adaptive_scoring_engine.py:37:31\n   |\n36 |         if spec and spec.loader:\n37 |             original_module = importlib_util.module_from_spec(spec)\n   |                               ^^^^^^^^^^^^^^\n38 |             spec.loader.exec_module(original_module)\n   |\n\nF821 Undefined name `e`\n  --> canonical_flow/L_classification_evaluation/adaptive_scoring_engine.py:56:67\n   |\n54 |     def process(data=None, context=None):\n55 |         \"\"\"Placeholder process function for failed import.\"\"\"\n56 |         return {\"error\": f\"Module {alias_source} failed to load: {e}\"}\n   |                                                                   ^\n   |\n\nF401 [*] `collections` imported but unused\n  --> canonical_flow/L_classification_evaluation/conformal_prediction.py:18:8\n   |\n16 | import numpy as np\n17 | # # # from scipy import stats  # Module not found  # Module not found  # Module not found\n18 | import collections\n   |        ^^^^^^^^^^^\n19 |\n20 | # # # from .decalogo_scoring_system import ScoringSystem, PointScore, DimensionScore  # Module not found  # Module not found  # Module\u2026\n   |\nhelp: Remove unused import: `collections`\n\nF821 Undefined name `dataclass`\n  --> canonical_flow/L_classification_evaluation/conformal_prediction.py:25:2\n   |\n25 | @dataclass\n   |  ^^^^^^^^^\n26 | class ConformalInterval:\n27 |     \"\"\"Conformal prediction interval with bounds and metadata\"\"\"\n   |\n\nF821 Undefined name `dataclass`\n  --> canonical_flow/L_classification_evaluation/conformal_prediction.py:36:2\n   |\n36 | @dataclass\n   |  ^^^^^^^^^\n37 | class CalibrationMetrics:\n38 |     \"\"\"Calibration quality metrics for conformal predictions\"\"\"\n   |\n\nF821 Undefined name `dataclass`\n  --> canonical_flow/L_classification_evaluation/conformal_prediction.py:49:2\n   |\n49 | @dataclass\n   |  ^^^^^^^^^\n50 | class ConformalPrediction:\n51 |     \"\"\"Complete conformal prediction result for a dimension or point\"\"\"\n   |\n\nF821 Undefined name `Dict`\n  --> canonical_flow/L_classification_evaluation/conformal_prediction.py:54:27\n   |\n52 |     prediction_id: str\n53 |     predicted_score: float\n54 |     confidence_intervals: Dict[str, ConformalInterval]  # \"90%\", \"95%\", \"99%\"\n   |                           ^^^^\n55 |     calibration_metrics: CalibrationMetrics\n56 |     bootstrap_estimates: Dict[str, float]  # variance estimates\n   |\n\nF821 Undefined name `Dict`\n  --> canonical_flow/L_classification_evaluation/conformal_prediction.py:56:26\n   |\n54 |     confidence_intervals: Dict[str, ConformalInterval]  # \"90%\", \"95%\", \"99%\"\n55 |     calibration_metrics: CalibrationMetrics\n56 |     bootstrap_estimates: Dict[str, float]  # variance estimates\n   |                          ^^^^\n   |\n\nF821 Undefined name `dataclass`\n  --> canonical_flow/L_classification_evaluation/conformal_prediction.py:59:2\n   |\n59 | @dataclass\n   |  ^^^^^^^^^\n60 | class DocumentConformalResults:\n61 |     \"\"\"Complete conformal prediction results for a document\"\"\"\n   |\n\nF821 Undefined name `Dict`\n  --> canonical_flow/L_classification_evaluation/conformal_prediction.py:63:24\n   |\n61 |     \"\"\"Complete conformal prediction results for a document\"\"\"\n62 |     document_id: str\n63 |     point_predictions: Dict[str, ConformalPrediction]  # \"P1\", \"P2\", etc.\n   |                        ^^^^\n64 |     dimension_predictions: Dict[str, ConformalPrediction]  # \"P1_DE-1\", etc.\n65 |     aggregated_predictions: Dict[str, ConformalPrediction]  # \"overall_point_score\"\n   |\n\nF821 Undefined name `Dict`\n  --> canonical_flow/L_classification_evaluation/conformal_prediction.py:64:28\n   |\n62 |     document_id: str\n63 |     point_predictions: Dict[str, ConformalPrediction]  # \"P1\", \"P2\", etc.\n64 |     dimension_predictions: Dict[str, ConformalPrediction]  # \"P1_DE-1\", etc.\n   |                            ^^^^\n65 |     aggregated_predictions: Dict[str, ConformalPrediction]  # \"overall_point_score\"\n66 |     generation_timestamp: str\n   |\n\nF821 Undefined name `Dict`\n  --> canonical_flow/L_classification_evaluation/conformal_prediction.py:65:29\n   |\n63 |     point_predictions: Dict[str, ConformalPrediction]  # \"P1\", \"P2\", etc.\n64 |     dimension_predictions: Dict[str, ConformalPrediction]  # \"P1_DE-1\", etc.\n65 |     aggregated_predictions: Dict[str, ConformalPrediction]  # \"overall_point_score\"\n   |                             ^^^^\n66 |     generation_timestamp: str\n67 |     calibration_dataset_size: int\n   |\n\nF821 Undefined name `List`\n  --> canonical_flow/L_classification_evaluation/conformal_prediction.py:68:24\n   |\n66 |     generation_timestamp: str\n67 |     calibration_dataset_size: int\n68 |     confidence_levels: List[float]\n   |                        ^^^^\n   |\n\nF821 Undefined name `Optional`\n  --> canonical_flow/L_classification_evaluation/conformal_prediction.py:83:25\n   |\n81 |     def __init__(\n82 |         self,\n83 |         scoring_system: Optional[ScoringSystem] = None,\n   |                         ^^^^^^^^\n84 |         calibration_split: float = DEFAULT_CALIBRATION_SPLIT,\n85 |         bootstrap_samples: int = BOOTSTRAP_SAMPLES,\n   |\n\nF821 Undefined name `ScoringSystem`\n  --> canonical_flow/L_classification_evaluation/conformal_prediction.py:83:34\n   |\n81 |     def __init__(\n82 |         self,\n83 |         scoring_system: Optional[ScoringSystem] = None,\n   |                                  ^^^^^^^^^^^^^\n84 |         calibration_split: float = DEFAULT_CALIBRATION_SPLIT,\n85 |         bootstrap_samples: int = BOOTSTRAP_SAMPLES,\n   |\n\nF821 Undefined name `Optional`\n  --> canonical_flow/L_classification_evaluation/conformal_prediction.py:86:28\n   |\n84 |         calibration_split: float = DEFAULT_CALIBRATION_SPLIT,\n85 |         bootstrap_samples: int = BOOTSTRAP_SAMPLES,\n86 |         confidence_levels: Optional[List[float]] = None,\n   |                            ^^^^^^^^\n87 |         random_seed: int = 42\n88 |     ):\n   |\n\nF821 Undefined name `List`\n  --> canonical_flow/L_classification_evaluation/conformal_prediction.py:86:37\n   |\n84 |         calibration_split: float = DEFAULT_CALIBRATION_SPLIT,\n85 |         bootstrap_samples: int = BOOTSTRAP_SAMPLES,\n86 |         confidence_levels: Optional[List[float]] = None,\n   |                                     ^^^^\n87 |         random_seed: int = 42\n88 |     ):\n   |\n\nF821 Undefined name `ScoringSystem`\n   --> canonical_flow/L_classification_evaluation/conformal_prediction.py:99:49\n    |\n 97 |             random_seed: Random seed for reproducibility\n 98 |         \"\"\"\n 99 |         self.scoring_system = scoring_system or ScoringSystem()\n    |                                                 ^^^^^^^^^^^^^\n100 |         self.calibration_split = calibration_split\n101 |         self.bootstrap_samples = bootstrap_samples\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/L_classification_evaluation/conformal_prediction.py:137:29\n    |\n135 |     def fit_calibration_data(\n136 |         self,\n137 |         calibration_scores: List[Tuple[float, float]]\n    |                             ^^^^\n138 |     ) -> List[float]:\n139 |         \"\"\"\n    |\n\nF821 Undefined name `Tuple`\n   --> canonical_flow/L_classification_evaluation/conformal_prediction.py:137:34\n    |\n135 |     def fit_calibration_data(\n136 |         self,\n137 |         calibration_scores: List[Tuple[float, float]]\n    |                                  ^^^^^\n138 |     ) -> List[float]:\n139 |         \"\"\"\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/L_classification_evaluation/conformal_prediction.py:138:10\n    |\n136 |         self,\n137 |         calibration_scores: List[Tuple[float, float]]\n138 |     ) -> List[float]:\n    |          ^^^^\n139 |         \"\"\"\n140 |         Fit calibration data and compute nonconformity scores.\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/L_classification_evaluation/conformal_prediction.py:158:31\n    |\n156 |     def compute_quantile(\n157 |         self,\n158 |         nonconformity_scores: List[float],\n    |                               ^^^^\n159 |         confidence_level: float\n160 |     ) -> float:\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/L_classification_evaluation/conformal_prediction.py:185:17\n    |\n183 |     def bootstrap_variance_estimation(\n184 |         self,\n185 |         scores: List[float],\n    |                 ^^^^\n186 |         n_bootstrap: Optional[int] = None\n187 |     ) -> Dict[str, float]:\n    |\n\nF821 Undefined name `Optional`\n   --> canonical_flow/L_classification_evaluation/conformal_prediction.py:186:22\n    |\n184 |         self,\n185 |         scores: List[float],\n186 |         n_bootstrap: Optional[int] = None\n    |                      ^^^^^^^^\n187 |     ) -> Dict[str, float]:\n188 |         \"\"\"\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/L_classification_evaluation/conformal_prediction.py:187:10\n    |\n185 |         scores: List[float],\n186 |         n_bootstrap: Optional[int] = None\n187 |     ) -> Dict[str, float]:\n    |          ^^^^\n188 |         \"\"\"\n189 |         Bootstrap variance estimation for score uncertainty.\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/L_classification_evaluation/conformal_prediction.py:228:31\n    |\n226 |         self,\n227 |         predicted_score: float,\n228 |         nonconformity_scores: List[float],\n    |                               ^^^^\n229 |         confidence_levels: Optional[List[float]] = None\n230 |     ) -> Dict[str, ConformalInterval]:\n    |\n\nF821 Undefined name `Optional`\n   --> canonical_flow/L_classification_evaluation/conformal_prediction.py:229:28\n    |\n227 |         predicted_score: float,\n228 |         nonconformity_scores: List[float],\n229 |         confidence_levels: Optional[List[float]] = None\n    |                            ^^^^^^^^\n230 |     ) -> Dict[str, ConformalInterval]:\n231 |         \"\"\"\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/L_classification_evaluation/conformal_prediction.py:229:37\n    |\n227 |         predicted_score: float,\n228 |         nonconformity_scores: List[float],\n229 |         confidence_levels: Optional[List[float]] = None\n    |                                     ^^^^\n230 |     ) -> Dict[str, ConformalInterval]:\n231 |         \"\"\"\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/L_classification_evaluation/conformal_prediction.py:230:10\n    |\n228 |         nonconformity_scores: List[float],\n229 |         confidence_levels: Optional[List[float]] = None\n230 |     ) -> Dict[str, ConformalInterval]:\n    |          ^^^^\n231 |         \"\"\"\n232 |         Generate conformal prediction intervals at multiple confidence levels.\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/L_classification_evaluation/conformal_prediction.py:266:27\n    |\n264 |     def evaluate_calibration_quality(\n265 |         self,\n266 |         test_predictions: List[Tuple[float, float, Dict[str, ConformalInterval]]],\n    |                           ^^^^\n267 |         confidence_levels: Optional[List[float]] = None\n268 |     ) -> CalibrationMetrics:\n    |\n\nF821 Undefined name `Tuple`\n   --> canonical_flow/L_classification_evaluation/conformal_prediction.py:266:32\n    |\n264 |     def evaluate_calibration_quality(\n265 |         self,\n266 |         test_predictions: List[Tuple[float, float, Dict[str, ConformalInterval]]],\n    |                                ^^^^^\n267 |         confidence_levels: Optional[List[float]] = None\n268 |     ) -> CalibrationMetrics:\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/L_classification_evaluation/conformal_prediction.py:266:52\n    |\n264 |     def evaluate_calibration_quality(\n265 |         self,\n266 |         test_predictions: List[Tuple[float, float, Dict[str, ConformalInterval]]],\n    |                                                    ^^^^\n267 |         confidence_levels: Optional[List[float]] = None\n268 |     ) -> CalibrationMetrics:\n    |\n\nF821 Undefined name `Optional`\n   --> canonical_flow/L_classification_evaluation/conformal_prediction.py:267:28\n    |\n265 |         self,\n266 |         test_predictions: List[Tuple[float, float, Dict[str, ConformalInterval]]],\n267 |         confidence_levels: Optional[List[float]] = None\n    |                            ^^^^^^^^\n268 |     ) -> CalibrationMetrics:\n269 |         \"\"\"\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/L_classification_evaluation/conformal_prediction.py:267:37\n    |\n265 |         self,\n266 |         test_predictions: List[Tuple[float, float, Dict[str, ConformalInterval]]],\n267 |         confidence_levels: Optional[List[float]] = None\n    |                                     ^^^^\n268 |     ) -> CalibrationMetrics:\n269 |         \"\"\"\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/L_classification_evaluation/conformal_prediction.py:327:27\n    |\n325 |         self,\n326 |         predicted_score: float,\n327 |         calibration_data: List[Tuple[float, float]],\n    |                           ^^^^\n328 |         prediction_id: str\n329 |     ) -> ConformalPrediction:\n    |\n\nF821 Undefined name `Tuple`\n   --> canonical_flow/L_classification_evaluation/conformal_prediction.py:327:32\n    |\n325 |         self,\n326 |         predicted_score: float,\n327 |         calibration_data: List[Tuple[float, float]],\n    |                                ^^^^^\n328 |         prediction_id: str\n329 |     ) -> ConformalPrediction:\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/L_classification_evaluation/conformal_prediction.py:370:23\n    |\n368 |         self,\n369 |         document_id: str,\n370 |         point_scores: Dict[str, float],  # \"P1\": score, \"P2\": score, etc.\n    |                       ^^^^\n371 |         dimension_scores: Dict[str, float],  # \"P1_DE-1\": score, etc.\n372 |         calibration_dataset: Dict[str, List[Tuple[float, float]]],\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/L_classification_evaluation/conformal_prediction.py:371:27\n    |\n369 |         document_id: str,\n370 |         point_scores: Dict[str, float],  # \"P1\": score, \"P2\": score, etc.\n371 |         dimension_scores: Dict[str, float],  # \"P1_DE-1\": score, etc.\n    |                           ^^^^\n372 |         calibration_dataset: Dict[str, List[Tuple[float, float]]],\n373 |         output_dir: str = \"canonical_flow/classification\"\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/L_classification_evaluation/conformal_prediction.py:372:30\n    |\n370 |         point_scores: Dict[str, float],  # \"P1\": score, \"P2\": score, etc.\n371 |         dimension_scores: Dict[str, float],  # \"P1_DE-1\": score, etc.\n372 |         calibration_dataset: Dict[str, List[Tuple[float, float]]],\n    |                              ^^^^\n373 |         output_dir: str = \"canonical_flow/classification\"\n374 |     ) -> DocumentConformalResults:\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/L_classification_evaluation/conformal_prediction.py:372:40\n    |\n370 |         point_scores: Dict[str, float],  # \"P1\": score, \"P2\": score, etc.\n371 |         dimension_scores: Dict[str, float],  # \"P1_DE-1\": score, etc.\n372 |         calibration_dataset: Dict[str, List[Tuple[float, float]]],\n    |                                        ^^^^\n373 |         output_dir: str = \"canonical_flow/classification\"\n374 |     ) -> DocumentConformalResults:\n    |\n\nF821 Undefined name `Tuple`\n   --> canonical_flow/L_classification_evaluation/conformal_prediction.py:372:45\n    |\n370 |         point_scores: Dict[str, float],  # \"P1\": score, \"P2\": score, etc.\n371 |         dimension_scores: Dict[str, float],  # \"P1_DE-1\": score, etc.\n372 |         calibration_dataset: Dict[str, List[Tuple[float, float]]],\n    |                                             ^^^^^\n373 |         output_dir: str = \"canonical_flow/classification\"\n374 |     ) -> DocumentConformalResults:\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/L_classification_evaluation/conformal_prediction.py:496:39\n    |\n494 |         return doc_dir\n495 |     \n496 |     def _make_serializable(self, obj: Any) -> Any:\n    |                                       ^^^\n497 |         \"\"\"Convert dataclass objects to JSON-serializable dictionaries.\"\"\"\n498 |         if isinstance(obj, (DocumentConformalResults, ConformalPrediction, \n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/L_classification_evaluation/conformal_prediction.py:496:47\n    |\n494 |         return doc_dir\n495 |     \n496 |     def _make_serializable(self, obj: Any) -> Any:\n    |                                               ^^^\n497 |         \"\"\"Convert dataclass objects to JSON-serializable dictionaries.\"\"\"\n498 |         if isinstance(obj, (DocumentConformalResults, ConformalPrediction, \n    |\n\nF821 Undefined name `asdict`\n   --> canonical_flow/L_classification_evaluation/conformal_prediction.py:500:20\n    |\n498 |         if isinstance(obj, (DocumentConformalResults, ConformalPrediction, \n499 |                            ConformalInterval, CalibrationMetrics)):\n500 |             return asdict(obj)\n    |                    ^^^^^^\n501 |         elif isinstance(obj, dict):\n502 |             return {k: self._make_serializable(v) for k, v in obj.items()}\n    |\n\nF821 Undefined name `datetime`\n   --> canonical_flow/L_classification_evaluation/conformal_prediction.py:515:16\n    |\n513 |         \"\"\"Get current timestamp in ISO format.\"\"\"\n514 | # # #         from datetime import datetime  # Module not found  # Module not found  # Module not found\n515 |         return datetime.utcnow().isoformat() + \"Z\"\n    |                ^^^^^^^^\n    |\n\nF821 Undefined name `Tuple`\n   --> canonical_flow/L_classification_evaluation/conformal_prediction.py:522:18\n    |\n520 | def generate_sample_calibration_data(\n521 |     n_samples: int = 100,\n522 |     score_range: Tuple[float, float] = (0.0, 1.0),\n    |                  ^^^^^\n523 |     noise_level: float = 0.1,\n524 |     random_seed: int = 42\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/L_classification_evaluation/conformal_prediction.py:525:6\n    |\n523 |     noise_level: float = 0.1,\n524 |     random_seed: int = 42\n525 | ) -> Dict[str, List[Tuple[float, float]]]:\n    |      ^^^^\n526 |     \"\"\"\n527 |     Generate sample calibration data for testing conformal predictions.\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/L_classification_evaluation/conformal_prediction.py:525:16\n    |\n523 |     noise_level: float = 0.1,\n524 |     random_seed: int = 42\n525 | ) -> Dict[str, List[Tuple[float, float]]]:\n    |                ^^^^\n526 |     \"\"\"\n527 |     Generate sample calibration data for testing conformal predictions.\n    |\n\nF821 Undefined name `Tuple`\n   --> canonical_flow/L_classification_evaluation/conformal_prediction.py:525:21\n    |\n523 |     noise_level: float = 0.1,\n524 |     random_seed: int = 42\n525 | ) -> Dict[str, List[Tuple[float, float]]]:\n    |                     ^^^^^\n526 |     \"\"\"\n527 |     Generate sample calibration data for testing conformal predictions.\n    |\n\nF821 Undefined name `Path`\n  --> canonical_flow/L_classification_evaluation/conformal_risk_control.py:24:20\n   |\n22 | try:\n23 |     # Add project root to path for imports\n24 |     project_root = Path(__file__).resolve().parents[1]\n   |                    ^^^^\n25 |     if str(project_root) not in sys.path:\n26 |         sys.path.insert(0, str(project_root))\n   |\n\nF821 Undefined name `importlib_util`\n  --> canonical_flow/L_classification_evaluation/conformal_risk_control.py:31:16\n   |\n29 |     original_file = project_root / \"egw_query_expansion/core/conformal_risk_control.py\"\n30 |     if original_file.exists():\n31 |         spec = importlib_util.spec_from_file_location(\n   |                ^^^^^^^^^^^^^^\n32 |             f\"original_{alias_code.lower()}\", \n33 |             str(original_file)\n   |\n\nF821 Undefined name `importlib_util`\n  --> canonical_flow/L_classification_evaluation/conformal_risk_control.py:37:31\n   |\n36 |         if spec and spec.loader:\n37 |             original_module = importlib_util.module_from_spec(spec)\n   |                               ^^^^^^^^^^^^^^\n38 |             spec.loader.exec_module(original_module)\n   |\n\nF821 Undefined name `e`\n  --> canonical_flow/L_classification_evaluation/conformal_risk_control.py:56:67\n   |\n54 |     def process(data=None, context=None):\n55 |         \"\"\"Placeholder process function for failed import.\"\"\"\n56 |         return {\"error\": f\"Module {alias_source} failed to load: {e}\"}\n   |                                                                   ^\n   |\n\nF821 Undefined name `dataclass`\n  --> canonical_flow/L_classification_evaluation/decalogo_scoring_system.py:23:2\n   |\n23 | @dataclass\n   |  ^^^^^^^^^\n24 | class QuestionResponse:\n25 |     \"\"\"Individual question response with evidence quality assessment\"\"\"\n   |\n\nF821 Undefined name `dataclass`\n  --> canonical_flow/L_classification_evaluation/decalogo_scoring_system.py:34:2\n   |\n34 | @dataclass\n   |  ^^^^^^^^^\n35 | class DimensionScore:\n36 |     \"\"\"Score for a dimension (DE-1, DE-2, DE-3, DE-4)\"\"\"\n   |\n\nF821 Undefined name `List`\n  --> canonical_flow/L_classification_evaluation/decalogo_scoring_system.py:38:25\n   |\n36 |     \"\"\"Score for a dimension (DE-1, DE-2, DE-3, DE-4)\"\"\"\n37 |     dimension_id: str\n38 |     question_responses: List[QuestionResponse]\n   |                         ^^^^\n39 |     weighted_average: float\n40 |     total_questions: int\n   |\n\nF821 Undefined name `dataclass`\n  --> canonical_flow/L_classification_evaluation/decalogo_scoring_system.py:43:2\n   |\n43 | @dataclass\n   |  ^^^^^^^^^\n44 | class PointScore:\n45 |     \"\"\"Final score for a Dec\u00e1logo point\"\"\"\n   |\n\nF821 Undefined name `List`\n  --> canonical_flow/L_classification_evaluation/decalogo_scoring_system.py:47:23\n   |\n45 |     \"\"\"Final score for a Dec\u00e1logo point\"\"\"\n46 |     point_id: int\n47 |     dimension_scores: List[DimensionScore]\n   |                       ^^^^\n48 |     final_score: float\n49 |     total_questions: int\n   |\n\nF821 Undefined name `Decimal`\n  --> canonical_flow/L_classification_evaluation/decalogo_scoring_system.py:67:33\n   |\n65 |         \"\"\"\n66 |         self.precision = precision\n67 |         self.rounding_context = Decimal(10) ** (-precision)\n   |                                 ^^^^^^^\n68 |         \n69 |         # Load configuration or use defaults\n   |\n\nF821 Undefined name `get_thresholds`\n  --> canonical_flow/L_classification_evaluation/decalogo_scoring_system.py:76:26\n   |\n74 |         if THRESHOLDS_AVAILABLE:\n75 |             try:\n76 |                 config = get_thresholds()\n   |                          ^^^^^^^^^^^^^^\n77 |                 decalogo_config = config.decalogo_scoring\n78 |                 evidence_config = config.evidence_multipliers\n   |\n\nF821 Undefined name `Decimal`\n  --> canonical_flow/L_classification_evaluation/decalogo_scoring_system.py:82:24\n   |\n80 |                 # Load base scores\n81 |                 self.BASE_SCORES = {\n82 |                     k: Decimal(str(v)) for k, v in decalogo_config.base_scores.items()\n   |                        ^^^^^^^\n83 |                 }\n   |\n\nF821 Undefined name `Decimal`\n  --> canonical_flow/L_classification_evaluation/decalogo_scoring_system.py:87:24\n   |\n85 |                 # Load dimension weights  \n86 |                 self.DECALOGO_WEIGHTS = {\n87 |                     k: Decimal(str(v)) for k, v in decalogo_config.dimension_weights.items()\n   |                        ^^^^^^^\n88 |                 }\n   |\n\nF821 Undefined name `Decimal`\n  --> canonical_flow/L_classification_evaluation/decalogo_scoring_system.py:91:39\n   |\n90 |                 # Load multiplier ranges\n91 |                 self.MIN_MULTIPLIER = Decimal(str(evidence_config.MIN_MULTIPLIER))\n   |                                       ^^^^^^^\n92 |                 self.MAX_MULTIPLIER = Decimal(str(evidence_config.MAX_MULTIPLIER))\n   |\n\nF821 Undefined name `Decimal`\n  --> canonical_flow/L_classification_evaluation/decalogo_scoring_system.py:92:39\n   |\n90 |                 # Load multiplier ranges\n91 |                 self.MIN_MULTIPLIER = Decimal(str(evidence_config.MIN_MULTIPLIER))\n92 |                 self.MAX_MULTIPLIER = Decimal(str(evidence_config.MAX_MULTIPLIER))\n   |                                       ^^^^^^^\n93 |                 \n94 |                 # Load compliance thresholds\n   |\n\nF821 Undefined name `Decimal`\n   --> canonical_flow/L_classification_evaluation/decalogo_scoring_system.py:99:44\n    |\n 98 |                 # Load evidence weights\n 99 |                 self.completeness_weight = Decimal(str(evidence_config.completeness_weight))\n    |                                            ^^^^^^^\n100 |                 self.reference_quality_weight = Decimal(str(evidence_config.reference_quality_weight))\n    |\n\nF821 Undefined name `Decimal`\n   --> canonical_flow/L_classification_evaluation/decalogo_scoring_system.py:100:49\n    |\n 98 |                 # Load evidence weights\n 99 |                 self.completeness_weight = Decimal(str(evidence_config.completeness_weight))\n100 |                 self.reference_quality_weight = Decimal(str(evidence_config.reference_quality_weight))\n    |                                                 ^^^^^^^\n101 |                 \n102 | # # #                 logger.info(\"Loaded Dec\u00e1logo scoring configuration from centralized thresholds\")  # Module not found  # Module \u2026\n    |\n\nF821 Undefined name `Decimal`\n   --> canonical_flow/L_classification_evaluation/decalogo_scoring_system.py:114:19\n    |\n112 |         # Exact scoring values\n113 |         self.BASE_SCORES = {\n114 |             \"S\u00ed\": Decimal(\"1.0\"),\n    |                   ^^^^^^^\n115 |             \"Parcial\": Decimal(\"0.5\"),\n116 |             \"No\": Decimal(\"0.0\"),\n    |\n\nF821 Undefined name `Decimal`\n   --> canonical_flow/L_classification_evaluation/decalogo_scoring_system.py:115:24\n    |\n113 |         self.BASE_SCORES = {\n114 |             \"S\u00ed\": Decimal(\"1.0\"),\n115 |             \"Parcial\": Decimal(\"0.5\"),\n    |                        ^^^^^^^\n116 |             \"No\": Decimal(\"0.0\"),\n117 |             \"NI\": Decimal(\"0.0\")\n    |\n\nF821 Undefined name `Decimal`\n   --> canonical_flow/L_classification_evaluation/decalogo_scoring_system.py:116:19\n    |\n114 |             \"S\u00ed\": Decimal(\"1.0\"),\n115 |             \"Parcial\": Decimal(\"0.5\"),\n116 |             \"No\": Decimal(\"0.0\"),\n    |                   ^^^^^^^\n117 |             \"NI\": Decimal(\"0.0\")\n118 |         }\n    |\n\nF821 Undefined name `Decimal`\n   --> canonical_flow/L_classification_evaluation/decalogo_scoring_system.py:117:19\n    |\n115 |             \"Parcial\": Decimal(\"0.5\"),\n116 |             \"No\": Decimal(\"0.0\"),\n117 |             \"NI\": Decimal(\"0.0\")\n    |                   ^^^^^^^\n118 |         }\n    |\n\nF821 Undefined name `Decimal`\n   --> canonical_flow/L_classification_evaluation/decalogo_scoring_system.py:122:21\n    |\n120 |         # Predefined Dec\u00e1logo weights for point-level composition\n121 |         self.DECALOGO_WEIGHTS = {\n122 |             \"DE-1\": Decimal(\"0.30\"),  # Productos\n    |                     ^^^^^^^\n123 |             \"DE-2\": Decimal(\"0.25\"),  # Diagn\u00f3stico\n124 |             \"DE-3\": Decimal(\"0.25\"),  # Seguimiento\n    |\n\nF821 Undefined name `Decimal`\n   --> canonical_flow/L_classification_evaluation/decalogo_scoring_system.py:123:21\n    |\n121 |         self.DECALOGO_WEIGHTS = {\n122 |             \"DE-1\": Decimal(\"0.30\"),  # Productos\n123 |             \"DE-2\": Decimal(\"0.25\"),  # Diagn\u00f3stico\n    |                     ^^^^^^^\n124 |             \"DE-3\": Decimal(\"0.25\"),  # Seguimiento\n125 |             \"DE-4\": Decimal(\"0.20\")   # Evaluaci\u00f3n\n    |\n\nF821 Undefined name `Decimal`\n   --> canonical_flow/L_classification_evaluation/decalogo_scoring_system.py:124:21\n    |\n122 |             \"DE-1\": Decimal(\"0.30\"),  # Productos\n123 |             \"DE-2\": Decimal(\"0.25\"),  # Diagn\u00f3stico\n124 |             \"DE-3\": Decimal(\"0.25\"),  # Seguimiento\n    |                     ^^^^^^^\n125 |             \"DE-4\": Decimal(\"0.20\")   # Evaluaci\u00f3n\n126 |         }\n    |\n\nF821 Undefined name `Decimal`\n   --> canonical_flow/L_classification_evaluation/decalogo_scoring_system.py:125:21\n    |\n123 |             \"DE-2\": Decimal(\"0.25\"),  # Diagn\u00f3stico\n124 |             \"DE-3\": Decimal(\"0.25\"),  # Seguimiento\n125 |             \"DE-4\": Decimal(\"0.20\")   # Evaluaci\u00f3n\n    |                     ^^^^^^^\n126 |         }\n    |\n\nF821 Undefined name `Decimal`\n   --> canonical_flow/L_classification_evaluation/decalogo_scoring_system.py:129:31\n    |\n128 |         # Evidence quality multiplier ranges\n129 |         self.MIN_MULTIPLIER = Decimal(\"0.5\")\n    |                               ^^^^^^^\n130 |         self.MAX_MULTIPLIER = Decimal(\"1.2\")\n    |\n\nF821 Undefined name `Decimal`\n   --> canonical_flow/L_classification_evaluation/decalogo_scoring_system.py:130:31\n    |\n128 |         # Evidence quality multiplier ranges\n129 |         self.MIN_MULTIPLIER = Decimal(\"0.5\")\n130 |         self.MAX_MULTIPLIER = Decimal(\"1.2\")\n    |                               ^^^^^^^\n131 |         \n132 |         # Default evidence weights\n    |\n\nF821 Undefined name `Decimal`\n   --> canonical_flow/L_classification_evaluation/decalogo_scoring_system.py:133:36\n    |\n132 |         # Default evidence weights\n133 |         self.completeness_weight = Decimal(\"0.7\")\n    |                                    ^^^^^^^\n134 |         self.reference_quality_weight = Decimal(\"0.3\")\n    |\n\nF821 Undefined name `Decimal`\n   --> canonical_flow/L_classification_evaluation/decalogo_scoring_system.py:134:41\n    |\n132 |         # Default evidence weights\n133 |         self.completeness_weight = Decimal(\"0.7\")\n134 |         self.reference_quality_weight = Decimal(\"0.3\")\n    |                                         ^^^^^^^\n135 |         \n136 |         # Default compliance thresholds\n    |\n\nF821 Undefined name `Decimal`\n   --> canonical_flow/L_classification_evaluation/decalogo_scoring_system.py:159:54\n    |\n159 |     def calculate_base_score(self, response: str) -> Decimal:\n    |                                                      ^^^^^^^\n160 |         \"\"\"\n161 |         Calculate deterministic base score for a response.\n    |\n\nF821 Undefined name `Decimal`\n   --> canonical_flow/L_classification_evaluation/decalogo_scoring_system.py:179:10\n    |\n177 |         evidence_completeness: float, \n178 |         page_reference_quality: float\n179 |     ) -> Decimal:\n    |          ^^^^^^^\n180 |         \"\"\"\n181 |         Calculate evidence quality multiplier based on completeness and reference quality.\n    |\n\nF821 Undefined name `Decimal`\n   --> canonical_flow/L_classification_evaluation/decalogo_scoring_system.py:191:24\n    |\n189 |         \"\"\"\n190 |         # Convert to Decimal for precision\n191 |         completeness = Decimal(str(max(0.0, min(1.0, evidence_completeness))))\n    |                        ^^^^^^^\n192 |         reference_quality = Decimal(str(max(0.0, min(1.0, page_reference_quality))))\n    |\n\nF821 Undefined name `Decimal`\n   --> canonical_flow/L_classification_evaluation/decalogo_scoring_system.py:192:29\n    |\n190 |         # Convert to Decimal for precision\n191 |         completeness = Decimal(str(max(0.0, min(1.0, evidence_completeness))))\n192 |         reference_quality = Decimal(str(max(0.0, min(1.0, page_reference_quality))))\n    |                             ^^^^^^^\n193 |         \n194 |         # Combined evidence quality (weighted average)\n    |\n\nF821 Undefined name `Tuple`\n   --> canonical_flow/L_classification_evaluation/decalogo_scoring_system.py:209:10\n    |\n207 |         evidence_completeness: float,\n208 |         page_reference_quality: float\n209 |     ) -> Tuple[Decimal, Decimal, Decimal]:\n    |          ^^^^^\n210 |         \"\"\"\n211 |         Calculate final score with evidence quality adjustment.\n    |\n\nF821 Undefined name `Decimal`\n   --> canonical_flow/L_classification_evaluation/decalogo_scoring_system.py:209:16\n    |\n207 |         evidence_completeness: float,\n208 |         page_reference_quality: float\n209 |     ) -> Tuple[Decimal, Decimal, Decimal]:\n    |                ^^^^^^^\n210 |         \"\"\"\n211 |         Calculate final score with evidence quality adjustment.\n    |\n\nF821 Undefined name `Decimal`\n   --> canonical_flow/L_classification_evaluation/decalogo_scoring_system.py:209:25\n    |\n207 |         evidence_completeness: float,\n208 |         page_reference_quality: float\n209 |     ) -> Tuple[Decimal, Decimal, Decimal]:\n    |                         ^^^^^^^\n210 |         \"\"\"\n211 |         Calculate final score with evidence quality adjustment.\n    |\n\nF821 Undefined name `Decimal`\n   --> canonical_flow/L_classification_evaluation/decalogo_scoring_system.py:209:34\n    |\n207 |         evidence_completeness: float,\n208 |         page_reference_quality: float\n209 |     ) -> Tuple[Decimal, Decimal, Decimal]:\n    |                                  ^^^^^^^\n210 |         \"\"\"\n211 |         Calculate final score with evidence quality adjustment.\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/L_classification_evaluation/decalogo_scoring_system.py:232:29\n    |\n230 |     def aggregate_dimension_score(\n231 |         self, \n232 |         question_responses: List[QuestionResponse]\n    |                             ^^^^\n233 |     ) -> Decimal:\n234 |         \"\"\"\n    |\n\nF821 Undefined name `Decimal`\n   --> canonical_flow/L_classification_evaluation/decalogo_scoring_system.py:233:10\n    |\n231 |         self, \n232 |         question_responses: List[QuestionResponse]\n233 |     ) -> Decimal:\n    |          ^^^^^^^\n234 |         \"\"\"\n235 |         Calculate dimension-level weighted average score.\n    |\n\nF821 Undefined name `Decimal`\n   --> canonical_flow/L_classification_evaluation/decalogo_scoring_system.py:244:20\n    |\n242 |         \"\"\"\n243 |         if not question_responses:\n244 |             return Decimal(\"0.0\")\n    |                    ^^^^^^^\n245 |         \n246 |         # Calculate weighted average (all questions have equal weight)\n    |\n\nF821 Undefined name `Decimal`\n   --> canonical_flow/L_classification_evaluation/decalogo_scoring_system.py:247:27\n    |\n246 |         # Calculate weighted average (all questions have equal weight)\n247 |         total_score = sum(Decimal(str(resp.final_score)) for resp in question_responses)\n    |                           ^^^^^^^\n248 |         dimension_score = total_score / Decimal(str(len(question_responses)))\n    |\n\nF821 Undefined name `Decimal`\n   --> canonical_flow/L_classification_evaluation/decalogo_scoring_system.py:248:41\n    |\n246 |         # Calculate weighted average (all questions have equal weight)\n247 |         total_score = sum(Decimal(str(resp.final_score)) for resp in question_responses)\n248 |         dimension_score = total_score / Decimal(str(len(question_responses)))\n    |                                         ^^^^^^^\n249 |         \n250 |         return self._round_decimal(dimension_score)\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/L_classification_evaluation/decalogo_scoring_system.py:252:53\n    |\n250 |         return self._round_decimal(dimension_score)\n251 |     \n252 |     def compose_point_score(self, dimension_scores: Dict[str, Decimal]) -> Decimal:\n    |                                                     ^^^^\n253 |         \"\"\"\n254 |         Combine dimension scores using predefined Dec\u00e1logo weights.\n    |\n\nF821 Undefined name `Decimal`\n   --> canonical_flow/L_classification_evaluation/decalogo_scoring_system.py:252:63\n    |\n250 |         return self._round_decimal(dimension_score)\n251 |     \n252 |     def compose_point_score(self, dimension_scores: Dict[str, Decimal]) -> Decimal:\n    |                                                               ^^^^^^^\n253 |         \"\"\"\n254 |         Combine dimension scores using predefined Dec\u00e1logo weights.\n    |\n\nF821 Undefined name `Decimal`\n   --> canonical_flow/L_classification_evaluation/decalogo_scoring_system.py:252:76\n    |\n250 |         return self._round_decimal(dimension_score)\n251 |     \n252 |     def compose_point_score(self, dimension_scores: Dict[str, Decimal]) -> Decimal:\n    |                                                                            ^^^^^^^\n253 |         \"\"\"\n254 |         Combine dimension scores using predefined Dec\u00e1logo weights.\n    |\n\nF821 Undefined name `Decimal`\n   --> canonical_flow/L_classification_evaluation/decalogo_scoring_system.py:262:24\n    |\n260 |             Final point score\n261 |         \"\"\"\n262 |         weighted_sum = Decimal(\"0.0\")\n    |                        ^^^^^^^\n263 |         total_weight = Decimal(\"0.0\")\n    |\n\nF821 Undefined name `Decimal`\n   --> canonical_flow/L_classification_evaluation/decalogo_scoring_system.py:263:24\n    |\n261 |         \"\"\"\n262 |         weighted_sum = Decimal(\"0.0\")\n263 |         total_weight = Decimal(\"0.0\")\n    |                        ^^^^^^^\n264 |         \n265 |         for dimension_id, score in dimension_scores.items():\n    |\n\nF821 Undefined name `Decimal`\n   --> canonical_flow/L_classification_evaluation/decalogo_scoring_system.py:268:33\n    |\n266 |             if dimension_id in self.DECALOGO_WEIGHTS:\n267 |                 weight = self.DECALOGO_WEIGHTS[dimension_id]\n268 |                 weighted_sum += Decimal(str(score)) * weight\n    |                                 ^^^^^^^\n269 |                 total_weight += weight\n270 |             else:\n    |\n\nF821 Undefined name `Decimal`\n   --> canonical_flow/L_classification_evaluation/decalogo_scoring_system.py:274:20\n    |\n273 |         if total_weight == 0:\n274 |             return Decimal(\"0.0\")\n    |                    ^^^^^^^\n275 |         \n276 |         point_score = weighted_sum / total_weight\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/L_classification_evaluation/decalogo_scoring_system.py:308:26\n    |\n306 |         self,\n307 |         point_id: int,\n308 |         evaluation_data: Dict[str, List[Dict[str, Any]]]\n    |                          ^^^^\n309 |     ) -> PointScore:\n310 |         \"\"\"\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/L_classification_evaluation/decalogo_scoring_system.py:308:36\n    |\n306 |         self,\n307 |         point_id: int,\n308 |         evaluation_data: Dict[str, List[Dict[str, Any]]]\n    |                                    ^^^^\n309 |     ) -> PointScore:\n310 |         \"\"\"\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/L_classification_evaluation/decalogo_scoring_system.py:308:41\n    |\n306 |         self,\n307 |         point_id: int,\n308 |         evaluation_data: Dict[str, List[Dict[str, Any]]]\n    |                                         ^^^^\n309 |     ) -> PointScore:\n310 |         \"\"\"\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/L_classification_evaluation/decalogo_scoring_system.py:308:51\n    |\n306 |         self,\n307 |         point_id: int,\n308 |         evaluation_data: Dict[str, List[Dict[str, Any]]]\n    |                                                   ^^^\n309 |     ) -> PointScore:\n310 |         \"\"\"\n    |\n\nF821 Undefined name `Decimal`\n   --> canonical_flow/L_classification_evaluation/decalogo_scoring_system.py:385:37\n    |\n383 |         )\n384 |     \n385 |     def _round_decimal(self, value: Decimal) -> Decimal:\n    |                                     ^^^^^^^\n386 |         \"\"\"Round decimal to specified precision using consistent rounding.\"\"\"\n387 |         return value.quantize(self.rounding_context, rounding=ROUND_HALF_UP)\n    |\n\nF821 Undefined name `Decimal`\n   --> canonical_flow/L_classification_evaluation/decalogo_scoring_system.py:385:49\n    |\n383 |         )\n384 |     \n385 |     def _round_decimal(self, value: Decimal) -> Decimal:\n    |                                                 ^^^^^^^\n386 |         \"\"\"Round decimal to specified precision using consistent rounding.\"\"\"\n387 |         return value.quantize(self.rounding_context, rounding=ROUND_HALF_UP)\n    |\n\nF821 Undefined name `ROUND_HALF_UP`\n   --> canonical_flow/L_classification_evaluation/decalogo_scoring_system.py:387:63\n    |\n385 |     def _round_decimal(self, value: Decimal) -> Decimal:\n386 |         \"\"\"Round decimal to specified precision using consistent rounding.\"\"\"\n387 |         return value.quantize(self.rounding_context, rounding=ROUND_HALF_UP)\n    |                                                               ^^^^^^^^^^^^^\n388 |     \n389 |     def get_system_info(self) -> Dict[str, Any]:\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/L_classification_evaluation/decalogo_scoring_system.py:389:34\n    |\n387 |         return value.quantize(self.rounding_context, rounding=ROUND_HALF_UP)\n388 |     \n389 |     def get_system_info(self) -> Dict[str, Any]:\n    |                                  ^^^^\n390 |         \"\"\"Get information about the scoring system configuration.\"\"\"\n391 |         return {\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/L_classification_evaluation/decalogo_scoring_system.py:389:44\n    |\n387 |         return value.quantize(self.rounding_context, rounding=ROUND_HALF_UP)\n388 |     \n389 |     def get_system_info(self) -> Dict[str, Any]:\n    |                                            ^^^\n390 |         \"\"\"Get information about the scoring system configuration.\"\"\"\n391 |         return {\n    |\n\ninvalid-syntax: Unexpected indentation\n  --> canonical_flow/L_classification_evaluation/demo_schema_validation.py:12:1\n   |\n11 | # # # from schemas import (  # Module not found  # Module not found  # Module not found\n12 |     QuestionEvalInput,\n   | ^^^^\n13 |     DimensionEvalOutput, \n14 |     PointEvalOutput,\n   |\n\ninvalid-syntax: Expected a statement\n  --> canonical_flow/L_classification_evaluation/demo_schema_validation.py:21:1\n   |\n19 |     DimensionType,\n20 |     ComplianceLevel\n21 | )\n   | ^\n22 | # # # from question_registry import DecalogoQuestionRegistry, get_default_registry  # Module not found  # Module not found  # Module n\u2026\n23 | # # # from decalogo_scoring_system import ScoringSystem  # Module not found  # Module not found  # Module not found\n   |\n\ninvalid-syntax: Expected a statement\n  --> canonical_flow/L_classification_evaluation/demo_schema_validation.py:21:2\n   |\n19 |     DimensionType,\n20 |     ComplianceLevel\n21 | )\n   |  ^\n22 | # # # from question_registry import DecalogoQuestionRegistry, get_default_registry  # Module not found  # Module not found  # Module n\u2026\n23 | # # # from decalogo_scoring_system import ScoringSystem  # Module not found  # Module not found  # Module not found\n   |\n\nF821 Undefined name `Path`\n   --> canonical_flow/L_classification_evaluation/demo_stage_orchestrator.py:107:16\n    |\n106 |     # Create temporary directory and sample data\n107 |     temp_dir = Path(tempfile.mkdtemp())\n    |                ^^^^\n108 |     sample_data = create_sample_data()\n    |\n\nF821 Undefined name `LClassificationStageOrchestrator`\n   --> canonical_flow/L_classification_evaluation/demo_stage_orchestrator.py:120:20\n    |\n119 |     # Initialize orchestrator\n120 |     orchestrator = LClassificationStageOrchestrator(precision=4)\n    |                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n121 |     \n122 |     # Process using direct orchestrator\n    |\n\nF821 Undefined name `Path`\n   --> canonical_flow/L_classification_evaluation/demo_stage_orchestrator.py:155:16\n    |\n153 |     print(\"=== MULTIPLE POINTS DEMO ===\")\n154 |     \n155 |     temp_dir = Path(tempfile.mkdtemp())\n    |                ^^^^\n156 |     \n157 |     # Create different scenarios for different points\n    |\n\nF821 Undefined name `process`\n   --> canonical_flow/L_classification_evaluation/demo_stage_orchestrator.py:187:15\n    |\n185 |     # Process using standalone function\n186 |     print(\"\\nProcessing multiple points...\")\n187 |     results = process(\n    |               ^^^^^^^\n188 |         data=str(input_dir),\n189 |         context={\"batch_mode\": True}\n    |\n\nF541 [*] f-string without any placeholders\n   --> canonical_flow/L_classification_evaluation/demo_stage_orchestrator.py:194:11\n    |\n192 |     # Display results summary\n193 |     summary = results['status_report']['processing_summary']\n194 |     print(f\"\\nProcessing Summary:\")\n    |           ^^^^^^^^^^^^^^^^^^^^^^^^\n195 |     print(f\"  Total Points: {summary['total_points']}\")\n196 |     print(f\"  Successful: {summary['successful_points']}\")\n    |\nhelp: Remove extraneous `f` prefix\n\nF541 [*] f-string without any placeholders\n   --> canonical_flow/L_classification_evaluation/demo_stage_orchestrator.py:201:11\n    |\n200 |     # Show individual point scores\n201 |     print(f\"\\nIndividual Point Scores:\")\n    |           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n202 |     for point_key, summary in results['artifacts']['point_summaries'].items():\n203 |         print(f\"  {point_key}: {summary['final_score']:.4f} (Questions: {summary['total_questions']})\")\n    |\nhelp: Remove extraneous `f` prefix\n\nF541 [*] f-string without any placeholders\n   --> canonical_flow/L_classification_evaluation/demo_stage_orchestrator.py:206:11\n    |\n205 |     # Show points index\n206 |     print(f\"\\nPoints Index:\")\n    |           ^^^^^^^^^^^^^^^^^^\n207 |     for point_key, index_info in results['artifacts']['points_index'].items():\n208 |         artifacts_count = len(index_info['artifacts_available'])\n    |\nhelp: Remove extraneous `f` prefix\n\nF821 Undefined name `Path`\n   --> canonical_flow/L_classification_evaluation/demo_stage_orchestrator.py:221:16\n    |\n219 |     print(\"=== ERROR HANDLING DEMO ===\")\n220 |     \n221 |     temp_dir = Path(tempfile.mkdtemp())\n    |                ^^^^\n222 |     input_dir = temp_dir / \"classification_input/error_demo\"\n    |\n\nF821 Undefined name `process`\n   --> canonical_flow/L_classification_evaluation/demo_stage_orchestrator.py:248:15\n    |\n246 |     # Process with error handling\n247 |     print(\"\\nProcessing with error isolation...\")\n248 |     results = process(data=str(input_dir))\n    |               ^^^^^^^\n249 |     \n250 |     # Show results\n    |\n\nF541 [*] f-string without any placeholders\n   --> canonical_flow/L_classification_evaluation/demo_stage_orchestrator.py:251:11\n    |\n250 |     # Show results\n251 |     print(f\"\\nError Handling Results:\")\n    |           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n252 |     print(f\"  Successful Points: {results['status_report']['successful_points']}\")\n253 |     print(f\"  Failed Points: {len(results['status_report']['failed_points'])}\")\n    |\nhelp: Remove extraneous `f` prefix\n\nF541 [*] f-string without any placeholders\n   --> canonical_flow/L_classification_evaluation/demo_stage_orchestrator.py:257:15\n    |\n255 |     # Show failure details\n256 |     if results['status_report']['failed_points']:\n257 |         print(f\"\\nFailure Details:\")\n    |               ^^^^^^^^^^^^^^^^^^^^^\n258 |         for failure in results['status_report']['failed_points']:\n259 |             print(f\"  Point {failure['point_id']}: {failure['error']}\")\n    |\nhelp: Remove extraneous `f` prefix\n\nF821 Undefined name `Path`\n   --> canonical_flow/L_classification_evaluation/demo_stage_orchestrator.py:275:16\n    |\n273 |     print(\"=== DETERMINISTIC SERIALIZATION DEMO ===\")\n274 |     \n275 |     temp_dir = Path(tempfile.mkdtemp())\n    |                ^^^^\n276 |     sample_data = create_sample_data()\n    |\n\nF821 Undefined name `LClassificationStageOrchestrator`\n   --> canonical_flow/L_classification_evaluation/demo_stage_orchestrator.py:286:20\n    |\n285 |     # Process multiple times\n286 |     orchestrator = LClassificationStageOrchestrator()\n    |                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n287 |     \n288 |     print(\"Running multiple processing iterations...\")\n    |\n\nF541 [*] f-string without any placeholders\n   --> canonical_flow/L_classification_evaluation/demo_stage_orchestrator.py:311:15\n    |\n310 |         # Show JSON format sample\n311 |         print(f\"\\nJSON format sample (first 500 characters):\")\n    |               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n312 |         print(json_outputs[0][:500] + \"...\")\n313 |     else:\n    |\nhelp: Remove extraneous `f` prefix\n\nF821 Undefined name `Path`\n   --> canonical_flow/L_classification_evaluation/demo_stage_orchestrator.py:338:16\n    |\n336 |     print(\"=== ARTIFACT TYPES DEMO ===\")\n337 |     \n338 |     temp_dir = Path(tempfile.mkdtemp())\n    |                ^^^^\n339 |     sample_data = create_sample_data()\n    |\n\nF821 Undefined name `process`\n   --> canonical_flow/L_classification_evaluation/demo_stage_orchestrator.py:349:15\n    |\n348 |     # Process\n349 |     results = process(data=str(input_file))\n    |               ^^^^^^^\n350 |     \n351 |     # Show all artifact types\n    |\n\nF541 [*] f-string without any placeholders\n   --> canonical_flow/L_classification_evaluation/demo_stage_orchestrator.py:353:11\n    |\n351 |     # Show all artifact types\n352 |     artifacts = results['artifacts']\n353 |     print(f\"Generated Artifact Types:\")\n    |           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n354 |     \n355 |     for artifact_type, artifact_data in artifacts.items():\n    |\nhelp: Remove extraneous `f` prefix\n\nF541 [*] f-string without any placeholders\n   --> canonical_flow/L_classification_evaluation/demo_stage_orchestrator.py:368:11\n    |\n367 |     # Show specific artifact details\n368 |     print(f\"\\nDetailed Artifact Examples:\")\n    |           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n369 |     \n370 |     # Point Summary\n    |\nhelp: Remove extraneous `f` prefix\n\nF541 [*] f-string without any placeholders\n   --> canonical_flow/L_classification_evaluation/demo_stage_orchestrator.py:373:15\n    |\n371 |     if 'point_summaries' in artifacts and artifacts['point_summaries']:\n372 |         first_summary = list(artifacts['point_summaries'].values())[0]\n373 |         print(f\"\\nPoint Summary:\")\n    |               ^^^^^^^^^^^^^^^^^^^\n374 |         for key, value in first_summary.items():\n375 |             print(f\"  {key}: {value}\")\n    |\nhelp: Remove extraneous `f` prefix\n\nF541 [*] f-string without any placeholders\n   --> canonical_flow/L_classification_evaluation/demo_stage_orchestrator.py:380:15\n    |\n378 |     if 'guard_reports' in artifacts and artifacts['guard_reports']:\n379 |         first_guard = list(artifacts['guard_reports'].values())[0]\n380 |         print(f\"\\nGuard Report:\")\n    |               ^^^^^^^^^^^^^^^^^^\n381 |         print(f\"  Guard Status: {first_guard['guard_status']}\")\n382 |         print(f\"  Validation Checks: {first_guard['validation_checks']}\")\n    |\nhelp: Remove extraneous `f` prefix\n\nF541 [*] f-string without any placeholders\n   --> canonical_flow/L_classification_evaluation/demo_stage_orchestrator.py:387:15\n    |\n385 |     if 'composition_traces' in artifacts and artifacts['composition_traces']:\n386 |         first_trace = list(artifacts['composition_traces'].values())[0]\n387 |         print(f\"\\nComposition Trace:\")\n    |               ^^^^^^^^^^^^^^^^^^^^^^^\n388 |         print(f\"  Component Order: {first_trace['component_execution_order']}\")\n389 |         print(f\"  Dimension Weights: {first_trace['dimension_weights']}\")\n    |\nhelp: Remove extraneous `f` prefix\n\nF821 Undefined name `LClassificationStageOrchestrator`\n   --> canonical_flow/L_classification_evaluation/demo_stage_orchestrator.py:401:20\n    |\n399 |     print(\"=== ORCHESTRATOR CONFIGURATION ===\")\n400 |     \n401 |     orchestrator = LClassificationStageOrchestrator()\n    |                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n402 |     info = orchestrator.get_orchestrator_info()\n    |\n\nF821 Undefined name `pprint`\n   --> canonical_flow/L_classification_evaluation/demo_stage_orchestrator.py:405:5\n    |\n404 |     print(\"Orchestrator Information:\")\n405 |     pprint(info, indent=2)\n    |     ^^^^^^\n406 |     print()\n    |\n\nF821 Undefined name `dataclass`\n  --> canonical_flow/L_classification_evaluation/evidence_adapter.py:17:2\n   |\n17 | @dataclass\n   |  ^^^^^^^^^\n18 | class QuestionEvalInput:\n19 |     \"\"\"Standardized input object for question evaluation with evidence quality metrics.\"\"\"\n   |\n\nF821 Undefined name `Dict`\n  --> canonical_flow/L_classification_evaluation/evidence_adapter.py:24:24\n   |\n22 |     evidence_completeness: float  # Ratio of questions with valid evidence to total questions\n23 |     page_reference_quality: float  # Fraction of evidence items with both page_num and exact_text\n24 |     evidence_metadata: Dict[str, Any] = field(default_factory=dict)\n   |                        ^^^^\n25 |     processing_timestamp: str = \"\"\n   |\n\nF821 Undefined name `Any`\n  --> canonical_flow/L_classification_evaluation/evidence_adapter.py:24:34\n   |\n22 |     evidence_completeness: float  # Ratio of questions with valid evidence to total questions\n23 |     page_reference_quality: float  # Fraction of evidence items with both page_num and exact_text\n24 |     evidence_metadata: Dict[str, Any] = field(default_factory=dict)\n   |                                  ^^^\n25 |     processing_timestamp: str = \"\"\n   |\n\nF821 Undefined name `field`\n  --> canonical_flow/L_classification_evaluation/evidence_adapter.py:24:41\n   |\n22 |     evidence_completeness: float  # Ratio of questions with valid evidence to total questions\n23 |     page_reference_quality: float  # Fraction of evidence items with both page_num and exact_text\n24 |     evidence_metadata: Dict[str, Any] = field(default_factory=dict)\n   |                                         ^^^^^\n25 |     processing_timestamp: str = \"\"\n   |\n\nF821 Undefined name `Dict`\n  --> canonical_flow/L_classification_evaluation/evidence_adapter.py:27:26\n   |\n25 |     processing_timestamp: str = \"\"\n26 |\n27 |     def to_dict(self) -> Dict[str, Any]:\n   |                          ^^^^\n28 |         \"\"\"Convert to dictionary with deterministic ordering.\"\"\"\n29 |         return OrderedDict([\n   |\n\nF821 Undefined name `Any`\n  --> canonical_flow/L_classification_evaluation/evidence_adapter.py:27:36\n   |\n25 |     processing_timestamp: str = \"\"\n26 |\n27 |     def to_dict(self) -> Dict[str, Any]:\n   |                                    ^^^\n28 |         \"\"\"Convert to dictionary with deterministic ordering.\"\"\"\n29 |         return OrderedDict([\n   |\n\nF821 Undefined name `OrderedDict`\n  --> canonical_flow/L_classification_evaluation/evidence_adapter.py:29:16\n   |\n27 |     def to_dict(self) -> Dict[str, Any]:\n28 |         \"\"\"Convert to dictionary with deterministic ordering.\"\"\"\n29 |         return OrderedDict([\n   |                ^^^^^^^^^^^\n30 |             (\"question_id\", self.question_id),\n31 |             (\"response\", self.response),\n   |\n\nF821 Undefined name `dataclass`\n  --> canonical_flow/L_classification_evaluation/evidence_adapter.py:43:2\n   |\n43 | @dataclass\n   |  ^^^^^^^^^\n44 | class EvidenceQualityMetrics:\n45 |     \"\"\"Aggregated evidence quality metrics for a set of questions.\"\"\"\n   |\n\nF821 Undefined name `Dict`\n  --> canonical_flow/L_classification_evaluation/evidence_adapter.py:53:26\n   |\n51 |     page_reference_quality: float\n52 |\n53 |     def to_dict(self) -> Dict[str, Any]:\n   |                          ^^^^\n54 |         \"\"\"Convert to dictionary with deterministic ordering.\"\"\"\n55 |         return OrderedDict([\n   |\n\nF821 Undefined name `Any`\n  --> canonical_flow/L_classification_evaluation/evidence_adapter.py:53:36\n   |\n51 |     page_reference_quality: float\n52 |\n53 |     def to_dict(self) -> Dict[str, Any]:\n   |                                    ^^^\n54 |         \"\"\"Convert to dictionary with deterministic ordering.\"\"\"\n55 |         return OrderedDict([\n   |\n\nF821 Undefined name `OrderedDict`\n  --> canonical_flow/L_classification_evaluation/evidence_adapter.py:55:16\n   |\n53 |     def to_dict(self) -> Dict[str, Any]:\n54 |         \"\"\"Convert to dictionary with deterministic ordering.\"\"\"\n55 |         return OrderedDict([\n   |                ^^^^^^^^^^^\n56 |             (\"total_questions\", self.total_questions),\n57 |             (\"questions_with_valid_evidence\", self.questions_with_valid_evidence),\n   |\n\nF821 Undefined name `Dict`\n  --> canonical_flow/L_classification_evaluation/evidence_adapter.py:88:24\n   |\n86 |     def transform_evidence_to_eval_inputs(\n87 |         self,\n88 |         evidence_data: Dict[str, Any],\n   |                        ^^^^\n89 |         question_responses: Dict[str, str]\n90 |     ) -> Tuple[List[QuestionEvalInput], EvidenceQualityMetrics]:\n   |\n\nF821 Undefined name `Any`\n  --> canonical_flow/L_classification_evaluation/evidence_adapter.py:88:34\n   |\n86 |     def transform_evidence_to_eval_inputs(\n87 |         self,\n88 |         evidence_data: Dict[str, Any],\n   |                                  ^^^\n89 |         question_responses: Dict[str, str]\n90 |     ) -> Tuple[List[QuestionEvalInput], EvidenceQualityMetrics]:\n   |\n\nF821 Undefined name `Dict`\n  --> canonical_flow/L_classification_evaluation/evidence_adapter.py:89:29\n   |\n87 |         self,\n88 |         evidence_data: Dict[str, Any],\n89 |         question_responses: Dict[str, str]\n   |                             ^^^^\n90 |     ) -> Tuple[List[QuestionEvalInput], EvidenceQualityMetrics]:\n91 |         \"\"\"\n   |\n\nF821 Undefined name `Tuple`\n  --> canonical_flow/L_classification_evaluation/evidence_adapter.py:90:10\n   |\n88 |         evidence_data: Dict[str, Any],\n89 |         question_responses: Dict[str, str]\n90 |     ) -> Tuple[List[QuestionEvalInput], EvidenceQualityMetrics]:\n   |          ^^^^^\n91 |         \"\"\"\n92 |         Transform evidence processor output into evaluation inputs with calculated quality metrics.\n   |\n\nF821 Undefined name `List`\n  --> canonical_flow/L_classification_evaluation/evidence_adapter.py:90:16\n   |\n88 |         evidence_data: Dict[str, Any],\n89 |         question_responses: Dict[str, str]\n90 |     ) -> Tuple[List[QuestionEvalInput], EvidenceQualityMetrics]:\n   |                ^^^^\n91 |         \"\"\"\n92 |         Transform evidence processor output into evaluation inputs with calculated quality metrics.\n   |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/L_classification_evaluation/evidence_adapter.py:124:54\n    |\n122 |         return eval_inputs, quality_metrics\n123 |\n124 |     def _extract_evidence_items(self, evidence_data: Dict[str, Any]) -> List[Dict[str, Any]]:\n    |                                                      ^^^^\n125 |         \"\"\"\n126 | # # #         Extract evidence items from EvidenceProcessor output.  # Module not found  # Module not found  # Module not found\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/L_classification_evaluation/evidence_adapter.py:124:64\n    |\n122 |         return eval_inputs, quality_metrics\n123 |\n124 |     def _extract_evidence_items(self, evidence_data: Dict[str, Any]) -> List[Dict[str, Any]]:\n    |                                                                ^^^\n125 |         \"\"\"\n126 | # # #         Extract evidence items from EvidenceProcessor output.  # Module not found  # Module not found  # Module not found\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/L_classification_evaluation/evidence_adapter.py:124:73\n    |\n122 |         return eval_inputs, quality_metrics\n123 |\n124 |     def _extract_evidence_items(self, evidence_data: Dict[str, Any]) -> List[Dict[str, Any]]:\n    |                                                                         ^^^^\n125 |         \"\"\"\n126 | # # #         Extract evidence items from EvidenceProcessor output.  # Module not found  # Module not found  # Module not found\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/L_classification_evaluation/evidence_adapter.py:124:78\n    |\n122 |         return eval_inputs, quality_metrics\n123 |\n124 |     def _extract_evidence_items(self, evidence_data: Dict[str, Any]) -> List[Dict[str, Any]]:\n    |                                                                              ^^^^\n125 |         \"\"\"\n126 | # # #         Extract evidence items from EvidenceProcessor output.  # Module not found  # Module not found  # Module not found\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/L_classification_evaluation/evidence_adapter.py:124:88\n    |\n122 |         return eval_inputs, quality_metrics\n123 |\n124 |     def _extract_evidence_items(self, evidence_data: Dict[str, Any]) -> List[Dict[str, Any]]:\n    |                                                                                        ^^^\n125 |         \"\"\"\n126 | # # #         Extract evidence items from EvidenceProcessor output.  # Module not found  # Module not found  # Module not found\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/L_classification_evaluation/evidence_adapter.py:160:52\n    |\n158 |         return evidence_items\n159 |\n160 |     def _standardize_evidence_item(self, evidence: Dict[str, Any]) -> Dict[str, Any]:\n    |                                                    ^^^^\n161 |         \"\"\"\n162 |         Standardize evidence item structure for consistent processing.\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/L_classification_evaluation/evidence_adapter.py:160:62\n    |\n158 |         return evidence_items\n159 |\n160 |     def _standardize_evidence_item(self, evidence: Dict[str, Any]) -> Dict[str, Any]:\n    |                                                              ^^^\n161 |         \"\"\"\n162 |         Standardize evidence item structure for consistent processing.\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/L_classification_evaluation/evidence_adapter.py:160:71\n    |\n158 |         return evidence_items\n159 |\n160 |     def _standardize_evidence_item(self, evidence: Dict[str, Any]) -> Dict[str, Any]:\n    |                                                                       ^^^^\n161 |         \"\"\"\n162 |         Standardize evidence item structure for consistent processing.\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/L_classification_evaluation/evidence_adapter.py:160:81\n    |\n158 |         return evidence_items\n159 |\n160 |     def _standardize_evidence_item(self, evidence: Dict[str, Any]) -> Dict[str, Any]:\n    |                                                                                 ^^^\n161 |         \"\"\"\n162 |         Standardize evidence item structure for consistent processing.\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/L_classification_evaluation/evidence_adapter.py:206:25\n    |\n204 |     def _calculate_quality_metrics(\n205 |         self, \n206 |         evidence_items: List[Dict[str, Any]], \n    |                         ^^^^\n207 |         total_questions: int\n208 |     ) -> EvidenceQualityMetrics:\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/L_classification_evaluation/evidence_adapter.py:206:30\n    |\n204 |     def _calculate_quality_metrics(\n205 |         self, \n206 |         evidence_items: List[Dict[str, Any]], \n    |                              ^^^^\n207 |         total_questions: int\n208 |     ) -> EvidenceQualityMetrics:\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/L_classification_evaluation/evidence_adapter.py:206:40\n    |\n204 |     def _calculate_quality_metrics(\n205 |         self, \n206 |         evidence_items: List[Dict[str, Any]], \n    |                                        ^^^\n207 |         total_questions: int\n208 |     ) -> EvidenceQualityMetrics:\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/L_classification_evaluation/evidence_adapter.py:262:25\n    |\n260 |         self, \n261 |         question_id: str, \n262 |         evidence_items: List[Dict[str, Any]]\n    |                         ^^^^\n263 |     ) -> Dict[str, Any]:\n264 |         \"\"\"\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/L_classification_evaluation/evidence_adapter.py:262:30\n    |\n260 |         self, \n261 |         question_id: str, \n262 |         evidence_items: List[Dict[str, Any]]\n    |                              ^^^^\n263 |     ) -> Dict[str, Any]:\n264 |         \"\"\"\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/L_classification_evaluation/evidence_adapter.py:262:40\n    |\n260 |         self, \n261 |         question_id: str, \n262 |         evidence_items: List[Dict[str, Any]]\n    |                                        ^^^\n263 |     ) -> Dict[str, Any]:\n264 |         \"\"\"\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/L_classification_evaluation/evidence_adapter.py:263:10\n    |\n261 |         question_id: str, \n262 |         evidence_items: List[Dict[str, Any]]\n263 |     ) -> Dict[str, Any]:\n    |          ^^^^\n264 |         \"\"\"\n265 |         Extract relevant evidence metadata for a specific question.\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/L_classification_evaluation/evidence_adapter.py:263:20\n    |\n261 |         question_id: str, \n262 |         evidence_items: List[Dict[str, Any]]\n263 |     ) -> Dict[str, Any]:\n    |                    ^^^\n264 |         \"\"\"\n265 |         Extract relevant evidence metadata for a specific question.\n    |\n\nF821 Undefined name `OrderedDict`\n   --> canonical_flow/L_classification_evaluation/evidence_adapter.py:286:20\n    |\n284 |             relevant_items = evidence_items[:1]\n285 |         \n286 |         metadata = OrderedDict()\n    |                    ^^^^^^^^^^^\n287 |         if relevant_items:\n288 |             item = relevant_items[0]\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/L_classification_evaluation/evidence_adapter.py:304:28\n    |\n302 |         evidence_completeness: float,\n303 |         page_reference_quality: float,\n304 |         evidence_metadata: Dict[str, Any]\n    |                            ^^^^\n305 |     ) -> QuestionEvalInput:\n306 |         \"\"\"\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/L_classification_evaluation/evidence_adapter.py:304:38\n    |\n302 |         evidence_completeness: float,\n303 |         page_reference_quality: float,\n304 |         evidence_metadata: Dict[str, Any]\n    |                                      ^^^\n305 |     ) -> QuestionEvalInput:\n306 |         \"\"\"\n    |\n\nF821 Undefined name `datetime`\n   --> canonical_flow/L_classification_evaluation/evidence_adapter.py:327:34\n    |\n325 |             page_reference_quality=page_reference_quality,\n326 |             evidence_metadata=evidence_metadata,\n327 |             processing_timestamp=datetime.now().isoformat()\n    |                                  ^^^^^^^^\n328 |         )\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/L_classification_evaluation/evidence_adapter.py:330:56\n    |\n328 |         )\n329 |\n330 |     def _update_processing_stats(self, evidence_items: List[Dict[str, Any]]) -> None:\n    |                                                        ^^^^\n331 |         \"\"\"Update internal processing statistics.\"\"\"\n332 |         self.processing_stats[\"total_adaptations\"] += 1\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/L_classification_evaluation/evidence_adapter.py:330:61\n    |\n328 |         )\n329 |\n330 |     def _update_processing_stats(self, evidence_items: List[Dict[str, Any]]) -> None:\n    |                                                             ^^^^\n331 |         \"\"\"Update internal processing statistics.\"\"\"\n332 |         self.processing_stats[\"total_adaptations\"] += 1\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/L_classification_evaluation/evidence_adapter.py:330:71\n    |\n328 |         )\n329 |\n330 |     def _update_processing_stats(self, evidence_items: List[Dict[str, Any]]) -> None:\n    |                                                                       ^^^\n331 |         \"\"\"Update internal processing statistics.\"\"\"\n332 |         self.processing_stats[\"total_adaptations\"] += 1\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/L_classification_evaluation/evidence_adapter.py:354:39\n    |\n352 |         return round(value * factor) / factor\n353 |\n354 |     def get_processing_stats(self) -> Dict[str, Any]:\n    |                                       ^^^^\n355 |         \"\"\"Get processing statistics.\"\"\"\n356 |         return OrderedDict([\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/L_classification_evaluation/evidence_adapter.py:354:49\n    |\n352 |         return round(value * factor) / factor\n353 |\n354 |     def get_processing_stats(self) -> Dict[str, Any]:\n    |                                                 ^^^\n355 |         \"\"\"Get processing statistics.\"\"\"\n356 |         return OrderedDict([\n    |\n\nF821 Undefined name `OrderedDict`\n   --> canonical_flow/L_classification_evaluation/evidence_adapter.py:356:16\n    |\n354 |     def get_processing_stats(self) -> Dict[str, Any]:\n355 |         \"\"\"Get processing statistics.\"\"\"\n356 |         return OrderedDict([\n    |                ^^^^^^^^^^^\n357 |             (\"total_adaptations\", self.processing_stats[\"total_adaptations\"]),\n358 |             (\"valid_evidence_count\", self.processing_stats[\"valid_evidence_count\"]),\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/L_classification_evaluation/evidence_adapter.py:363:50\n    |\n361 |         ])\n362 |\n363 |     def serialize_eval_inputs(self, eval_inputs: List[QuestionEvalInput]) -> str:\n    |                                                  ^^^^\n364 |         \"\"\"\n365 |         Serialize evaluation inputs to deterministic JSON.\n    |\n\ninvalid-syntax: Unexpected indentation\n  --> canonical_flow/L_classification_evaluation/question_registry.py:14:1\n   |\n13 | # # # from schemas import (  # Module not found  # Module not found  # Module not found\n14 |     RegistryChecksumValidator, \n   | ^^^^\n15 |     DimensionType, \n16 |     ResponseType,\n   |\n\ninvalid-syntax: Expected a statement\n  --> canonical_flow/L_classification_evaluation/question_registry.py:18:1\n   |\n16 |     ResponseType,\n17 |     QuestionEvalInput\n18 | )\n   | ^\n   |\n\ninvalid-syntax: Expected a statement\n  --> canonical_flow/L_classification_evaluation/question_registry.py:18:2\n   |\n16 |     ResponseType,\n17 |     QuestionEvalInput\n18 | )\n   |  ^\n   |\n\ninvalid-syntax: Expected an indented block after `except` clause\n   --> canonical_flow/L_classification_evaluation/question_registry.py:183:31\n    |\n181 |                 self.dimensions[dimension.dimension_id] = dimension\n182 |                 \n183 |         except Exception as e:\n    |                               ^\n184 | # # #             raise RuntimeError(f\"Failed to load registry from {file_path}: {e}\")  # Module not found  # Module not found  # Mod\u2026\n    |\n\ninvalid-syntax: Expected ',', found name\n   --> canonical_flow/L_classification_evaluation/schemas.py:602:1\n    |\n601 | # Export all schema classes and utilities\n602 | __all__ = [\n    | ^^^^^^^\n603 |     'ResponseValue',\n604 |     'DimensionId', \n    |\n\ninvalid-syntax: unexpected EOF while parsing\n   --> canonical_flow/L_classification_evaluation/schemas.py:619:2\n    |\n617 |     'StageMetaSorted',\n618 |     'reject_unknown_responses'\n619 | ]\n    |  ^\n    |\n\nF821 Undefined name `Path`\n  --> canonical_flow/L_classification_evaluation/score_calculator.py:24:20\n   |\n22 | try:\n23 |     # Add project root to path for imports\n24 |     project_root = Path(__file__).resolve().parents[1]\n   |                    ^^^^\n25 |     if str(project_root) not in sys.path:\n26 |         sys.path.insert(0, str(project_root))\n   |\n\nF821 Undefined name `importlib_util`\n  --> canonical_flow/L_classification_evaluation/score_calculator.py:31:16\n   |\n29 |     original_file = project_root / \"score_calculator.py\"\n30 |     if original_file.exists():\n31 |         spec = importlib_util.spec_from_file_location(\n   |                ^^^^^^^^^^^^^^\n32 |             f\"original_{alias_code.lower()}\", \n33 |             str(original_file)\n   |\n\nF821 Undefined name `importlib_util`\n  --> canonical_flow/L_classification_evaluation/score_calculator.py:37:31\n   |\n36 |         if spec and spec.loader:\n37 |             original_module = importlib_util.module_from_spec(spec)\n   |                               ^^^^^^^^^^^^^^\n38 |             spec.loader.exec_module(original_module)\n   |\n\nF821 Undefined name `e`\n  --> canonical_flow/L_classification_evaluation/score_calculator.py:56:67\n   |\n54 |     def process(data=None, context=None):\n55 |         \"\"\"Placeholder process function for failed import.\"\"\"\n56 |         return {\"error\": f\"Module {alias_source} failed to load: {e}\"}\n   |                                                                   ^\n   |\n\ninvalid-syntax: Expected an indented block after `try` statement\n  --> canonical_flow/L_classification_evaluation/stage_orchestrator.py:33:1\n   |\n31 | # # #     from ..A_analysis_nlp.evidence_validation_model import EvidenceValidationModel as EvidenceValidator  # Module not found  # M\u2026\n32 | # # #     from ..A_analysis_nlp.dnp_alignment_adapter import DNPAlignmentAdapter  # Module not found  # Module not found  # Module not\u2026\n33 | except ImportError:\n   | ^^^^^^\n34 |     # Fallback for standalone execution\n35 | # # #     from decalogo_scoring_system import ScoringSystem, PointScore  # Module not found  # Module not found  # Module not found\n   |\n\nF401 [*] `shutil` imported but unused\n  --> canonical_flow/L_classification_evaluation/test_composition_trace.py:8:8\n   |\n 6 | import json\n 7 | import tempfile\n 8 | import shutil\n   |        ^^^^^^\n 9 | # # # from pathlib import Path  # Module not found  # Module not found  # Module not found\n10 | # # # from decalogo_scoring_system import ScoringSystem, CompositionTrace  # Module not found  # Module not found  # Module not found\n   |\nhelp: Remove unused import: `shutil`\n\nF821 Undefined name `ScoringSystem`\n  --> canonical_flow/L_classification_evaluation/test_composition_trace.py:17:22\n   |\n15 |     print(\"=== COMPOSITION TRACE GENERATION TEST ===\")\n16 |     \n17 |     scoring_system = ScoringSystem(precision=4)\n   |                      ^^^^^^^^^^^^^\n18 |     \n19 |     # Create temporary directory for testing\n   |\n\nF841 Local variable `original_path` is assigned to but never used\n  --> canonical_flow/L_classification_evaluation/test_composition_trace.py:22:9\n   |\n20 |     with tempfile.TemporaryDirectory() as temp_dir:\n21 |         # Override the canonical_flow path for testing\n22 |         original_path = Path(\"canonical_flow/classification\")\n   |         ^^^^^^^^^^^^^\n23 |         test_path = Path(temp_dir) / \"classification\"\n   |\nhelp: Remove assignment to unused variable `original_path`\n\nF821 Undefined name `Path`\n  --> canonical_flow/L_classification_evaluation/test_composition_trace.py:22:25\n   |\n20 |     with tempfile.TemporaryDirectory() as temp_dir:\n21 |         # Override the canonical_flow path for testing\n22 |         original_path = Path(\"canonical_flow/classification\")\n   |                         ^^^^\n23 |         test_path = Path(temp_dir) / \"classification\"\n   |\n\nF841 Local variable `test_path` is assigned to but never used\n  --> canonical_flow/L_classification_evaluation/test_composition_trace.py:23:9\n   |\n21 |         # Override the canonical_flow path for testing\n22 |         original_path = Path(\"canonical_flow/classification\")\n23 |         test_path = Path(temp_dir) / \"classification\"\n   |         ^^^^^^^^^\n24 |         \n25 |         # Mock the path by temporarily changing working directory context\n   |\nhelp: Remove assignment to unused variable `test_path`\n\nF821 Undefined name `Path`\n  --> canonical_flow/L_classification_evaluation/test_composition_trace.py:23:21\n   |\n21 |         # Override the canonical_flow path for testing\n22 |         original_path = Path(\"canonical_flow/classification\")\n23 |         test_path = Path(temp_dir) / \"classification\"\n   |                     ^^^^\n24 |         \n25 |         # Mock the path by temporarily changing working directory context\n   |\n\nF821 Undefined name `Decimal`\n  --> canonical_flow/L_classification_evaluation/test_composition_trace.py:61:40\n   |\n60 | # # #         from decimal import Decimal  # Module not found  # Module not found  # Module not found\n61 |         dimension_scores_decimal = {k: Decimal(str(v)) for k, v in dimension_scores.items()}\n   |                                        ^^^^^^^\n62 |         final_score, composition_trace = scoring_system.compose_point_score(dimension_scores_decimal)\n   |\n\nF541 [*] f-string without any placeholders\n  --> canonical_flow/L_classification_evaluation/test_composition_trace.py:64:15\n   |\n62 |         final_score, composition_trace = scoring_system.compose_point_score(dimension_scores_decimal)\n63 |         \n64 |         print(f\"\u2713 Composition trace generated\")\n   |               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n65 |         print(f\"  Weighted sum: {composition_trace.weighted_sum:.4f}\")\n66 |         print(f\"  Total weight: {composition_trace.total_weight:.4f}\")\n   |\nhelp: Remove extraneous `f` prefix\n\nF821 Undefined name `ScoringSystem`\n  --> canonical_flow/L_classification_evaluation/test_composition_trace.py:76:22\n   |\n74 |     print(\"\\n=== TRACE FILE STRUCTURE TEST ===\")\n75 |     \n76 |     scoring_system = ScoringSystem()\n   |                      ^^^^^^^^^^^^^\n77 |     \n78 |     evaluation_data = {\n   |\n\nF841 Local variable `point_score` is assigned to but never used\n  --> canonical_flow/L_classification_evaluation/test_composition_trace.py:94:5\n   |\n93 |     # Process with trace saving enabled\n94 |     point_score = scoring_system.process_point_evaluation(\n   |     ^^^^^^^^^^^\n95 |         point_id=5, \n96 |         evaluation_data=evaluation_data,\n   |\nhelp: Remove assignment to unused variable `point_score`\n\nF821 Undefined name `Path`\n   --> canonical_flow/L_classification_evaluation/test_composition_trace.py:102:18\n    |\n101 |     # Check that trace file was created\n102 |     trace_file = Path(\"canonical_flow/classification/TEST_DOC_002/P5_composition.json\")\n    |                  ^^^^\n103 |     \n104 |     if trace_file.exists():\n    |\n\nF541 [*] f-string without any placeholders\n   --> canonical_flow/L_classification_evaluation/test_composition_trace.py:134:19\n    |\n132 |         dimensions = trace_data.get(\"dimension_details\", [])\n133 |         if len(dimensions) == 4:\n134 |             print(f\"  \u2713 All 4 dimensions present in trace\")\n    |                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n135 |         else:\n136 |             print(f\"  \u2717 Expected 4 dimensions, found {len(dimensions)}\")\n    |\nhelp: Remove extraneous `f` prefix\n\nF821 Undefined name `ScoringSystem`\n   --> canonical_flow/L_classification_evaluation/test_composition_trace.py:157:22\n    |\n155 |     print(\"\\n=== MATHEMATICAL TRANSPARENCY TEST ===\")\n156 |     \n157 |     scoring_system = ScoringSystem(precision=6)\n    |                      ^^^^^^^^^^^^^\n158 |     \n159 |     # Simple evaluation data for precise testing\n    |\n\nF841 Local variable `point_score` is assigned to but never used\n   --> canonical_flow/L_classification_evaluation/test_composition_trace.py:175:5\n    |\n173 |     }\n174 |     \n175 |     point_score = scoring_system.process_point_evaluation(\n    |     ^^^^^^^^^^^\n176 |         point_id=7, \n177 |         evaluation_data=evaluation_data,\n    |\nhelp: Remove assignment to unused variable `point_score`\n\nF821 Undefined name `Path`\n   --> canonical_flow/L_classification_evaluation/test_composition_trace.py:183:18\n    |\n182 |     # Load trace and manually verify calculations\n183 |     trace_file = Path(\"canonical_flow/classification/MATH_TEST_DOC/P7_composition.json\")\n    |                  ^^^^\n184 |     \n185 |     if trace_file.exists():\n    |\n\nF821 Undefined name `ScoringSystem`\n   --> canonical_flow/L_classification_evaluation/test_composition_trace.py:242:22\n    |\n240 |     print(\"\\n=== DETERMINISTIC SERIALIZATION TEST ===\")\n241 |     \n242 |     scoring_system = ScoringSystem()\n    |                      ^^^^^^^^^^^^^\n243 |     \n244 |     evaluation_data = {\n    |\n\nF821 Undefined name `Path`\n   --> canonical_flow/L_classification_evaluation/test_composition_trace.py:261:28\n    |\n259 |             save_trace=True\n260 |         )\n261 |         trace_files.append(Path(f\"canonical_flow/classification/{doc_id}/P9_composition.json\"))\n    |                            ^^^^\n262 |     \n263 |     # Compare serializations (excluding timestamp and document_id)\n    |\n\ninvalid-syntax: Unexpected indentation\n  --> canonical_flow/L_classification_evaluation/test_conformal_prediction.py:14:1\n   |\n13 | # # # from .conformal_prediction import (  # Module not found  # Module not found  # Module not found\n14 |     ConformalPredictor,\n   | ^^^^\n15 |     ConformalInterval,\n16 |     ConformalPrediction,\n   |\n\ninvalid-syntax: Expected a statement\n  --> canonical_flow/L_classification_evaluation/test_conformal_prediction.py:20:1\n   |\n18 |     DocumentConformalResults,\n19 |     generate_sample_calibration_data\n20 | )\n   | ^\n21 | # # # from .decalogo_scoring_system import ScoringSystem  # Module not found  # Module not found  # Module not found\n   |\n\ninvalid-syntax: Expected a statement\n  --> canonical_flow/L_classification_evaluation/test_conformal_prediction.py:20:2\n   |\n18 |     DocumentConformalResults,\n19 |     generate_sample_calibration_data\n20 | )\n   |  ^\n21 | # # # from .decalogo_scoring_system import ScoringSystem  # Module not found  # Module not found  # Module not found\n   |\n\ninvalid-syntax: Unexpected indentation\n  --> canonical_flow/L_classification_evaluation/test_evidence_adapter.py:9:1\n   |\n 7 | import json\n 8 | # # # from evidence_adapter import (  # Module not found  # Module not found  # Module not found\n 9 |     EvidenceAdapter, \n   | ^^^^\n10 |     QuestionEvalInput, \n11 |     EvidenceQualityMetrics,\n   |\n\ninvalid-syntax: Expected a statement\n  --> canonical_flow/L_classification_evaluation/test_evidence_adapter.py:13:1\n   |\n11 |     EvidenceQualityMetrics,\n12 |     create_evidence_adapter\n13 | )\n   | ^\n   |\n\ninvalid-syntax: Expected a statement\n  --> canonical_flow/L_classification_evaluation/test_evidence_adapter.py:13:2\n   |\n11 |     EvidenceQualityMetrics,\n12 |     create_evidence_adapter\n13 | )\n   |  ^\n   |\n\ninvalid-syntax: Unexpected indentation\n  --> canonical_flow/L_classification_evaluation/test_schemas.py:12:1\n   |\n11 | # # # from .schemas import (  # Module not found  # Module not found  # Module not found\n12 |     QuestionEvalInput, DimensionEvalOutput, PointEvalOutput, StageMeta,\n   | ^^^^\n13 |     ResponseValue, DimensionId, validate_input_schema, validate_output_schema,\n14 |     validate_both_schemas, reject_unknown_responses, ValidationResult,\n   |\n\ninvalid-syntax: Expected a statement\n  --> canonical_flow/L_classification_evaluation/test_schemas.py:16:1\n   |\n14 |     validate_both_schemas, reject_unknown_responses, ValidationResult,\n15 |     DeterministicSortingMixin\n16 | )\n   | ^\n   |\n\ninvalid-syntax: Expected a statement\n  --> canonical_flow/L_classification_evaluation/test_schemas.py:16:2\n   |\n14 |     validate_both_schemas, reject_unknown_responses, ValidationResult,\n15 |     DeterministicSortingMixin\n16 | )\n   |  ^\n   |\n\nF821 Undefined name `Path`\n  --> canonical_flow/L_classification_evaluation/test_stage_orchestrator.py:15:24\n   |\n13 | import sys\n14 | # # # from pathlib import Path  # Module not found  # Module not found  # Module not found\n15 | sys.path.insert(0, str(Path(__file__).resolve().parent))\n   |                        ^^^^\n16 | # # # from stage_orchestrator import LClassificationStageOrchestrator, process  # Module not found  # Module not found  # Module not f\u2026\n   |\n\nF821 Undefined name `LClassificationStageOrchestrator`\n  --> canonical_flow/L_classification_evaluation/test_stage_orchestrator.py:24:29\n   |\n22 |     def setUp(self):\n23 |         \"\"\"Set up test fixtures\"\"\"\n24 |         self.orchestrator = LClassificationStageOrchestrator()\n   |                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n25 |         self.temp_dir = Path(tempfile.mkdtemp())\n   |\n\nF821 Undefined name `Path`\n  --> canonical_flow/L_classification_evaluation/test_stage_orchestrator.py:25:25\n   |\n23 |         \"\"\"Set up test fixtures\"\"\"\n24 |         self.orchestrator = LClassificationStageOrchestrator()\n25 |         self.temp_dir = Path(tempfile.mkdtemp())\n   |                         ^^^^\n26 |         \n27 |         # Create sample input files\n   |\n\nF821 Undefined name `LClassificationStageOrchestrator`\n  --> canonical_flow/L_classification_evaluation/test_stage_orchestrator.py:92:16\n   |\n90 |     def test_orchestrator_initialization(self):\n91 |         \"\"\"Test orchestrator initialization\"\"\"\n92 |         orch = LClassificationStageOrchestrator(precision=2)\n   |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n93 |         \n94 |         self.assertEqual(orch.precision, 2)\n   |\n\nF821 Undefined name `Path`\n   --> canonical_flow/L_classification_evaluation/test_stage_orchestrator.py:132:21\n    |\n130 |     def test_extract_point_id(self):\n131 | # # #         \"\"\"Test extracting point ID from file path\"\"\"  # Module not found  # Module not found  # Module not found\n132 |         file_path = Path(\"classification_input/doc1/P5_questions.json\")\n    |                     ^^^^\n133 |         point_id = self.orchestrator._extract_point_id(file_path)\n    |\n\nF821 Undefined name `Path`\n   --> canonical_flow/L_classification_evaluation/test_stage_orchestrator.py:139:49\n    |\n137 |         # Test invalid format\n138 |         with self.assertRaises(ValueError):\n139 |             self.orchestrator._extract_point_id(Path(\"invalid_file.json\"))\n    |                                                 ^^^^\n140 |     \n141 |     def test_convert_to_evaluation_format(self):\n    |\n\nF821 Undefined name `patch`\n   --> canonical_flow/L_classification_evaluation/test_stage_orchestrator.py:192:6\n    |\n190 |             self.assertEqual(artifacts[artifact_type][\"point_id\"], 1)\n191 |     \n192 |     @patch.object(LClassificationStageOrchestrator, '_execute_evidence_validation')\n    |      ^^^^^\n193 |     @patch.object(LClassificationStageOrchestrator, '_execute_dnp_alignment')\n194 |     def test_process_point_success(self, mock_dnp, mock_evidence):\n    |\n\nF821 Undefined name `LClassificationStageOrchestrator`\n   --> canonical_flow/L_classification_evaluation/test_stage_orchestrator.py:192:19\n    |\n190 |             self.assertEqual(artifacts[artifact_type][\"point_id\"], 1)\n191 |     \n192 |     @patch.object(LClassificationStageOrchestrator, '_execute_evidence_validation')\n    |                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n193 |     @patch.object(LClassificationStageOrchestrator, '_execute_dnp_alignment')\n194 |     def test_process_point_success(self, mock_dnp, mock_evidence):\n    |\n\nF821 Undefined name `patch`\n   --> canonical_flow/L_classification_evaluation/test_stage_orchestrator.py:193:6\n    |\n192 |     @patch.object(LClassificationStageOrchestrator, '_execute_evidence_validation')\n193 |     @patch.object(LClassificationStageOrchestrator, '_execute_dnp_alignment')\n    |      ^^^^^\n194 |     def test_process_point_success(self, mock_dnp, mock_evidence):\n195 |         \"\"\"Test successful processing of a single point\"\"\"\n    |\n\nF821 Undefined name `LClassificationStageOrchestrator`\n   --> canonical_flow/L_classification_evaluation/test_stage_orchestrator.py:193:19\n    |\n192 |     @patch.object(LClassificationStageOrchestrator, '_execute_evidence_validation')\n193 |     @patch.object(LClassificationStageOrchestrator, '_execute_dnp_alignment')\n    |                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n194 |     def test_process_point_success(self, mock_dnp, mock_evidence):\n195 |         \"\"\"Test successful processing of a single point\"\"\"\n    |\n\nF821 Undefined name `patch`\n   --> canonical_flow/L_classification_evaluation/test_stage_orchestrator.py:225:14\n    |\n223 |         \"\"\"Test point processing with component failure\"\"\"\n224 |         # Create orchestrator with faulty evidence validator\n225 |         with patch.object(self.orchestrator, '_execute_evidence_validation') as mock_evidence:\n    |              ^^^^^\n226 |             mock_evidence.side_effect = Exception(\"Evidence validation failed\")\n    |\n\nF821 Undefined name `patch`\n   --> canonical_flow/L_classification_evaluation/test_stage_orchestrator.py:231:6\n    |\n229 |                 self.orchestrator._process_point(self.input_files[0], 1)\n230 |     \n231 |     @patch.object(LClassificationStageOrchestrator, '_execute_evidence_validation')\n    |      ^^^^^\n232 |     @patch.object(LClassificationStageOrchestrator, '_execute_dnp_alignment')\n233 |     def test_full_process_api(self, mock_dnp, mock_evidence):\n    |\n\nF821 Undefined name `LClassificationStageOrchestrator`\n   --> canonical_flow/L_classification_evaluation/test_stage_orchestrator.py:231:19\n    |\n229 |                 self.orchestrator._process_point(self.input_files[0], 1)\n230 |     \n231 |     @patch.object(LClassificationStageOrchestrator, '_execute_evidence_validation')\n    |                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n232 |     @patch.object(LClassificationStageOrchestrator, '_execute_dnp_alignment')\n233 |     def test_full_process_api(self, mock_dnp, mock_evidence):\n    |\n\nF821 Undefined name `patch`\n   --> canonical_flow/L_classification_evaluation/test_stage_orchestrator.py:232:6\n    |\n231 |     @patch.object(LClassificationStageOrchestrator, '_execute_evidence_validation')\n232 |     @patch.object(LClassificationStageOrchestrator, '_execute_dnp_alignment')\n    |      ^^^^^\n233 |     def test_full_process_api(self, mock_dnp, mock_evidence):\n234 |         \"\"\"Test the complete process() API contract\"\"\"\n    |\n\nF821 Undefined name `LClassificationStageOrchestrator`\n   --> canonical_flow/L_classification_evaluation/test_stage_orchestrator.py:232:19\n    |\n231 |     @patch.object(LClassificationStageOrchestrator, '_execute_evidence_validation')\n232 |     @patch.object(LClassificationStageOrchestrator, '_execute_dnp_alignment')\n    |                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n233 |     def test_full_process_api(self, mock_dnp, mock_evidence):\n234 |         \"\"\"Test the complete process() API contract\"\"\"\n    |\n\nF821 Undefined name `patch`\n   --> canonical_flow/L_classification_evaluation/test_stage_orchestrator.py:298:14\n    |\n297 |         # Mock components to work for valid data\n298 |         with patch.object(self.orchestrator, '_execute_evidence_validation') as mock_evidence:\n    |              ^^^^^\n299 |             with patch.object(self.orchestrator, '_execute_dnp_alignment') as mock_dnp:\n300 |                 mock_evidence.return_value = {\"validation_status\": \"completed\"}\n    |\n\nF821 Undefined name `patch`\n   --> canonical_flow/L_classification_evaluation/test_stage_orchestrator.py:299:18\n    |\n297 |         # Mock components to work for valid data\n298 |         with patch.object(self.orchestrator, '_execute_evidence_validation') as mock_evidence:\n299 |             with patch.object(self.orchestrator, '_execute_dnp_alignment') as mock_dnp:\n    |                  ^^^^^\n300 |                 mock_evidence.return_value = {\"validation_status\": \"completed\"}\n301 |                 mock_dnp.return_value = {\"alignment_status\": \"completed\"}\n    |\n\nF821 Undefined name `patch`\n   --> canonical_flow/L_classification_evaluation/test_stage_orchestrator.py:314:14\n    |\n312 |     def test_deterministic_ordering(self):\n313 |         \"\"\"Test that results maintain deterministic ordering\"\"\"\n314 |         with patch.object(self.orchestrator, '_execute_evidence_validation') as mock_evidence:\n    |              ^^^^^\n315 |             with patch.object(self.orchestrator, '_execute_dnp_alignment') as mock_dnp:\n316 |                 mock_evidence.return_value = {\"validation_status\": \"completed\"}\n    |\n\nF821 Undefined name `patch`\n   --> canonical_flow/L_classification_evaluation/test_stage_orchestrator.py:315:18\n    |\n313 |         \"\"\"Test that results maintain deterministic ordering\"\"\"\n314 |         with patch.object(self.orchestrator, '_execute_evidence_validation') as mock_evidence:\n315 |             with patch.object(self.orchestrator, '_execute_dnp_alignment') as mock_dnp:\n    |                  ^^^^^\n316 |                 mock_evidence.return_value = {\"validation_status\": \"completed\"}\n317 |                 mock_dnp.return_value = {\"alignment_status\": \"completed\"}\n    |\n\nF821 Undefined name `LClassificationStageOrchestrator`\n   --> canonical_flow/L_classification_evaluation/test_stage_orchestrator.py:325:33\n    |\n324 |                 # Create new orchestrator to ensure independence\n325 |                 orchestrator2 = LClassificationStageOrchestrator()\n    |                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n326 |                 results2 = orchestrator2.process(\n327 |                     data=str(self.temp_dir / \"classification_input\")\n    |\n\nF821 Undefined name `patch`\n   --> canonical_flow/L_classification_evaluation/test_stage_orchestrator.py:426:14\n    |\n424 |     def test_standalone_process_function(self):\n425 |         \"\"\"Test the standalone process() function\"\"\"\n426 |         with patch.object(LClassificationStageOrchestrator, 'process') as mock_process:\n    |              ^^^^^\n427 |             mock_process.return_value = {\"test\": \"result\"}\n    |\n\nF821 Undefined name `LClassificationStageOrchestrator`\n   --> canonical_flow/L_classification_evaluation/test_stage_orchestrator.py:426:27\n    |\n424 |     def test_standalone_process_function(self):\n425 |         \"\"\"Test the standalone process() function\"\"\"\n426 |         with patch.object(LClassificationStageOrchestrator, 'process') as mock_process:\n    |                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n427 |             mock_process.return_value = {\"test\": \"result\"}\n    |\n\nF821 Undefined name `process`\n   --> canonical_flow/L_classification_evaluation/test_stage_orchestrator.py:429:22\n    |\n427 |             mock_process.return_value = {\"test\": \"result\"}\n428 |             \n429 |             result = process(data=\"test_data\", context=\"test_context\")\n    |                      ^^^^^^^\n430 |             \n431 |             # Should create orchestrator and call process\n    |\n\nF821 Undefined name `patch`\n   --> canonical_flow/L_classification_evaluation/test_stage_orchestrator.py:445:14\n    |\n443 |             return original_execute_scoring(point_id, question_data)\n444 |         \n445 |         with patch.object(self.orchestrator, '_execute_evidence_validation') as mock_evidence:\n    |              ^^^^^\n446 |             with patch.object(self.orchestrator, '_execute_dnp_alignment') as mock_dnp:\n447 |                 with patch.object(self.orchestrator, '_execute_scoring_system', side_effect=failing_execute_scoring):\n    |\n\nF821 Undefined name `patch`\n   --> canonical_flow/L_classification_evaluation/test_stage_orchestrator.py:446:18\n    |\n445 |         with patch.object(self.orchestrator, '_execute_evidence_validation') as mock_evidence:\n446 |             with patch.object(self.orchestrator, '_execute_dnp_alignment') as mock_dnp:\n    |                  ^^^^^\n447 |                 with patch.object(self.orchestrator, '_execute_scoring_system', side_effect=failing_execute_scoring):\n448 |                     mock_evidence.return_value = {\"validation_status\": \"completed\"}\n    |\n\nF821 Undefined name `patch`\n   --> canonical_flow/L_classification_evaluation/test_stage_orchestrator.py:447:22\n    |\n445 |         with patch.object(self.orchestrator, '_execute_evidence_validation') as mock_evidence:\n446 |             with patch.object(self.orchestrator, '_execute_dnp_alignment') as mock_dnp:\n447 |                 with patch.object(self.orchestrator, '_execute_scoring_system', side_effect=failing_execute_scoring):\n    |                      ^^^^^\n448 |                     mock_evidence.return_value = {\"validation_status\": \"completed\"}\n449 |                     mock_dnp.return_value = {\"alignment_status\": \"completed\"}\n    |\n\nF821 Undefined name `Path`\n  --> canonical_flow/L_classification_evaluation/test_thresholds_integration.py:12:24\n   |\n11 | # Add paths to import modules\n12 | sys.path.insert(0, str(Path(__file__).parent.parent.parent))\n   |                        ^^^^\n13 | sys.path.insert(0, str(Path(__file__).parent))\n   |\n\nF821 Undefined name `Path`\n  --> canonical_flow/L_classification_evaluation/test_thresholds_integration.py:13:24\n   |\n11 | # Add paths to import modules\n12 | sys.path.insert(0, str(Path(__file__).parent.parent.parent))\n13 | sys.path.insert(0, str(Path(__file__).parent))\n   |                        ^^^^\n14 |\n15 | # # # from decalogo_scoring_system import ScoringSystem  # Module not found  # Module not found  # Module not found\n   |\n\nF821 Undefined name `ScoringSystem`\n  --> canonical_flow/L_classification_evaluation/test_thresholds_integration.py:26:26\n   |\n24 |     # Initialize both scoring systems\n25 |     try:\n26 |         scoring_system = ScoringSystem()\n   |                          ^^^^^^^^^^^^^\n27 |         adaptive_engine = AdaptiveScoringEngine(models_path=\"test_models\")\n   |\n\nF821 Undefined name `AdaptiveScoringEngine`\n  --> canonical_flow/L_classification_evaluation/test_thresholds_integration.py:27:27\n   |\n25 |     try:\n26 |         scoring_system = ScoringSystem()\n27 |         adaptive_engine = AdaptiveScoringEngine(models_path=\"test_models\")\n   |                           ^^^^^^^^^^^^^^^^^^^^^\n28 |         \n29 |         print(\"\u2713 Both systems initialized successfully\")\n   |\n\nF821 Undefined name `Path`\n   --> canonical_flow/L_classification_evaluation/test_thresholds_integration.py:100:27\n    |\n 98 |     # Test 4: Schema validation\n 99 |     try:\n100 |         thresholds_file = Path(__file__).parent.parent / \"thresholds.json\"\n    |                           ^^^^\n101 |         \n102 |         if thresholds_file.exists():\n    |\n\nF821 Undefined name `AdaptiveScoringEngine`\n   --> canonical_flow/L_classification_evaluation/test_thresholds_integration.py:171:27\n    |\n170 |     try:\n171 |         adaptive_engine = AdaptiveScoringEngine(models_path=\"test_models\")\n    |                           ^^^^^^^^^^^^^^^^^^^^^\n172 |         \n173 |         # Mock correction data\n    |\n\nF821 Undefined name `Path`\n   --> canonical_flow/L_classification_evaluation/test_thresholds_integration.py:236:29\n    |\n234 |         # Test artifact file creation\n235 |         if \"artifact_location\" in serialized:\n236 |             artifact_path = Path(serialized[\"artifact_location\"])\n    |                             ^^^^\n237 |             if artifact_path.exists():\n238 |                 print(f\"\u2713 Artifact file created: {artifact_path}\")\n    |\n\nF821 Undefined name `Path`\n  --> canonical_flow/O_orchestration_control/adaptive_controller.py:24:20\n   |\n22 | try:\n23 |     # Add project root to path for imports\n24 |     project_root = Path(__file__).resolve().parents[1]\n   |                    ^^^^\n25 |     if str(project_root) not in sys.path:\n26 |         sys.path.insert(0, str(project_root))\n   |\n\nF821 Undefined name `importlib_util`\n  --> canonical_flow/O_orchestration_control/adaptive_controller.py:31:16\n   |\n29 |     original_file = project_root / \"adaptive_controller.py\"\n30 |     if original_file.exists():\n31 |         spec = importlib_util.spec_from_file_location(\n   |                ^^^^^^^^^^^^^^\n32 |             f\"original_{alias_code.lower()}\", \n33 |             str(original_file)\n   |\n\nF821 Undefined name `importlib_util`\n  --> canonical_flow/O_orchestration_control/adaptive_controller.py:37:31\n   |\n36 |         if spec and spec.loader:\n37 |             original_module = importlib_util.module_from_spec(spec)\n   |                               ^^^^^^^^^^^^^^\n38 |             spec.loader.exec_module(original_module)\n   |\n\nF821 Undefined name `e`\n  --> canonical_flow/O_orchestration_control/adaptive_controller.py:56:67\n   |\n54 |     def process(data=None, context=None):\n55 |         \"\"\"Placeholder process function for failed import.\"\"\"\n56 |         return {\"error\": f\"Module {alias_source} failed to load: {e}\"}\n   |                                                                   ^\n   |\n\nF821 Undefined name `Path`\n  --> canonical_flow/O_orchestration_control/airflow_orchestrator.py:24:20\n   |\n22 | try:\n23 |     # Add project root to path for imports\n24 |     project_root = Path(__file__).resolve().parents[1]\n   |                    ^^^^\n25 |     if str(project_root) not in sys.path:\n26 |         sys.path.insert(0, str(project_root))\n   |\n\nF821 Undefined name `importlib_util`\n  --> canonical_flow/O_orchestration_control/airflow_orchestrator.py:31:16\n   |\n29 |     original_file = project_root / \"airflow_orchestrator.py\"\n30 |     if original_file.exists():\n31 |         spec = importlib_util.spec_from_file_location(\n   |                ^^^^^^^^^^^^^^\n32 |             f\"original_{alias_code.lower()}\", \n33 |             str(original_file)\n   |\n\nF821 Undefined name `importlib_util`\n  --> canonical_flow/O_orchestration_control/airflow_orchestrator.py:37:31\n   |\n36 |         if spec and spec.loader:\n37 |             original_module = importlib_util.module_from_spec(spec)\n   |                               ^^^^^^^^^^^^^^\n38 |             spec.loader.exec_module(original_module)\n   |\n\nF821 Undefined name `e`\n  --> canonical_flow/O_orchestration_control/airflow_orchestrator.py:56:67\n   |\n54 |     def process(data=None, context=None):\n55 |         \"\"\"Placeholder process function for failed import.\"\"\"\n56 |         return {\"error\": f\"Module {alias_source} failed to load: {e}\"}\n   |                                                                   ^\n   |\n\nF821 Undefined name `Path`\n  --> canonical_flow/O_orchestration_control/alert_system.py:24:20\n   |\n22 | try:\n23 |     # Add project root to path for imports\n24 |     project_root = Path(__file__).resolve().parents[1]\n   |                    ^^^^\n25 |     if str(project_root) not in sys.path:\n26 |         sys.path.insert(0, str(project_root))\n   |\n\nF821 Undefined name `importlib_util`\n  --> canonical_flow/O_orchestration_control/alert_system.py:31:16\n   |\n29 |     original_file = project_root / \"alert_system.py\"\n30 |     if original_file.exists():\n31 |         spec = importlib_util.spec_from_file_location(\n   |                ^^^^^^^^^^^^^^\n32 |             f\"original_{alias_code.lower()}\", \n33 |             str(original_file)\n   |\n\nF821 Undefined name `importlib_util`\n  --> canonical_flow/O_orchestration_control/alert_system.py:37:31\n   |\n36 |         if spec and spec.loader:\n37 |             original_module = importlib_util.module_from_spec(spec)\n   |                               ^^^^^^^^^^^^^^\n38 |             spec.loader.exec_module(original_module)\n   |\n\nF821 Undefined name `e`\n  --> canonical_flow/O_orchestration_control/alert_system.py:56:67\n   |\n54 |     def process(data=None, context=None):\n55 |         \"\"\"Placeholder process function for failed import.\"\"\"\n56 |         return {\"error\": f\"Module {alias_source} failed to load: {e}\"}\n   |                                                                   ^\n   |\n\nF821 Undefined name `Path`\n  --> canonical_flow/O_orchestration_control/backpressure_manager.py:24:20\n   |\n22 | try:\n23 |     # Add project root to path for imports\n24 |     project_root = Path(__file__).resolve().parents[1]\n   |                    ^^^^\n25 |     if str(project_root) not in sys.path:\n26 |         sys.path.insert(0, str(project_root))\n   |\n\nF821 Undefined name `importlib_util`\n  --> canonical_flow/O_orchestration_control/backpressure_manager.py:31:16\n   |\n29 |     original_file = project_root / \"backpressure_manager.py\"\n30 |     if original_file.exists():\n31 |         spec = importlib_util.spec_from_file_location(\n   |                ^^^^^^^^^^^^^^\n32 |             f\"original_{alias_code.lower()}\", \n33 |             str(original_file)\n   |\n\nF821 Undefined name `importlib_util`\n  --> canonical_flow/O_orchestration_control/backpressure_manager.py:37:31\n   |\n36 |         if spec and spec.loader:\n37 |             original_module = importlib_util.module_from_spec(spec)\n   |                               ^^^^^^^^^^^^^^\n38 |             spec.loader.exec_module(original_module)\n   |\n\nF821 Undefined name `e`\n  --> canonical_flow/O_orchestration_control/backpressure_manager.py:56:67\n   |\n54 |     def process(data=None, context=None):\n55 |         \"\"\"Placeholder process function for failed import.\"\"\"\n56 |         return {\"error\": f\"Module {alias_source} failed to load: {e}\"}\n   |                                                                   ^\n   |\n\nF821 Undefined name `Path`\n  --> canonical_flow/O_orchestration_control/circuit_breaker.py:24:20\n   |\n22 | try:\n23 |     # Add project root to path for imports\n24 |     project_root = Path(__file__).resolve().parents[1]\n   |                    ^^^^\n25 |     if str(project_root) not in sys.path:\n26 |         sys.path.insert(0, str(project_root))\n   |\n\nF821 Undefined name `importlib_util`\n  --> canonical_flow/O_orchestration_control/circuit_breaker.py:31:16\n   |\n29 |     original_file = project_root / \"circuit_breaker.py\"\n30 |     if original_file.exists():\n31 |         spec = importlib_util.spec_from_file_location(\n   |                ^^^^^^^^^^^^^^\n32 |             f\"original_{alias_code.lower()}\", \n33 |             str(original_file)\n   |\n\nF821 Undefined name `importlib_util`\n  --> canonical_flow/O_orchestration_control/circuit_breaker.py:37:31\n   |\n36 |         if spec and spec.loader:\n37 |             original_module = importlib_util.module_from_spec(spec)\n   |                               ^^^^^^^^^^^^^^\n38 |             spec.loader.exec_module(original_module)\n   |\n\nF821 Undefined name `e`\n  --> canonical_flow/O_orchestration_control/circuit_breaker.py:56:67\n   |\n54 |     def process(data=None, context=None):\n55 |         \"\"\"Placeholder process function for failed import.\"\"\"\n56 |         return {\"error\": f\"Module {alias_source} failed to load: {e}\"}\n   |                                                                   ^\n   |\n\nF821 Undefined name `Path`\n  --> canonical_flow/O_orchestration_control/confluent_orchestrator.py:24:20\n   |\n22 | try:\n23 |     # Add project root to path for imports\n24 |     project_root = Path(__file__).resolve().parents[1]\n   |                    ^^^^\n25 |     if str(project_root) not in sys.path:\n26 |         sys.path.insert(0, str(project_root))\n   |\n\nF821 Undefined name `importlib_util`\n  --> canonical_flow/O_orchestration_control/confluent_orchestrator.py:31:16\n   |\n29 |     original_file = project_root / \"confluent_orchestrator.py\"\n30 |     if original_file.exists():\n31 |         spec = importlib_util.spec_from_file_location(\n   |                ^^^^^^^^^^^^^^\n32 |             f\"original_{alias_code.lower()}\", \n33 |             str(original_file)\n   |\n\nF821 Undefined name `importlib_util`\n  --> canonical_flow/O_orchestration_control/confluent_orchestrator.py:37:31\n   |\n36 |         if spec and spec.loader:\n37 |             original_module = importlib_util.module_from_spec(spec)\n   |                               ^^^^^^^^^^^^^^\n38 |             spec.loader.exec_module(original_module)\n   |\n\nF821 Undefined name `e`\n  --> canonical_flow/O_orchestration_control/confluent_orchestrator.py:56:67\n   |\n54 |     def process(data=None, context=None):\n55 |         \"\"\"Placeholder process function for failed import.\"\"\"\n56 |         return {\"error\": f\"Module {alias_source} failed to load: {e}\"}\n   |                                                                   ^\n   |\n\nF821 Undefined name `Path`\n  --> canonical_flow/O_orchestration_control/constraint_validator.py:24:20\n   |\n22 | try:\n23 |     # Add project root to path for imports\n24 |     project_root = Path(__file__).resolve().parents[1]\n   |                    ^^^^\n25 |     if str(project_root) not in sys.path:\n26 |         sys.path.insert(0, str(project_root))\n   |\n\nF821 Undefined name `importlib_util`\n  --> canonical_flow/O_orchestration_control/constraint_validator.py:31:16\n   |\n29 |     original_file = project_root / \"constraint_validator.py\"\n30 |     if original_file.exists():\n31 |         spec = importlib_util.spec_from_file_location(\n   |                ^^^^^^^^^^^^^^\n32 |             f\"original_{alias_code.lower()}\", \n33 |             str(original_file)\n   |\n\nF821 Undefined name `importlib_util`\n  --> canonical_flow/O_orchestration_control/constraint_validator.py:37:31\n   |\n36 |         if spec and spec.loader:\n37 |             original_module = importlib_util.module_from_spec(spec)\n   |                               ^^^^^^^^^^^^^^\n38 |             spec.loader.exec_module(original_module)\n   |\n\nF821 Undefined name `e`\n  --> canonical_flow/O_orchestration_control/constraint_validator.py:56:67\n   |\n54 |     def process(data=None, context=None):\n55 |         \"\"\"Placeholder process function for failed import.\"\"\"\n56 |         return {\"error\": f\"Module {alias_source} failed to load: {e}\"}\n   |                                                                   ^\n   |\n\nF821 Undefined name `Path`\n  --> canonical_flow/O_orchestration_control/contract_validator.py:24:20\n   |\n22 | try:\n23 |     # Add project root to path for imports\n24 |     project_root = Path(__file__).resolve().parents[1]\n   |                    ^^^^\n25 |     if str(project_root) not in sys.path:\n26 |         sys.path.insert(0, str(project_root))\n   |\n\nF821 Undefined name `importlib_util`\n  --> canonical_flow/O_orchestration_control/contract_validator.py:31:16\n   |\n29 |     original_file = project_root / \"contract_validator.py\"\n30 |     if original_file.exists():\n31 |         spec = importlib_util.spec_from_file_location(\n   |                ^^^^^^^^^^^^^^\n32 |             f\"original_{alias_code.lower()}\", \n33 |             str(original_file)\n   |\n\nF821 Undefined name `importlib_util`\n  --> canonical_flow/O_orchestration_control/contract_validator.py:37:31\n   |\n36 |         if spec and spec.loader:\n37 |             original_module = importlib_util.module_from_spec(spec)\n   |                               ^^^^^^^^^^^^^^\n38 |             spec.loader.exec_module(original_module)\n   |\n\nF821 Undefined name `e`\n  --> canonical_flow/O_orchestration_control/contract_validator.py:56:67\n   |\n54 |     def process(data=None, context=None):\n55 |         \"\"\"Placeholder process function for failed import.\"\"\"\n56 |         return {\"error\": f\"Module {alias_source} failed to load: {e}\"}\n   |                                                                   ^\n   |\n\nF821 Undefined name `Path`\n  --> canonical_flow/O_orchestration_control/core_orchestrator.py:24:20\n   |\n22 | try:\n23 |     # Add project root to path for imports\n24 |     project_root = Path(__file__).resolve().parents[1]\n   |                    ^^^^\n25 |     if str(project_root) not in sys.path:\n26 |         sys.path.insert(0, str(project_root))\n   |\n\nF821 Undefined name `importlib_util`\n  --> canonical_flow/O_orchestration_control/core_orchestrator.py:31:16\n   |\n29 |     original_file = project_root / \"core_orchestrator.py\"\n30 |     if original_file.exists():\n31 |         spec = importlib_util.spec_from_file_location(\n   |                ^^^^^^^^^^^^^^\n32 |             f\"original_{alias_code.lower()}\", \n33 |             str(original_file)\n   |\n\nF821 Undefined name `importlib_util`\n  --> canonical_flow/O_orchestration_control/core_orchestrator.py:37:31\n   |\n36 |         if spec and spec.loader:\n37 |             original_module = importlib_util.module_from_spec(spec)\n   |                               ^^^^^^^^^^^^^^\n38 |             spec.loader.exec_module(original_module)\n   |\n\nF821 Undefined name `e`\n  --> canonical_flow/O_orchestration_control/core_orchestrator.py:56:67\n   |\n54 |     def process(data=None, context=None):\n55 |         \"\"\"Placeholder process function for failed import.\"\"\"\n56 |         return {\"error\": f\"Module {alias_source} failed to load: {e}\"}\n   |                                                                   ^\n   |\n\nF821 Undefined name `Path`\n  --> canonical_flow/O_orchestration_control/decision_engine.py:24:20\n   |\n22 | try:\n23 |     # Add project root to path for imports\n24 |     project_root = Path(__file__).resolve().parents[1]\n   |                    ^^^^\n25 |     if str(project_root) not in sys.path:\n26 |         sys.path.insert(0, str(project_root))\n   |\n\nF821 Undefined name `importlib_util`\n  --> canonical_flow/O_orchestration_control/decision_engine.py:31:16\n   |\n29 |     original_file = project_root / \"decision_engine.py\"\n30 |     if original_file.exists():\n31 |         spec = importlib_util.spec_from_file_location(\n   |                ^^^^^^^^^^^^^^\n32 |             f\"original_{alias_code.lower()}\", \n33 |             str(original_file)\n   |\n\nF821 Undefined name `importlib_util`\n  --> canonical_flow/O_orchestration_control/decision_engine.py:37:31\n   |\n36 |         if spec and spec.loader:\n37 |             original_module = importlib_util.module_from_spec(spec)\n   |                               ^^^^^^^^^^^^^^\n38 |             spec.loader.exec_module(original_module)\n   |\n\nF821 Undefined name `e`\n  --> canonical_flow/O_orchestration_control/decision_engine.py:56:67\n   |\n54 |     def process(data=None, context=None):\n55 |         \"\"\"Placeholder process function for failed import.\"\"\"\n56 |         return {\"error\": f\"Module {alias_source} failed to load: {e}\"}\n   |                                                                   ^\n   |\n\nF821 Undefined name `Path`\n  --> canonical_flow/O_orchestration_control/deterministic_router.py:24:20\n   |\n22 | try:\n23 |     # Add project root to path for imports\n24 |     project_root = Path(__file__).resolve().parents[1]\n   |                    ^^^^\n25 |     if str(project_root) not in sys.path:\n26 |         sys.path.insert(0, str(project_root))\n   |\n\nF821 Undefined name `importlib_util`\n  --> canonical_flow/O_orchestration_control/deterministic_router.py:31:16\n   |\n29 |     original_file = project_root / \"deterministic_router.py\"\n30 |     if original_file.exists():\n31 |         spec = importlib_util.spec_from_file_location(\n   |                ^^^^^^^^^^^^^^\n32 |             f\"original_{alias_code.lower()}\", \n33 |             str(original_file)\n   |\n\nF821 Undefined name `importlib_util`\n  --> canonical_flow/O_orchestration_control/deterministic_router.py:37:31\n   |\n36 |         if spec and spec.loader:\n37 |             original_module = importlib_util.module_from_spec(spec)\n   |                               ^^^^^^^^^^^^^^\n38 |             spec.loader.exec_module(original_module)\n   |\n\nF821 Undefined name `e`\n  --> canonical_flow/O_orchestration_control/deterministic_router.py:56:67\n   |\n54 |     def process(data=None, context=None):\n55 |         \"\"\"Placeholder process function for failed import.\"\"\"\n56 |         return {\"error\": f\"Module {alias_source} failed to load: {e}\"}\n   |                                                                   ^\n   |\n\nF821 Undefined name `Path`\n  --> canonical_flow/O_orchestration_control/distributed_processor.py:24:20\n   |\n22 | try:\n23 |     # Add project root to path for imports\n24 |     project_root = Path(__file__).resolve().parents[1]\n   |                    ^^^^\n25 |     if str(project_root) not in sys.path:\n26 |         sys.path.insert(0, str(project_root))\n   |\n\nF821 Undefined name `importlib_util`\n  --> canonical_flow/O_orchestration_control/distributed_processor.py:31:16\n   |\n29 |     original_file = project_root / \"distributed_processor.py\"\n30 |     if original_file.exists():\n31 |         spec = importlib_util.spec_from_file_location(\n   |                ^^^^^^^^^^^^^^\n32 |             f\"original_{alias_code.lower()}\", \n33 |             str(original_file)\n   |\n\nF821 Undefined name `importlib_util`\n  --> canonical_flow/O_orchestration_control/distributed_processor.py:37:31\n   |\n36 |         if spec and spec.loader:\n37 |             original_module = importlib_util.module_from_spec(spec)\n   |                               ^^^^^^^^^^^^^^\n38 |             spec.loader.exec_module(original_module)\n   |\n\nF821 Undefined name `e`\n  --> canonical_flow/O_orchestration_control/distributed_processor.py:56:67\n   |\n54 |     def process(data=None, context=None):\n55 |         \"\"\"Placeholder process function for failed import.\"\"\"\n56 |         return {\"error\": f\"Module {alias_source} failed to load: {e}\"}\n   |                                                                   ^\n   |\n\nF821 Undefined name `Path`\n  --> canonical_flow/O_orchestration_control/enhanced_core_orchestrator.py:24:20\n   |\n22 | try:\n23 |     # Add project root to path for imports\n24 |     project_root = Path(__file__).resolve().parents[1]\n   |                    ^^^^\n25 |     if str(project_root) not in sys.path:\n26 |         sys.path.insert(0, str(project_root))\n   |\n\nF821 Undefined name `importlib_util`\n  --> canonical_flow/O_orchestration_control/enhanced_core_orchestrator.py:31:16\n   |\n29 |     original_file = project_root / \"enhanced_core_orchestrator.py\"\n30 |     if original_file.exists():\n31 |         spec = importlib_util.spec_from_file_location(\n   |                ^^^^^^^^^^^^^^\n32 |             f\"original_{alias_code.lower()}\", \n33 |             str(original_file)\n   |\n\nF821 Undefined name `importlib_util`\n  --> canonical_flow/O_orchestration_control/enhanced_core_orchestrator.py:37:31\n   |\n36 |         if spec and spec.loader:\n37 |             original_module = importlib_util.module_from_spec(spec)\n   |                               ^^^^^^^^^^^^^^\n38 |             spec.loader.exec_module(original_module)\n   |\n\nF821 Undefined name `e`\n  --> canonical_flow/O_orchestration_control/enhanced_core_orchestrator.py:56:67\n   |\n54 |     def process(data=None, context=None):\n55 |         \"\"\"Placeholder process function for failed import.\"\"\"\n56 |         return {\"error\": f\"Module {alias_source} failed to load: {e}\"}\n   |                                                                   ^\n   |\n\nF821 Undefined name `Path`\n  --> canonical_flow/O_orchestration_control/evidence_router.py:24:20\n   |\n22 | try:\n23 |     # Add project root to path for imports\n24 |     project_root = Path(__file__).resolve().parents[1]\n   |                    ^^^^\n25 |     if str(project_root) not in sys.path:\n26 |         sys.path.insert(0, str(project_root))\n   |\n\nF821 Undefined name `importlib_util`\n  --> canonical_flow/O_orchestration_control/evidence_router.py:31:16\n   |\n29 |     original_file = project_root / \"evidence_router.py\"\n30 |     if original_file.exists():\n31 |         spec = importlib_util.spec_from_file_location(\n   |                ^^^^^^^^^^^^^^\n32 |             f\"original_{alias_code.lower()}\", \n33 |             str(original_file)\n   |\n\nF821 Undefined name `importlib_util`\n  --> canonical_flow/O_orchestration_control/evidence_router.py:37:31\n   |\n36 |         if spec and spec.loader:\n37 |             original_module = importlib_util.module_from_spec(spec)\n   |                               ^^^^^^^^^^^^^^\n38 |             spec.loader.exec_module(original_module)\n   |\n\nF821 Undefined name `e`\n  --> canonical_flow/O_orchestration_control/evidence_router.py:56:67\n   |\n54 |     def process(data=None, context=None):\n55 |         \"\"\"Placeholder process function for failed import.\"\"\"\n56 |         return {\"error\": f\"Module {alias_source} failed to load: {e}\"}\n   |                                                                   ^\n   |\n\nF821 Undefined name `Path`\n  --> canonical_flow/O_orchestration_control/exception_monitoring.py:24:20\n   |\n22 | try:\n23 |     # Add project root to path for imports\n24 |     project_root = Path(__file__).resolve().parents[1]\n   |                    ^^^^\n25 |     if str(project_root) not in sys.path:\n26 |         sys.path.insert(0, str(project_root))\n   |\n\nF821 Undefined name `importlib_util`\n  --> canonical_flow/O_orchestration_control/exception_monitoring.py:31:16\n   |\n29 |     original_file = project_root / \"exception_monitoring.py\"\n30 |     if original_file.exists():\n31 |         spec = importlib_util.spec_from_file_location(\n   |                ^^^^^^^^^^^^^^\n32 |             f\"original_{alias_code.lower()}\", \n33 |             str(original_file)\n   |\n\nF821 Undefined name `importlib_util`\n  --> canonical_flow/O_orchestration_control/exception_monitoring.py:37:31\n   |\n36 |         if spec and spec.loader:\n37 |             original_module = importlib_util.module_from_spec(spec)\n   |                               ^^^^^^^^^^^^^^\n38 |             spec.loader.exec_module(original_module)\n   |\n\nF821 Undefined name `e`\n  --> canonical_flow/O_orchestration_control/exception_monitoring.py:56:67\n   |\n54 |     def process(data=None, context=None):\n55 |         \"\"\"Placeholder process function for failed import.\"\"\"\n56 |         return {\"error\": f\"Module {alias_source} failed to load: {e}\"}\n   |                                                                   ^\n   |\n\nF821 Undefined name `Path`\n  --> canonical_flow/O_orchestration_control/exception_telemetry.py:24:20\n   |\n22 | try:\n23 |     # Add project root to path for imports\n24 |     project_root = Path(__file__).resolve().parents[1]\n   |                    ^^^^\n25 |     if str(project_root) not in sys.path:\n26 |         sys.path.insert(0, str(project_root))\n   |\n\nF821 Undefined name `importlib_util`\n  --> canonical_flow/O_orchestration_control/exception_telemetry.py:31:16\n   |\n29 |     original_file = project_root / \"exception_telemetry.py\"\n30 |     if original_file.exists():\n31 |         spec = importlib_util.spec_from_file_location(\n   |                ^^^^^^^^^^^^^^\n32 |             f\"original_{alias_code.lower()}\", \n33 |             str(original_file)\n   |\n\nF821 Undefined name `importlib_util`\n  --> canonical_flow/O_orchestration_control/exception_telemetry.py:37:31\n   |\n36 |         if spec and spec.loader:\n37 |             original_module = importlib_util.module_from_spec(spec)\n   |                               ^^^^^^^^^^^^^^\n38 |             spec.loader.exec_module(original_module)\n   |\n\nF821 Undefined name `e`\n  --> canonical_flow/O_orchestration_control/exception_telemetry.py:56:67\n   |\n54 |     def process(data=None, context=None):\n55 |         \"\"\"Placeholder process function for failed import.\"\"\"\n56 |         return {\"error\": f\"Module {alias_source} failed to load: {e}\"}\n   |                                                                   ^\n   |\n\nF821 Undefined name `Path`\n  --> canonical_flow/O_orchestration_control/rubric_validator.py:24:20\n   |\n22 | try:\n23 |     # Add project root to path for imports\n24 |     project_root = Path(__file__).resolve().parents[1]\n   |                    ^^^^\n25 |     if str(project_root) not in sys.path:\n26 |         sys.path.insert(0, str(project_root))\n   |\n\nF821 Undefined name `importlib_util`\n  --> canonical_flow/O_orchestration_control/rubric_validator.py:31:16\n   |\n29 |     original_file = project_root / \"rubric_validator.py\"\n30 |     if original_file.exists():\n31 |         spec = importlib_util.spec_from_file_location(\n   |                ^^^^^^^^^^^^^^\n32 |             f\"original_{alias_code.lower()}\", \n33 |             str(original_file)\n   |\n\nF821 Undefined name `importlib_util`\n  --> canonical_flow/O_orchestration_control/rubric_validator.py:37:31\n   |\n36 |         if spec and spec.loader:\n37 |             original_module = importlib_util.module_from_spec(spec)\n   |                               ^^^^^^^^^^^^^^\n38 |             spec.loader.exec_module(original_module)\n   |\n\nF821 Undefined name `e`\n  --> canonical_flow/O_orchestration_control/rubric_validator.py:56:67\n   |\n54 |     def process(data=None, context=None):\n55 |         \"\"\"Placeholder process function for failed import.\"\"\"\n56 |         return {\"error\": f\"Module {alias_source} failed to load: {e}\"}\n   |                                                                   ^\n   |\n\nF821 Undefined name `Path`\n  --> canonical_flow/R_search_retrieval/deterministic_hybrid_retrieval.py:24:20\n   |\n22 | try:\n23 |     # Add project root to path for imports\n24 |     project_root = Path(__file__).resolve().parents[1]\n   |                    ^^^^\n25 |     if str(project_root) not in sys.path:\n26 |         sys.path.insert(0, str(project_root))\n   |\n\nF821 Undefined name `importlib_util`\n  --> canonical_flow/R_search_retrieval/deterministic_hybrid_retrieval.py:31:16\n   |\n29 |     original_file = project_root / \"deterministic_hybrid_retrieval.py\"\n30 |     if original_file.exists():\n31 |         spec = importlib_util.spec_from_file_location(\n   |                ^^^^^^^^^^^^^^\n32 |             f\"original_{alias_code.lower()}\", \n33 |             str(original_file)\n   |\n\nF821 Undefined name `importlib_util`\n  --> canonical_flow/R_search_retrieval/deterministic_hybrid_retrieval.py:37:31\n   |\n36 |         if spec and spec.loader:\n37 |             original_module = importlib_util.module_from_spec(spec)\n   |                               ^^^^^^^^^^^^^^\n38 |             spec.loader.exec_module(original_module)\n   |\n\nF821 Undefined name `e`\n  --> canonical_flow/R_search_retrieval/deterministic_hybrid_retrieval.py:56:67\n   |\n54 |     def process(data=None, context=None):\n55 |         \"\"\"Placeholder process function for failed import.\"\"\"\n56 |         return {\"error\": f\"Module {alias_source} failed to load: {e}\"}\n   |                                                                   ^\n   |\n\nF821 Undefined name `Path`\n  --> canonical_flow/R_search_retrieval/hybrid_retrieval_bridge.py:24:20\n   |\n22 | try:\n23 |     # Add project root to path for imports\n24 |     project_root = Path(__file__).resolve().parents[1]\n   |                    ^^^^\n25 |     if str(project_root) not in sys.path:\n26 |         sys.path.insert(0, str(project_root))\n   |\n\nF821 Undefined name `importlib_util`\n  --> canonical_flow/R_search_retrieval/hybrid_retrieval_bridge.py:31:16\n   |\n29 |     original_file = project_root / \"hybrid_retrieval_bridge.py\"\n30 |     if original_file.exists():\n31 |         spec = importlib_util.spec_from_file_location(\n   |                ^^^^^^^^^^^^^^\n32 |             f\"original_{alias_code.lower()}\", \n33 |             str(original_file)\n   |\n\nF821 Undefined name `importlib_util`\n  --> canonical_flow/R_search_retrieval/hybrid_retrieval_bridge.py:37:31\n   |\n36 |         if spec and spec.loader:\n37 |             original_module = importlib_util.module_from_spec(spec)\n   |                               ^^^^^^^^^^^^^^\n38 |             spec.loader.exec_module(original_module)\n   |\n\nF821 Undefined name `e`\n  --> canonical_flow/R_search_retrieval/hybrid_retrieval_bridge.py:56:67\n   |\n54 |     def process(data=None, context=None):\n55 |         \"\"\"Placeholder process function for failed import.\"\"\"\n56 |         return {\"error\": f\"Module {alias_source} failed to load: {e}\"}\n   |                                                                   ^\n   |\n\nF821 Undefined name `Path`\n  --> canonical_flow/R_search_retrieval/hybrid_retrieval_core.py:24:20\n   |\n22 | try:\n23 |     # Add project root to path for imports\n24 |     project_root = Path(__file__).resolve().parents[1]\n   |                    ^^^^\n25 |     if str(project_root) not in sys.path:\n26 |         sys.path.insert(0, str(project_root))\n   |\n\nF821 Undefined name `importlib_util`\n  --> canonical_flow/R_search_retrieval/hybrid_retrieval_core.py:31:16\n   |\n29 |     original_file = project_root / \"hybrid_retrieval.py\"\n30 |     if original_file.exists():\n31 |         spec = importlib_util.spec_from_file_location(\n   |                ^^^^^^^^^^^^^^\n32 |             f\"original_{alias_code.lower()}\", \n33 |             str(original_file)\n   |\n\nF821 Undefined name `importlib_util`\n  --> canonical_flow/R_search_retrieval/hybrid_retrieval_core.py:37:31\n   |\n36 |         if spec and spec.loader:\n37 |             original_module = importlib_util.module_from_spec(spec)\n   |                               ^^^^^^^^^^^^^^\n38 |             spec.loader.exec_module(original_module)\n   |\n\nF821 Undefined name `e`\n  --> canonical_flow/R_search_retrieval/hybrid_retrieval_core.py:56:67\n   |\n54 |     def process(data=None, context=None):\n55 |         \"\"\"Placeholder process function for failed import.\"\"\"\n56 |         return {\"error\": f\"Module {alias_source} failed to load: {e}\"}\n   |                                                                   ^\n   |\n\nF821 Undefined name `Path`\n  --> canonical_flow/R_search_retrieval/hybrid_retriever.py:24:20\n   |\n22 | try:\n23 |     # Add project root to path for imports\n24 |     project_root = Path(__file__).resolve().parents[1]\n   |                    ^^^^\n25 |     if str(project_root) not in sys.path:\n26 |         sys.path.insert(0, str(project_root))\n   |\n\nF821 Undefined name `importlib_util`\n  --> canonical_flow/R_search_retrieval/hybrid_retriever.py:31:16\n   |\n29 |     original_file = project_root / \"retrieval_engine/hybrid_retriever.py\"\n30 |     if original_file.exists():\n31 |         spec = importlib_util.spec_from_file_location(\n   |                ^^^^^^^^^^^^^^\n32 |             f\"original_{alias_code.lower()}\", \n33 |             str(original_file)\n   |\n\nF821 Undefined name `importlib_util`\n  --> canonical_flow/R_search_retrieval/hybrid_retriever.py:37:31\n   |\n36 |         if spec and spec.loader:\n37 |             original_module = importlib_util.module_from_spec(spec)\n   |                               ^^^^^^^^^^^^^^\n38 |             spec.loader.exec_module(original_module)\n   |\n\nF821 Undefined name `e`\n  --> canonical_flow/R_search_retrieval/hybrid_retriever.py:56:67\n   |\n54 |     def process(data=None, context=None):\n55 |         \"\"\"Placeholder process function for failed import.\"\"\"\n56 |         return {\"error\": f\"Module {alias_source} failed to load: {e}\"}\n   |                                                                   ^\n   |\n\nF821 Undefined name `Path`\n  --> canonical_flow/R_search_retrieval/intelligent_recommendation_engine.py:24:20\n   |\n22 | try:\n23 |     # Add project root to path for imports\n24 |     project_root = Path(__file__).resolve().parents[1]\n   |                    ^^^^\n25 |     if str(project_root) not in sys.path:\n26 |         sys.path.insert(0, str(project_root))\n   |\n\nF821 Undefined name `importlib_util`\n  --> canonical_flow/R_search_retrieval/intelligent_recommendation_engine.py:31:16\n   |\n29 |     original_file = project_root / \"intelligent_recommendation_engine.py\"\n30 |     if original_file.exists():\n31 |         spec = importlib_util.spec_from_file_location(\n   |                ^^^^^^^^^^^^^^\n32 |             f\"original_{alias_code.lower()}\", \n33 |             str(original_file)\n   |\n\nF821 Undefined name `importlib_util`\n  --> canonical_flow/R_search_retrieval/intelligent_recommendation_engine.py:37:31\n   |\n36 |         if spec and spec.loader:\n37 |             original_module = importlib_util.module_from_spec(spec)\n   |                               ^^^^^^^^^^^^^^\n38 |             spec.loader.exec_module(original_module)\n   |\n\nF821 Undefined name `e`\n  --> canonical_flow/R_search_retrieval/intelligent_recommendation_engine.py:56:67\n   |\n54 |     def process(data=None, context=None):\n55 |         \"\"\"Placeholder process function for failed import.\"\"\"\n56 |         return {\"error\": f\"Module {alias_source} failed to load: {e}\"}\n   |                                                                   ^\n   |\n\nF821 Undefined name `Path`\n  --> canonical_flow/R_search_retrieval/lexical_index.py:24:20\n   |\n22 | try:\n23 |     # Add project root to path for imports\n24 |     project_root = Path(__file__).resolve().parents[1]\n   |                    ^^^^\n25 |     if str(project_root) not in sys.path:\n26 |         sys.path.insert(0, str(project_root))\n   |\n\nF821 Undefined name `importlib_util`\n  --> canonical_flow/R_search_retrieval/lexical_index.py:31:16\n   |\n29 |     original_file = project_root / \"lexical_index.py\"\n30 |     if original_file.exists():\n31 |         spec = importlib_util.spec_from_file_location(\n   |                ^^^^^^^^^^^^^^\n32 |             f\"original_{alias_code.lower()}\", \n33 |             str(original_file)\n   |\n\nF821 Undefined name `importlib_util`\n  --> canonical_flow/R_search_retrieval/lexical_index.py:37:31\n   |\n36 |         if spec and spec.loader:\n37 |             original_module = importlib_util.module_from_spec(spec)\n   |                               ^^^^^^^^^^^^^^\n38 |             spec.loader.exec_module(original_module)\n   |\n\nF821 Undefined name `e`\n  --> canonical_flow/R_search_retrieval/lexical_index.py:56:67\n   |\n54 |     def process(data=None, context=None):\n55 |         \"\"\"Placeholder process function for failed import.\"\"\"\n56 |         return {\"error\": f\"Module {alias_source} failed to load: {e}\"}\n   |                                                                   ^\n   |\n\nF821 Undefined name `Path`\n  --> canonical_flow/R_search_retrieval/lexical_index_base.py:24:20\n   |\n22 | try:\n23 |     # Add project root to path for imports\n24 |     project_root = Path(__file__).resolve().parents[1]\n   |                    ^^^^\n25 |     if str(project_root) not in sys.path:\n26 |         sys.path.insert(0, str(project_root))\n   |\n\nF821 Undefined name `importlib_util`\n  --> canonical_flow/R_search_retrieval/lexical_index_base.py:31:16\n   |\n29 |     original_file = project_root / \"retrieval_engine/lexical_index.py\"\n30 |     if original_file.exists():\n31 |         spec = importlib_util.spec_from_file_location(\n   |                ^^^^^^^^^^^^^^\n32 |             f\"original_{alias_code.lower()}\", \n33 |             str(original_file)\n   |\n\nF821 Undefined name `importlib_util`\n  --> canonical_flow/R_search_retrieval/lexical_index_base.py:37:31\n   |\n36 |         if spec and spec.loader:\n37 |             original_module = importlib_util.module_from_spec(spec)\n   |                               ^^^^^^^^^^^^^^\n38 |             spec.loader.exec_module(original_module)\n   |\n\nF821 Undefined name `e`\n  --> canonical_flow/R_search_retrieval/lexical_index_base.py:56:67\n   |\n54 |     def process(data=None, context=None):\n55 |         \"\"\"Placeholder process function for failed import.\"\"\"\n56 |         return {\"error\": f\"Module {alias_source} failed to load: {e}\"}\n   |                                                                   ^\n   |\n\nF821 Undefined name `Path`\n  --> canonical_flow/R_search_retrieval/reranker.py:24:20\n   |\n22 | try:\n23 |     # Add project root to path for imports\n24 |     project_root = Path(__file__).resolve().parents[1]\n   |                    ^^^^\n25 |     if str(project_root) not in sys.path:\n26 |         sys.path.insert(0, str(project_root))\n   |\n\nF821 Undefined name `importlib_util`\n  --> canonical_flow/R_search_retrieval/reranker.py:31:16\n   |\n29 |     original_file = project_root / \"semantic_reranking/reranker.py\"\n30 |     if original_file.exists():\n31 |         spec = importlib_util.spec_from_file_location(\n   |                ^^^^^^^^^^^^^^\n32 |             f\"original_{alias_code.lower()}\", \n33 |             str(original_file)\n   |\n\nF821 Undefined name `importlib_util`\n  --> canonical_flow/R_search_retrieval/reranker.py:37:31\n   |\n36 |         if spec and spec.loader:\n37 |             original_module = importlib_util.module_from_spec(spec)\n   |                               ^^^^^^^^^^^^^^\n38 |             spec.loader.exec_module(original_module)\n   |\n\nF821 Undefined name `e`\n  --> canonical_flow/R_search_retrieval/reranker.py:56:67\n   |\n54 |     def process(data=None, context=None):\n55 |         \"\"\"Placeholder process function for failed import.\"\"\"\n56 |         return {\"error\": f\"Module {alias_source} failed to load: {e}\"}\n   |                                                                   ^\n   |\n\nF821 Undefined name `Path`\n  --> canonical_flow/R_search_retrieval/vector_index.py:24:20\n   |\n22 | try:\n23 |     # Add project root to path for imports\n24 |     project_root = Path(__file__).resolve().parents[1]\n   |                    ^^^^\n25 |     if str(project_root) not in sys.path:\n26 |         sys.path.insert(0, str(project_root))\n   |\n\nF821 Undefined name `importlib_util`\n  --> canonical_flow/R_search_retrieval/vector_index.py:31:16\n   |\n29 |     original_file = project_root / \"retrieval_engine/vector_index.py\"\n30 |     if original_file.exists():\n31 |         spec = importlib_util.spec_from_file_location(\n   |                ^^^^^^^^^^^^^^\n32 |             f\"original_{alias_code.lower()}\", \n33 |             str(original_file)\n   |\n\nF821 Undefined name `importlib_util`\n  --> canonical_flow/R_search_retrieval/vector_index.py:37:31\n   |\n36 |         if spec and spec.loader:\n37 |             original_module = importlib_util.module_from_spec(spec)\n   |                               ^^^^^^^^^^^^^^\n38 |             spec.loader.exec_module(original_module)\n   |\n\nF821 Undefined name `e`\n  --> canonical_flow/R_search_retrieval/vector_index.py:56:67\n   |\n54 |     def process(data=None, context=None):\n55 |         \"\"\"Placeholder process function for failed import.\"\"\"\n56 |         return {\"error\": f\"Module {alias_source} failed to load: {e}\"}\n   |                                                                   ^\n   |\n\nF821 Undefined name `Path`\n  --> canonical_flow/S_synthesis_output/answer_formatter.py:24:20\n   |\n22 | try:\n23 |     # Add project root to path for imports\n24 |     project_root = Path(__file__).resolve().parents[1]\n   |                    ^^^^\n25 |     if str(project_root) not in sys.path:\n26 |         sys.path.insert(0, str(project_root))\n   |\n\nF821 Undefined name `importlib_util`\n  --> canonical_flow/S_synthesis_output/answer_formatter.py:31:16\n   |\n29 |     original_file = project_root / \"answer_formatter.py\"\n30 |     if original_file.exists():\n31 |         spec = importlib_util.spec_from_file_location(\n   |                ^^^^^^^^^^^^^^\n32 |             f\"original_{alias_code.lower()}\", \n33 |             str(original_file)\n   |\n\nF821 Undefined name `importlib_util`\n  --> canonical_flow/S_synthesis_output/answer_formatter.py:37:31\n   |\n36 |         if spec and spec.loader:\n37 |             original_module = importlib_util.module_from_spec(spec)\n   |                               ^^^^^^^^^^^^^^\n38 |             spec.loader.exec_module(original_module)\n   |\n\nF821 Undefined name `e`\n  --> canonical_flow/S_synthesis_output/answer_formatter.py:56:67\n   |\n54 |     def process(data=None, context=None):\n55 |         \"\"\"Placeholder process function for failed import.\"\"\"\n56 |         return {\"error\": f\"Module {alias_source} failed to load: {e}\"}\n   |                                                                   ^\n   |\n\nF821 Undefined name `Path`\n  --> canonical_flow/S_synthesis_output/answer_synthesizer.py:24:20\n   |\n22 | try:\n23 |     # Add project root to path for imports\n24 |     project_root = Path(__file__).resolve().parents[1]\n   |                    ^^^^\n25 |     if str(project_root) not in sys.path:\n26 |         sys.path.insert(0, str(project_root))\n   |\n\nF821 Undefined name `importlib_util`\n  --> canonical_flow/S_synthesis_output/answer_synthesizer.py:31:16\n   |\n29 |     original_file = project_root / \"answer_synthesizer.py\"\n30 |     if original_file.exists():\n31 |         spec = importlib_util.spec_from_file_location(\n   |                ^^^^^^^^^^^^^^\n32 |             f\"original_{alias_code.lower()}\", \n33 |             str(original_file)\n   |\n\nF821 Undefined name `importlib_util`\n  --> canonical_flow/S_synthesis_output/answer_synthesizer.py:37:31\n   |\n36 |         if spec and spec.loader:\n37 |             original_module = importlib_util.module_from_spec(spec)\n   |                               ^^^^^^^^^^^^^^\n38 |             spec.loader.exec_module(original_module)\n   |\n\nF821 Undefined name `e`\n  --> canonical_flow/S_synthesis_output/answer_synthesizer.py:56:67\n   |\n54 |     def process(data=None, context=None):\n55 |         \"\"\"Placeholder process function for failed import.\"\"\"\n56 |         return {\"error\": f\"Module {alias_source} failed to load: {e}\"}\n   |                                                                   ^\n   |\n\nF821 Undefined name `Path`\n  --> canonical_flow/T_integration_storage/analytics_enhancement.py:24:20\n   |\n22 | try:\n23 |     # Add project root to path for imports\n24 |     project_root = Path(__file__).resolve().parents[1]\n   |                    ^^^^\n25 |     if str(project_root) not in sys.path:\n26 |         sys.path.insert(0, str(project_root))\n   |\n\nF821 Undefined name `importlib_util`\n  --> canonical_flow/T_integration_storage/analytics_enhancement.py:31:16\n   |\n29 |     original_file = project_root / \"analytics_enhancement.py\"\n30 |     if original_file.exists():\n31 |         spec = importlib_util.spec_from_file_location(\n   |                ^^^^^^^^^^^^^^\n32 |             f\"original_{alias_code.lower()}\", \n33 |             str(original_file)\n   |\n\nF821 Undefined name `importlib_util`\n  --> canonical_flow/T_integration_storage/analytics_enhancement.py:37:31\n   |\n36 |         if spec and spec.loader:\n37 |             original_module = importlib_util.module_from_spec(spec)\n   |                               ^^^^^^^^^^^^^^\n38 |             spec.loader.exec_module(original_module)\n   |\n\nF821 Undefined name `e`\n  --> canonical_flow/T_integration_storage/analytics_enhancement.py:56:67\n   |\n54 |     def process(data=None, context=None):\n55 |         \"\"\"Placeholder process function for failed import.\"\"\"\n56 |         return {\"error\": f\"Module {alias_source} failed to load: {e}\"}\n   |                                                                   ^\n   |\n\nF821 Undefined name `Path`\n  --> canonical_flow/T_integration_storage/compensation_engine.py:24:20\n   |\n22 | try:\n23 |     # Add project root to path for imports\n24 |     project_root = Path(__file__).resolve().parents[1]\n   |                    ^^^^\n25 |     if str(project_root) not in sys.path:\n26 |         sys.path.insert(0, str(project_root))\n   |\n\nF821 Undefined name `importlib_util`\n  --> canonical_flow/T_integration_storage/compensation_engine.py:31:16\n   |\n29 |     original_file = project_root / \"compensation_engine.py\"\n30 |     if original_file.exists():\n31 |         spec = importlib_util.spec_from_file_location(\n   |                ^^^^^^^^^^^^^^\n32 |             f\"original_{alias_code.lower()}\", \n33 |             str(original_file)\n   |\n\nF821 Undefined name `importlib_util`\n  --> canonical_flow/T_integration_storage/compensation_engine.py:37:31\n   |\n36 |         if spec and spec.loader:\n37 |             original_module = importlib_util.module_from_spec(spec)\n   |                               ^^^^^^^^^^^^^^\n38 |             spec.loader.exec_module(original_module)\n   |\n\nF821 Undefined name `e`\n  --> canonical_flow/T_integration_storage/compensation_engine.py:56:67\n   |\n54 |     def process(data=None, context=None):\n55 |         \"\"\"Placeholder process function for failed import.\"\"\"\n56 |         return {\"error\": f\"Module {alias_source} failed to load: {e}\"}\n   |                                                                   ^\n   |\n\nF821 Undefined name `Path`\n  --> canonical_flow/T_integration_storage/feedback_loop.py:24:20\n   |\n22 | try:\n23 |     # Add project root to path for imports\n24 |     project_root = Path(__file__).resolve().parents[1]\n   |                    ^^^^\n25 |     if str(project_root) not in sys.path:\n26 |         sys.path.insert(0, str(project_root))\n   |\n\nF821 Undefined name `importlib_util`\n  --> canonical_flow/T_integration_storage/feedback_loop.py:31:16\n   |\n29 |     original_file = project_root / \"feedback_loop.py\"\n30 |     if original_file.exists():\n31 |         spec = importlib_util.spec_from_file_location(\n   |                ^^^^^^^^^^^^^^\n32 |             f\"original_{alias_code.lower()}\", \n33 |             str(original_file)\n   |\n\nF821 Undefined name `importlib_util`\n  --> canonical_flow/T_integration_storage/feedback_loop.py:37:31\n   |\n36 |         if spec and spec.loader:\n37 |             original_module = importlib_util.module_from_spec(spec)\n   |                               ^^^^^^^^^^^^^^\n38 |             spec.loader.exec_module(original_module)\n   |\n\nF821 Undefined name `e`\n  --> canonical_flow/T_integration_storage/feedback_loop.py:56:67\n   |\n54 |     def process(data=None, context=None):\n55 |         \"\"\"Placeholder process function for failed import.\"\"\"\n56 |         return {\"error\": f\"Module {alias_source} failed to load: {e}\"}\n   |                                                                   ^\n   |\n\nF821 Undefined name `Path`\n  --> canonical_flow/T_integration_storage/metrics_collector.py:24:20\n   |\n22 | try:\n23 |     # Add project root to path for imports\n24 |     project_root = Path(__file__).resolve().parents[1]\n   |                    ^^^^\n25 |     if str(project_root) not in sys.path:\n26 |         sys.path.insert(0, str(project_root))\n   |\n\nF821 Undefined name `importlib_util`\n  --> canonical_flow/T_integration_storage/metrics_collector.py:31:16\n   |\n29 |     original_file = project_root / \"metrics_collector.py\"\n30 |     if original_file.exists():\n31 |         spec = importlib_util.spec_from_file_location(\n   |                ^^^^^^^^^^^^^^\n32 |             f\"original_{alias_code.lower()}\", \n33 |             str(original_file)\n   |\n\nF821 Undefined name `importlib_util`\n  --> canonical_flow/T_integration_storage/metrics_collector.py:37:31\n   |\n36 |         if spec and spec.loader:\n37 |             original_module = importlib_util.module_from_spec(spec)\n   |                               ^^^^^^^^^^^^^^\n38 |             spec.loader.exec_module(original_module)\n   |\n\nF821 Undefined name `e`\n  --> canonical_flow/T_integration_storage/metrics_collector.py:56:67\n   |\n54 |     def process(data=None, context=None):\n55 |         \"\"\"Placeholder process function for failed import.\"\"\"\n56 |         return {\"error\": f\"Module {alias_source} failed to load: {e}\"}\n   |                                                                   ^\n   |\n\nF821 Undefined name `Path`\n  --> canonical_flow/T_integration_storage/optimization_engine.py:24:20\n   |\n22 | try:\n23 |     # Add project root to path for imports\n24 |     project_root = Path(__file__).resolve().parents[1]\n   |                    ^^^^\n25 |     if str(project_root) not in sys.path:\n26 |         sys.path.insert(0, str(project_root))\n   |\n\nF821 Undefined name `importlib_util`\n  --> canonical_flow/T_integration_storage/optimization_engine.py:31:16\n   |\n29 |     original_file = project_root / \"optimization_engine.py\"\n30 |     if original_file.exists():\n31 |         spec = importlib_util.spec_from_file_location(\n   |                ^^^^^^^^^^^^^^\n32 |             f\"original_{alias_code.lower()}\", \n33 |             str(original_file)\n   |\n\nF821 Undefined name `importlib_util`\n  --> canonical_flow/T_integration_storage/optimization_engine.py:37:31\n   |\n36 |         if spec and spec.loader:\n37 |             original_module = importlib_util.module_from_spec(spec)\n   |                               ^^^^^^^^^^^^^^\n38 |             spec.loader.exec_module(original_module)\n   |\n\nF821 Undefined name `e`\n  --> canonical_flow/T_integration_storage/optimization_engine.py:56:67\n   |\n54 |     def process(data=None, context=None):\n55 |         \"\"\"Placeholder process function for failed import.\"\"\"\n56 |         return {\"error\": f\"Module {alias_source} failed to load: {e}\"}\n   |                                                                   ^\n   |\n\nF821 Undefined name `Path`\n  --> canonical_flow/X_context_construction/context_adapter.py:24:20\n   |\n22 | try:\n23 |     # Add project root to path for imports\n24 |     project_root = Path(__file__).resolve().parents[1]\n   |                    ^^^^\n25 |     if str(project_root) not in sys.path:\n26 |         sys.path.insert(0, str(project_root))\n   |\n\nF821 Undefined name `importlib_util`\n  --> canonical_flow/X_context_construction/context_adapter.py:31:16\n   |\n29 |     original_file = project_root / \"context_adapter.py\"\n30 |     if original_file.exists():\n31 |         spec = importlib_util.spec_from_file_location(\n   |                ^^^^^^^^^^^^^^\n32 |             f\"original_{alias_code.lower()}\", \n33 |             str(original_file)\n   |\n\nF821 Undefined name `importlib_util`\n  --> canonical_flow/X_context_construction/context_adapter.py:37:31\n   |\n36 |         if spec and spec.loader:\n37 |             original_module = importlib_util.module_from_spec(spec)\n   |                               ^^^^^^^^^^^^^^\n38 |             spec.loader.exec_module(original_module)\n   |\n\nF821 Undefined name `e`\n  --> canonical_flow/X_context_construction/context_adapter.py:56:67\n   |\n54 |     def process(data=None, context=None):\n55 |         \"\"\"Placeholder process function for failed import.\"\"\"\n56 |         return {\"error\": f\"Module {alias_source} failed to load: {e}\"}\n   |                                                                   ^\n   |\n\nF821 Undefined name `Path`\n  --> canonical_flow/X_context_construction/immutable_context.py:24:20\n   |\n22 | try:\n23 |     # Add project root to path for imports\n24 |     project_root = Path(__file__).resolve().parents[1]\n   |                    ^^^^\n25 |     if str(project_root) not in sys.path:\n26 |         sys.path.insert(0, str(project_root))\n   |\n\nF821 Undefined name `importlib_util`\n  --> canonical_flow/X_context_construction/immutable_context.py:31:16\n   |\n29 |     original_file = project_root / \"immutable_context.py\"\n30 |     if original_file.exists():\n31 |         spec = importlib_util.spec_from_file_location(\n   |                ^^^^^^^^^^^^^^\n32 |             f\"original_{alias_code.lower()}\", \n33 |             str(original_file)\n   |\n\nF821 Undefined name `importlib_util`\n  --> canonical_flow/X_context_construction/immutable_context.py:37:31\n   |\n36 |         if spec and spec.loader:\n37 |             original_module = importlib_util.module_from_spec(spec)\n   |                               ^^^^^^^^^^^^^^\n38 |             spec.loader.exec_module(original_module)\n   |\n\nF821 Undefined name `e`\n  --> canonical_flow/X_context_construction/immutable_context.py:56:67\n   |\n54 |     def process(data=None, context=None):\n55 |         \"\"\"Placeholder process function for failed import.\"\"\"\n56 |         return {\"error\": f\"Module {alias_source} failed to load: {e}\"}\n   |                                                                   ^\n   |\n\nF821 Undefined name `Path`\n  --> canonical_flow/X_context_construction/lineage_tracker.py:24:20\n   |\n22 | try:\n23 |     # Add project root to path for imports\n24 |     project_root = Path(__file__).resolve().parents[1]\n   |                    ^^^^\n25 |     if str(project_root) not in sys.path:\n26 |         sys.path.insert(0, str(project_root))\n   |\n\nF821 Undefined name `importlib_util`\n  --> canonical_flow/X_context_construction/lineage_tracker.py:31:16\n   |\n29 |     original_file = project_root / \"lineage_tracker.py\"\n30 |     if original_file.exists():\n31 |         spec = importlib_util.spec_from_file_location(\n   |                ^^^^^^^^^^^^^^\n32 |             f\"original_{alias_code.lower()}\", \n33 |             str(original_file)\n   |\n\nF821 Undefined name `importlib_util`\n  --> canonical_flow/X_context_construction/lineage_tracker.py:37:31\n   |\n36 |         if spec and spec.loader:\n37 |             original_module = importlib_util.module_from_spec(spec)\n   |                               ^^^^^^^^^^^^^^\n38 |             spec.loader.exec_module(original_module)\n   |\n\nF821 Undefined name `e`\n  --> canonical_flow/X_context_construction/lineage_tracker.py:56:67\n   |\n54 |     def process(data=None, context=None):\n55 |         \"\"\"Placeholder process function for failed import.\"\"\"\n56 |         return {\"error\": f\"Module {alias_source} failed to load: {e}\"}\n   |                                                                   ^\n   |\n\nF401 [*] `os` imported but unused\n  --> canonical_flow/analysis/artifact_generator.py:13:8\n   |\n12 | import json\n13 | import os\n   |        ^^\n14 | # # # from pathlib import Path  # Module not found  # Module not found  # Module not found\n15 | # # # from typing import Dict, List, Any, Optional, Union, Tuple  # Module not found  # Module not found  # Module not found\n   |\nhelp: Remove unused import: `os`\n\nF821 Undefined name `dataclass`\n  --> canonical_flow/analysis/artifact_generator.py:26:2\n   |\n26 | @dataclass \n   |  ^^^^^^^^^\n27 | class EvidenceReference:\n28 |     \"\"\"Evidence reference with source metadata.\"\"\"\n   |\n\nF821 Undefined name `Dict`\n  --> canonical_flow/analysis/artifact_generator.py:35:26\n   |\n33 |     confidence_score: float\n34 |     \n35 |     def to_dict(self) -> Dict[str, Any]:\n   |                          ^^^^\n36 |         \"\"\"Convert to dictionary with deterministic ordering.\"\"\"\n37 |         return OrderedDict([\n   |\n\nF821 Undefined name `Any`\n  --> canonical_flow/analysis/artifact_generator.py:35:36\n   |\n33 |     confidence_score: float\n34 |     \n35 |     def to_dict(self) -> Dict[str, Any]:\n   |                                    ^^^\n36 |         \"\"\"Convert to dictionary with deterministic ordering.\"\"\"\n37 |         return OrderedDict([\n   |\n\nF821 Undefined name `OrderedDict`\n  --> canonical_flow/analysis/artifact_generator.py:37:16\n   |\n35 |     def to_dict(self) -> Dict[str, Any]:\n36 |         \"\"\"Convert to dictionary with deterministic ordering.\"\"\"\n37 |         return OrderedDict([\n   |                ^^^^^^^^^^^\n38 |             (\"evidence_id\", self.evidence_id),\n39 |             (\"source_type\", self.source_type),\n   |\n\nF821 Undefined name `dataclass`\n  --> canonical_flow/analysis/artifact_generator.py:46:2\n   |\n46 | @dataclass\n   |  ^^^^^^^^^\n47 | class QuestionEvaluation:\n48 |     \"\"\"Question-level evaluation with evidence references.\"\"\"\n   |\n\nF821 Undefined name `List`\n  --> canonical_flow/analysis/artifact_generator.py:56:26\n   |\n54 |     page_reference_quality: float\n55 |     final_score: float\n56 |     evidence_references: List[EvidenceReference]\n   |                          ^^^^\n57 |     \n58 |     def to_dict(self) -> Dict[str, Any]:\n   |\n\nF821 Undefined name `Dict`\n  --> canonical_flow/analysis/artifact_generator.py:58:26\n   |\n56 |     evidence_references: List[EvidenceReference]\n57 |     \n58 |     def to_dict(self) -> Dict[str, Any]:\n   |                          ^^^^\n59 |         \"\"\"Convert to dictionary with deterministic ordering.\"\"\"\n60 |         return OrderedDict([\n   |\n\nF821 Undefined name `Any`\n  --> canonical_flow/analysis/artifact_generator.py:58:36\n   |\n56 |     evidence_references: List[EvidenceReference]\n57 |     \n58 |     def to_dict(self) -> Dict[str, Any]:\n   |                                    ^^^\n59 |         \"\"\"Convert to dictionary with deterministic ordering.\"\"\"\n60 |         return OrderedDict([\n   |\n\nF821 Undefined name `OrderedDict`\n  --> canonical_flow/analysis/artifact_generator.py:60:16\n   |\n58 |     def to_dict(self) -> Dict[str, Any]:\n59 |         \"\"\"Convert to dictionary with deterministic ordering.\"\"\"\n60 |         return OrderedDict([\n   |                ^^^^^^^^^^^\n61 |             (\"question_id\", self.question_id),\n62 |             (\"question_text\", self.question_text),\n   |\n\nF821 Undefined name `dataclass`\n  --> canonical_flow/analysis/artifact_generator.py:72:2\n   |\n72 | @dataclass\n   |  ^^^^^^^^^\n73 | class DimensionSummary:\n74 |     \"\"\"Dimension-level summary with weighted aggregations.\"\"\"\n   |\n\nF821 Undefined name `List`\n  --> canonical_flow/analysis/artifact_generator.py:77:16\n   |\n75 |     dimension_id: str\n76 |     dimension_name: str\n77 |     questions: List[QuestionEvaluation]\n   |                ^^^^\n78 |     weighted_average: float\n79 |     total_questions: int\n   |\n\nF821 Undefined name `Dict`\n  --> canonical_flow/analysis/artifact_generator.py:80:26\n   |\n78 |     weighted_average: float\n79 |     total_questions: int\n80 |     confidence_interval: Dict[str, float]\n   |                          ^^^^\n81 |     aggregation_weights: Dict[str, float]\n   |\n\nF821 Undefined name `Dict`\n  --> canonical_flow/analysis/artifact_generator.py:81:26\n   |\n79 |     total_questions: int\n80 |     confidence_interval: Dict[str, float]\n81 |     aggregation_weights: Dict[str, float]\n   |                          ^^^^\n82 |     \n83 |     def to_dict(self) -> Dict[str, Any]:\n   |\n\nF821 Undefined name `Dict`\n  --> canonical_flow/analysis/artifact_generator.py:83:26\n   |\n81 |     aggregation_weights: Dict[str, float]\n82 |     \n83 |     def to_dict(self) -> Dict[str, Any]:\n   |                          ^^^^\n84 |         \"\"\"Convert to dictionary with deterministic ordering.\"\"\"\n85 |         return OrderedDict([\n   |\n\nF821 Undefined name `Any`\n  --> canonical_flow/analysis/artifact_generator.py:83:36\n   |\n81 |     aggregation_weights: Dict[str, float]\n82 |     \n83 |     def to_dict(self) -> Dict[str, Any]:\n   |                                    ^^^\n84 |         \"\"\"Convert to dictionary with deterministic ordering.\"\"\"\n85 |         return OrderedDict([\n   |\n\nF821 Undefined name `OrderedDict`\n  --> canonical_flow/analysis/artifact_generator.py:85:16\n   |\n83 |     def to_dict(self) -> Dict[str, Any]:\n84 |         \"\"\"Convert to dictionary with deterministic ordering.\"\"\"\n85 |         return OrderedDict([\n   |                ^^^^^^^^^^^\n86 |             (\"dimension_id\", self.dimension_id),\n87 |             (\"dimension_name\", self.dimension_name),\n   |\n\nF821 Undefined name `dataclass`\n  --> canonical_flow/analysis/artifact_generator.py:96:2\n   |\n96 | @dataclass\n   |  ^^^^^^^^^\n97 | class PointSummary:\n98 |     \"\"\"Point-level summary with compliance classifications.\"\"\"\n   |\n\nF821 Undefined name `List`\n   --> canonical_flow/analysis/artifact_generator.py:101:17\n    |\n 99 |     point_id: int\n100 |     point_name: str\n101 |     dimensions: List[DimensionSummary]\n    |                 ^^^^\n102 |     final_score: float\n103 |     compliance_classification: str\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/analysis/artifact_generator.py:107:26\n    |\n105 |     cluster_id: str\n106 |     \n107 |     def to_dict(self) -> Dict[str, Any]:\n    |                          ^^^^\n108 |         \"\"\"Convert to dictionary with deterministic ordering.\"\"\"\n109 |         return OrderedDict([\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/analysis/artifact_generator.py:107:36\n    |\n105 |     cluster_id: str\n106 |     \n107 |     def to_dict(self) -> Dict[str, Any]:\n    |                                    ^^^\n108 |         \"\"\"Convert to dictionary with deterministic ordering.\"\"\"\n109 |         return OrderedDict([\n    |\n\nF821 Undefined name `OrderedDict`\n   --> canonical_flow/analysis/artifact_generator.py:109:16\n    |\n107 |     def to_dict(self) -> Dict[str, Any]:\n108 |         \"\"\"Convert to dictionary with deterministic ordering.\"\"\"\n109 |         return OrderedDict([\n    |                ^^^^^^^^^^^\n110 |             (\"point_id\", self.point_id),\n111 |             (\"point_name\", self.point_name),\n    |\n\nF821 Undefined name `dataclass`\n   --> canonical_flow/analysis/artifact_generator.py:120:2\n    |\n120 | @dataclass\n    |  ^^^^^^^^^\n121 | class MesoClusterAnalysis:\n122 |     \"\"\"Meso cluster analysis with cross-point linkages.\"\"\"\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/analysis/artifact_generator.py:125:13\n    |\n123 |     cluster_id: str\n124 |     cluster_name: str\n125 |     points: List[int]\n    |             ^^^^\n126 |     cross_point_linkages: Dict[str, List[str]]\n127 |     cluster_score: float\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/analysis/artifact_generator.py:126:27\n    |\n124 |     cluster_name: str\n125 |     points: List[int]\n126 |     cross_point_linkages: Dict[str, List[str]]\n    |                           ^^^^\n127 |     cluster_score: float\n128 |     coherence_metrics: Dict[str, float]\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/analysis/artifact_generator.py:126:37\n    |\n124 |     cluster_name: str\n125 |     points: List[int]\n126 |     cross_point_linkages: Dict[str, List[str]]\n    |                                     ^^^^\n127 |     cluster_score: float\n128 |     coherence_metrics: Dict[str, float]\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/analysis/artifact_generator.py:128:24\n    |\n126 |     cross_point_linkages: Dict[str, List[str]]\n127 |     cluster_score: float\n128 |     coherence_metrics: Dict[str, float]\n    |                        ^^^^\n129 |     evidence_density: Dict[str, int]\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/analysis/artifact_generator.py:129:23\n    |\n127 |     cluster_score: float\n128 |     coherence_metrics: Dict[str, float]\n129 |     evidence_density: Dict[str, int]\n    |                       ^^^^\n130 |     \n131 |     def to_dict(self) -> Dict[str, Any]:\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/analysis/artifact_generator.py:131:26\n    |\n129 |     evidence_density: Dict[str, int]\n130 |     \n131 |     def to_dict(self) -> Dict[str, Any]:\n    |                          ^^^^\n132 |         \"\"\"Convert to dictionary with deterministic ordering.\"\"\"\n133 |         return OrderedDict([\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/analysis/artifact_generator.py:131:36\n    |\n129 |     evidence_density: Dict[str, int]\n130 |     \n131 |     def to_dict(self) -> Dict[str, Any]:\n    |                                    ^^^\n132 |         \"\"\"Convert to dictionary with deterministic ordering.\"\"\"\n133 |         return OrderedDict([\n    |\n\nF821 Undefined name `OrderedDict`\n   --> canonical_flow/analysis/artifact_generator.py:133:16\n    |\n131 |     def to_dict(self) -> Dict[str, Any]:\n132 |         \"\"\"Convert to dictionary with deterministic ordering.\"\"\"\n133 |         return OrderedDict([\n    |                ^^^^^^^^^^^\n134 |             (\"cluster_id\", self.cluster_id),\n135 |             (\"cluster_name\", self.cluster_name),\n    |\n\nF821 Undefined name `dataclass`\n   --> canonical_flow/analysis/artifact_generator.py:144:2\n    |\n144 | @dataclass\n    |  ^^^^^^^^^\n145 | class MacroAlignment:\n146 |     \"\"\"Macro alignment with overall Dec\u00e1logo scores.\"\"\"\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/analysis/artifact_generator.py:149:21\n    |\n147 |     document_stem: str\n148 |     overall_decalogo_score: float\n149 |     cluster_scores: Dict[str, float]\n    |                     ^^^^\n150 |     dimensional_alignment: Dict[str, float]\n151 |     coverage_metrics: Dict[str, float]\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/analysis/artifact_generator.py:150:28\n    |\n148 |     overall_decalogo_score: float\n149 |     cluster_scores: Dict[str, float]\n150 |     dimensional_alignment: Dict[str, float]\n    |                            ^^^^\n151 |     coverage_metrics: Dict[str, float]\n152 |     compliance_distribution: Dict[str, int]\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/analysis/artifact_generator.py:151:23\n    |\n149 |     cluster_scores: Dict[str, float]\n150 |     dimensional_alignment: Dict[str, float]\n151 |     coverage_metrics: Dict[str, float]\n    |                       ^^^^\n152 |     compliance_distribution: Dict[str, int]\n153 |     recommendation_priority: List[str]\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/analysis/artifact_generator.py:152:30\n    |\n150 |     dimensional_alignment: Dict[str, float]\n151 |     coverage_metrics: Dict[str, float]\n152 |     compliance_distribution: Dict[str, int]\n    |                              ^^^^\n153 |     recommendation_priority: List[str]\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/analysis/artifact_generator.py:153:30\n    |\n151 |     coverage_metrics: Dict[str, float]\n152 |     compliance_distribution: Dict[str, int]\n153 |     recommendation_priority: List[str]\n    |                              ^^^^\n154 |     \n155 |     def to_dict(self) -> Dict[str, Any]:\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/analysis/artifact_generator.py:155:26\n    |\n153 |     recommendation_priority: List[str]\n154 |     \n155 |     def to_dict(self) -> Dict[str, Any]:\n    |                          ^^^^\n156 |         \"\"\"Convert to dictionary with deterministic ordering.\"\"\"\n157 |         return OrderedDict([\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/analysis/artifact_generator.py:155:36\n    |\n153 |     recommendation_priority: List[str]\n154 |     \n155 |     def to_dict(self) -> Dict[str, Any]:\n    |                                    ^^^\n156 |         \"\"\"Convert to dictionary with deterministic ordering.\"\"\"\n157 |         return OrderedDict([\n    |\n\nF821 Undefined name `OrderedDict`\n   --> canonical_flow/analysis/artifact_generator.py:157:16\n    |\n155 |     def to_dict(self) -> Dict[str, Any]:\n156 |         \"\"\"Convert to dictionary with deterministic ordering.\"\"\"\n157 |         return OrderedDict([\n    |                ^^^^^^^^^^^\n158 |             (\"document_stem\", self.document_stem),\n159 |             (\"overall_decalogo_score\", self.overall_decalogo_score),\n    |\n\nF821 Undefined name `Path`\n   --> canonical_flow/analysis/artifact_generator.py:181:27\n    |\n179 |             output_dir: Directory for artifact output\n180 |         \"\"\"\n181 |         self.output_dir = Path(output_dir)\n    |                           ^^^^\n182 |         self.output_dir.mkdir(parents=True, exist_ok=True)\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/analysis/artifact_generator.py:184:41\n    |\n182 |         self.output_dir.mkdir(parents=True, exist_ok=True)\n183 |         \n184 |     def write_json_artifact(self, data: Dict[str, Any], filepath: Path) -> None:\n    |                                         ^^^^\n185 |         \"\"\"\n186 |         Write JSON artifact with UTF-8 encoding and consistent formatting.\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/analysis/artifact_generator.py:184:51\n    |\n182 |         self.output_dir.mkdir(parents=True, exist_ok=True)\n183 |         \n184 |     def write_json_artifact(self, data: Dict[str, Any], filepath: Path) -> None:\n    |                                                   ^^^\n185 |         \"\"\"\n186 |         Write JSON artifact with UTF-8 encoding and consistent formatting.\n    |\n\nF821 Undefined name `Path`\n   --> canonical_flow/analysis/artifact_generator.py:184:67\n    |\n182 |         self.output_dir.mkdir(parents=True, exist_ok=True)\n183 |         \n184 |     def write_json_artifact(self, data: Dict[str, Any], filepath: Path) -> None:\n    |                                                                   ^^^^\n185 |         \"\"\"\n186 |         Write JSON artifact with UTF-8 encoding and consistent formatting.\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/analysis/artifact_generator.py:200:74\n    |\n198 |             raise\n199 |     \n200 |     def generate_question_artifacts(self, document_stem: str, questions: List[QuestionEvaluation]) -> None:\n    |                                                                          ^^^^\n201 |         \"\"\"\n202 |         Generate question-level evaluation artifacts.\n    |\n\nF821 Undefined name `OrderedDict`\n   --> canonical_flow/analysis/artifact_generator.py:217:25\n    |\n216 |         # Create deterministically ordered structure\n217 |         artifact_data = OrderedDict([\n    |                         ^^^^^^^^^^^\n218 |             (\"metadata\", OrderedDict([\n219 |                 (\"document_stem\", document_stem),\n    |\n\nF821 Undefined name `OrderedDict`\n   --> canonical_flow/analysis/artifact_generator.py:218:26\n    |\n216 |         # Create deterministically ordered structure\n217 |         artifact_data = OrderedDict([\n218 |             (\"metadata\", OrderedDict([\n    |                          ^^^^^^^^^^^\n219 |                 (\"document_stem\", document_stem),\n220 |                 (\"artifact_type\", \"question_evaluations\"),\n    |\n\nF821 Undefined name `datetime`\n   --> canonical_flow/analysis/artifact_generator.py:221:41\n    |\n219 |                 (\"document_stem\", document_stem),\n220 |                 (\"artifact_type\", \"question_evaluations\"),\n221 |                 (\"generated_timestamp\", datetime.utcnow().isoformat() + \"Z\"),\n    |                                         ^^^^^^^^\n222 |                 (\"total_questions\", len(questions)),\n223 |                 (\"dimensions_included\", sorted(dimensions_data.keys()))\n    |\n\nF821 Undefined name `OrderedDict`\n   --> canonical_flow/analysis/artifact_generator.py:225:40\n    |\n223 |                 (\"dimensions_included\", sorted(dimensions_data.keys()))\n224 |             ])),\n225 |             (\"questions_by_dimension\", OrderedDict())\n    |                                        ^^^^^^^^^^^\n226 |         ])\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/analysis/artifact_generator.py:237:76\n    |\n235 |         self.write_json_artifact(artifact_data, filepath)\n236 |     \n237 |     def generate_dimension_artifacts(self, document_stem: str, dimensions: List[DimensionSummary]) -> None:\n    |                                                                            ^^^^\n238 |         \"\"\"\n239 |         Generate dimension-level summary artifacts.\n    |\n\nF821 Undefined name `OrderedDict`\n   --> canonical_flow/analysis/artifact_generator.py:245:25\n    |\n243 |             dimensions: List of dimension summaries\n244 |         \"\"\"\n245 |         artifact_data = OrderedDict([\n    |                         ^^^^^^^^^^^\n246 |             (\"metadata\", OrderedDict([\n247 |                 (\"document_stem\", document_stem),\n    |\n\nF821 Undefined name `OrderedDict`\n   --> canonical_flow/analysis/artifact_generator.py:246:26\n    |\n244 |         \"\"\"\n245 |         artifact_data = OrderedDict([\n246 |             (\"metadata\", OrderedDict([\n    |                          ^^^^^^^^^^^\n247 |                 (\"document_stem\", document_stem),\n248 |                 (\"artifact_type\", \"dimension_summaries\"),\n    |\n\nF821 Undefined name `datetime`\n   --> canonical_flow/analysis/artifact_generator.py:249:41\n    |\n247 |                 (\"document_stem\", document_stem),\n248 |                 (\"artifact_type\", \"dimension_summaries\"),\n249 |                 (\"generated_timestamp\", datetime.utcnow().isoformat() + \"Z\"),\n    |                                         ^^^^^^^^\n250 |                 (\"total_dimensions\", len(dimensions)),\n251 |                 (\"dimensions_included\", sorted([d.dimension_id for d in dimensions]))\n    |\n\nF821 Undefined name `OrderedDict`\n   --> canonical_flow/analysis/artifact_generator.py:253:37\n    |\n251 |                 (\"dimensions_included\", sorted([d.dimension_id for d in dimensions]))\n252 |             ])),\n253 |             (\"dimension_summaries\", OrderedDict())\n    |                                     ^^^^^^^^^^^\n254 |         ])\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/analysis/artifact_generator.py:263:68\n    |\n261 |         self.write_json_artifact(artifact_data, filepath)\n262 |     \n263 |     def generate_point_artifacts(self, document_stem: str, points: List[PointSummary]) -> None:\n    |                                                                    ^^^^\n264 |         \"\"\"\n265 |         Generate point-level summary artifacts.\n    |\n\nF821 Undefined name `OrderedDict`\n   --> canonical_flow/analysis/artifact_generator.py:271:25\n    |\n269 |             points: List of point summaries\n270 |         \"\"\"\n271 |         artifact_data = OrderedDict([\n    |                         ^^^^^^^^^^^\n272 |             (\"metadata\", OrderedDict([\n273 |                 (\"document_stem\", document_stem),\n    |\n\nF821 Undefined name `OrderedDict`\n   --> canonical_flow/analysis/artifact_generator.py:272:26\n    |\n270 |         \"\"\"\n271 |         artifact_data = OrderedDict([\n272 |             (\"metadata\", OrderedDict([\n    |                          ^^^^^^^^^^^\n273 |                 (\"document_stem\", document_stem),\n274 |                 (\"artifact_type\", \"point_summaries\"),\n    |\n\nF821 Undefined name `datetime`\n   --> canonical_flow/analysis/artifact_generator.py:275:41\n    |\n273 |                 (\"document_stem\", document_stem),\n274 |                 (\"artifact_type\", \"point_summaries\"),\n275 |                 (\"generated_timestamp\", datetime.utcnow().isoformat() + \"Z\"),\n    |                                         ^^^^^^^^\n276 |                 (\"total_points\", len(points)),\n277 |                 (\"points_included\", sorted([p.point_id for p in points]))\n    |\n\nF821 Undefined name `OrderedDict`\n   --> canonical_flow/analysis/artifact_generator.py:279:33\n    |\n277 |                 (\"points_included\", sorted([p.point_id for p in points]))\n278 |             ])),\n279 |             (\"point_summaries\", OrderedDict())\n    |                                 ^^^^^^^^^^^\n280 |         ])\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/analysis/artifact_generator.py:289:69\n    |\n287 |         self.write_json_artifact(artifact_data, filepath)\n288 |     \n289 |     def generate_meso_artifacts(self, document_stem: str, clusters: List[MesoClusterAnalysis]) -> None:\n    |                                                                     ^^^^\n290 |         \"\"\"\n291 |         Generate meso cluster analysis artifacts.\n    |\n\nF821 Undefined name `OrderedDict`\n   --> canonical_flow/analysis/artifact_generator.py:297:25\n    |\n295 |             clusters: List of meso cluster analyses\n296 |         \"\"\"\n297 |         artifact_data = OrderedDict([\n    |                         ^^^^^^^^^^^\n298 |             (\"metadata\", OrderedDict([\n299 |                 (\"document_stem\", document_stem),\n    |\n\nF821 Undefined name `OrderedDict`\n   --> canonical_flow/analysis/artifact_generator.py:298:26\n    |\n296 |         \"\"\"\n297 |         artifact_data = OrderedDict([\n298 |             (\"metadata\", OrderedDict([\n    |                          ^^^^^^^^^^^\n299 |                 (\"document_stem\", document_stem),\n300 |                 (\"artifact_type\", \"meso_cluster_analysis\"),\n    |\n\nF821 Undefined name `datetime`\n   --> canonical_flow/analysis/artifact_generator.py:301:41\n    |\n299 |                 (\"document_stem\", document_stem),\n300 |                 (\"artifact_type\", \"meso_cluster_analysis\"),\n301 |                 (\"generated_timestamp\", datetime.utcnow().isoformat() + \"Z\"),\n    |                                         ^^^^^^^^\n302 |                 (\"total_clusters\", len(clusters)),\n303 |                 (\"clusters_included\", sorted([c.cluster_id for c in clusters]))\n    |\n\nF821 Undefined name `OrderedDict`\n   --> canonical_flow/analysis/artifact_generator.py:305:34\n    |\n303 |                 (\"clusters_included\", sorted([c.cluster_id for c in clusters]))\n304 |             ])),\n305 |             (\"cluster_analysis\", OrderedDict())\n    |                                  ^^^^^^^^^^^\n306 |         ])\n    |\n\nF821 Undefined name `OrderedDict`\n   --> canonical_flow/analysis/artifact_generator.py:323:25\n    |\n321 |             macro_alignment: Macro alignment analysis\n322 |         \"\"\"\n323 |         artifact_data = OrderedDict([\n    |                         ^^^^^^^^^^^\n324 |             (\"metadata\", OrderedDict([\n325 |                 (\"document_stem\", document_stem),\n    |\n\nF821 Undefined name `OrderedDict`\n   --> canonical_flow/analysis/artifact_generator.py:324:26\n    |\n322 |         \"\"\"\n323 |         artifact_data = OrderedDict([\n324 |             (\"metadata\", OrderedDict([\n    |                          ^^^^^^^^^^^\n325 |                 (\"document_stem\", document_stem),\n326 |                 (\"artifact_type\", \"macro_alignment\"),\n    |\n\nF821 Undefined name `datetime`\n   --> canonical_flow/analysis/artifact_generator.py:327:41\n    |\n325 |                 (\"document_stem\", document_stem),\n326 |                 (\"artifact_type\", \"macro_alignment\"),\n327 |                 (\"generated_timestamp\", datetime.utcnow().isoformat() + \"Z\")\n    |                                         ^^^^^^^^\n328 |             ])),\n329 |             (\"macro_alignment\", macro_alignment.to_dict())\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/analysis/artifact_generator.py:338:20\n    |\n336 |         self, \n337 |         document_stem: str,\n338 |         questions: List[QuestionEvaluation],\n    |                    ^^^^\n339 |         dimensions: List[DimensionSummary],\n340 |         points: List[PointSummary],\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/analysis/artifact_generator.py:339:21\n    |\n337 |         document_stem: str,\n338 |         questions: List[QuestionEvaluation],\n339 |         dimensions: List[DimensionSummary],\n    |                     ^^^^\n340 |         points: List[PointSummary],\n341 |         clusters: List[MesoClusterAnalysis],\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/analysis/artifact_generator.py:340:17\n    |\n338 |         questions: List[QuestionEvaluation],\n339 |         dimensions: List[DimensionSummary],\n340 |         points: List[PointSummary],\n    |                 ^^^^\n341 |         clusters: List[MesoClusterAnalysis],\n342 |         macro_alignment: MacroAlignment\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/analysis/artifact_generator.py:341:19\n    |\n339 |         dimensions: List[DimensionSummary],\n340 |         points: List[PointSummary],\n341 |         clusters: List[MesoClusterAnalysis],\n    |                   ^^^^\n342 |         macro_alignment: MacroAlignment\n343 |     ) -> Dict[str, str]:\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/analysis/artifact_generator.py:343:10\n    |\n341 |         clusters: List[MesoClusterAnalysis],\n342 |         macro_alignment: MacroAlignment\n343 |     ) -> Dict[str, str]:\n    |          ^^^^\n344 |         \"\"\"\n345 |         Generate all artifact types for a document.\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/analysis/artifact_generator.py:385:57\n    |\n383 |         return artifacts\n384 |     \n385 |     def validate_artifacts(self, document_stem: str) -> Dict[str, bool]:\n    |                                                         ^^^^\n386 |         \"\"\"\n387 |         Validate that all expected artifacts exist and are valid JSON.\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/analysis/artifact_generator.py:404:57\n    |\n402 |         return validation_results\n403 |     \n404 |     def discover_artifacts(self, pattern: str = \"*\") -> Dict[str, List[str]]:\n    |                                                         ^^^^\n405 |         \"\"\"\n406 |         Discover existing artifacts using filename conventions.\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/analysis/artifact_generator.py:404:67\n    |\n402 |         return validation_results\n403 |     \n404 |     def discover_artifacts(self, pattern: str = \"*\") -> Dict[str, List[str]]:\n    |                                                                   ^^^^\n405 |         \"\"\"\n406 |         Discover existing artifacts using filename conventions.\n    |\n\nF821 Undefined name `Path`\n   --> canonical_flow/analysis/artifact_generator.py:443:45\n    |\n441 |         return \"UNKNOWN\"\n442 |     \n443 |     def _validate_json_file(self, filepath: Path) -> bool:\n    |                                             ^^^^\n444 |         \"\"\"Validate that a JSON file exists and is valid.\"\"\"\n445 |         try:\n    |\n\nF821 Undefined name `Tuple`\n   --> canonical_flow/analysis/artifact_generator.py:457:62\n    |\n457 | def create_sample_data(document_stem: str = \"SAMPLE-DOC\") -> Tuple[\n    |                                                              ^^^^^\n458 |     List[QuestionEvaluation],\n459 |     List[DimensionSummary], \n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/analysis/artifact_generator.py:458:5\n    |\n457 | def create_sample_data(document_stem: str = \"SAMPLE-DOC\") -> Tuple[\n458 |     List[QuestionEvaluation],\n    |     ^^^^\n459 |     List[DimensionSummary], \n460 |     List[PointSummary],\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/analysis/artifact_generator.py:459:5\n    |\n457 | def create_sample_data(document_stem: str = \"SAMPLE-DOC\") -> Tuple[\n458 |     List[QuestionEvaluation],\n459 |     List[DimensionSummary], \n    |     ^^^^\n460 |     List[PointSummary],\n461 |     List[MesoClusterAnalysis],\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/analysis/artifact_generator.py:460:5\n    |\n458 |     List[QuestionEvaluation],\n459 |     List[DimensionSummary], \n460 |     List[PointSummary],\n    |     ^^^^\n461 |     List[MesoClusterAnalysis],\n462 |     MacroAlignment\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/analysis/artifact_generator.py:461:5\n    |\n459 |     List[DimensionSummary], \n460 |     List[PointSummary],\n461 |     List[MesoClusterAnalysis],\n    |     ^^^^\n462 |     MacroAlignment\n463 | ]:\n    |\n\nF401 [*] `os` imported but unused\n  --> canonical_flow/analysis/audit_logger.py:11:8\n   |\n 9 | import traceback\n10 | import time\n11 | import os\n   |        ^^\n12 | import sys\n13 | # # # from datetime import datetime, timezone  # Module not found  # Module not found  # Module not found\n   |\nhelp: Remove unused import: `os`\n\nF401 [*] `sys` imported but unused\n  --> canonical_flow/analysis/audit_logger.py:12:8\n   |\n10 | import time\n11 | import os\n12 | import sys\n   |        ^^^\n13 | # # # from datetime import datetime, timezone  # Module not found  # Module not found  # Module not found\n14 | # # # from typing import Any, Dict, List, Optional, Union  # Module not found  # Module not found  # Module not found\n   |\nhelp: Remove unused import: `sys`\n\nF821 Undefined name `Enum`\n  --> canonical_flow/analysis/audit_logger.py:28:22\n   |\n28 | class AuditEventType(Enum):\n   |                      ^^^^\n29 |     \"\"\"Types of audit events that can be recorded.\"\"\"\n30 |     COMPONENT_START = \"component_start\"\n   |\n\nF821 Undefined name `dataclass`\n  --> canonical_flow/analysis/audit_logger.py:38:2\n   |\n38 | @dataclass\n   |  ^^^^^^^^^\n39 | class MemoryMetrics:\n40 |     \"\"\"Memory usage metrics captured during execution.\"\"\"\n   |\n\nF821 Undefined name `dataclass`\n  --> canonical_flow/analysis/audit_logger.py:47:2\n   |\n47 | @dataclass \n   |  ^^^^^^^^^\n48 | class PerformanceMetrics:\n49 |     \"\"\"Performance metrics for component execution.\"\"\"\n   |\n\nF821 Undefined name `Optional`\n  --> canonical_flow/analysis/audit_logger.py:51:15\n   |\n49 |     \"\"\"Performance metrics for component execution.\"\"\"\n50 |     start_time: float\n51 |     end_time: Optional[float] = None\n   |               ^^^^^^^^\n52 |     duration_seconds: Optional[float] = None\n53 |     cpu_percent: float = 0.0\n   |\n\nF821 Undefined name `Optional`\n  --> canonical_flow/analysis/audit_logger.py:52:23\n   |\n50 |     start_time: float\n51 |     end_time: Optional[float] = None\n52 |     duration_seconds: Optional[float] = None\n   |                       ^^^^^^^^\n53 |     cpu_percent: float = 0.0\n54 |     memory_metrics: Optional[MemoryMetrics] = None\n   |\n\nF821 Undefined name `Optional`\n  --> canonical_flow/analysis/audit_logger.py:54:21\n   |\n52 |     duration_seconds: Optional[float] = None\n53 |     cpu_percent: float = 0.0\n54 |     memory_metrics: Optional[MemoryMetrics] = None\n   |                     ^^^^^^^^\n55 |     io_operations: Dict[str, int] = field(default_factory=dict)\n   |\n\nF821 Undefined name `Dict`\n  --> canonical_flow/analysis/audit_logger.py:55:20\n   |\n53 |     cpu_percent: float = 0.0\n54 |     memory_metrics: Optional[MemoryMetrics] = None\n55 |     io_operations: Dict[str, int] = field(default_factory=dict)\n   |                    ^^^^\n   |\n\nF821 Undefined name `field`\n  --> canonical_flow/analysis/audit_logger.py:55:37\n   |\n53 |     cpu_percent: float = 0.0\n54 |     memory_metrics: Optional[MemoryMetrics] = None\n55 |     io_operations: Dict[str, int] = field(default_factory=dict)\n   |                                     ^^^^^\n   |\n\nF821 Undefined name `dataclass`\n  --> canonical_flow/analysis/audit_logger.py:58:2\n   |\n58 | @dataclass\n   |  ^^^^^^^^^\n59 | class InputOutputSchema:\n60 |     \"\"\"Schema information for component inputs and outputs.\"\"\"\n   |\n\nF821 Undefined name `List`\n  --> canonical_flow/analysis/audit_logger.py:62:18\n   |\n60 |     \"\"\"Schema information for component inputs and outputs.\"\"\"\n61 |     data_type: str\n62 |     schema_keys: List[str] = field(default_factory=list)\n   |                  ^^^^\n63 |     data_size_bytes: int = 0\n64 |     record_count: int = 0\n   |\n\nF821 Undefined name `field`\n  --> canonical_flow/analysis/audit_logger.py:62:30\n   |\n60 |     \"\"\"Schema information for component inputs and outputs.\"\"\"\n61 |     data_type: str\n62 |     schema_keys: List[str] = field(default_factory=list)\n   |                              ^^^^^\n63 |     data_size_bytes: int = 0\n64 |     record_count: int = 0\n   |\n\nF821 Undefined name `Optional`\n  --> canonical_flow/analysis/audit_logger.py:66:18\n   |\n64 |     record_count: int = 0\n65 |     validation_status: str = \"unknown\"\n66 |     sample_data: Optional[Dict[str, Any]] = None\n   |                  ^^^^^^^^\n   |\n\nF821 Undefined name `Dict`\n  --> canonical_flow/analysis/audit_logger.py:66:27\n   |\n64 |     record_count: int = 0\n65 |     validation_status: str = \"unknown\"\n66 |     sample_data: Optional[Dict[str, Any]] = None\n   |                           ^^^^\n   |\n\nF821 Undefined name `Any`\n  --> canonical_flow/analysis/audit_logger.py:66:37\n   |\n64 |     record_count: int = 0\n65 |     validation_status: str = \"unknown\"\n66 |     sample_data: Optional[Dict[str, Any]] = None\n   |                                     ^^^\n   |\n\nF821 Undefined name `dataclass`\n  --> canonical_flow/analysis/audit_logger.py:69:2\n   |\n69 | @dataclass\n   |  ^^^^^^^^^\n70 | class ErrorDetails:\n71 |     \"\"\"Detailed error information with full stack trace.\"\"\"\n   |\n\nF821 Undefined name `Dict`\n  --> canonical_flow/analysis/audit_logger.py:75:24\n   |\n73 |     error_message: str\n74 |     stack_trace: str\n75 |     component_context: Dict[str, Any] = field(default_factory=dict)\n   |                        ^^^^\n76 |     recovery_attempted: bool = False\n77 |     recovery_successful: bool = False\n   |\n\nF821 Undefined name `Any`\n  --> canonical_flow/analysis/audit_logger.py:75:34\n   |\n73 |     error_message: str\n74 |     stack_trace: str\n75 |     component_context: Dict[str, Any] = field(default_factory=dict)\n   |                                  ^^^\n76 |     recovery_attempted: bool = False\n77 |     recovery_successful: bool = False\n   |\n\nF821 Undefined name `field`\n  --> canonical_flow/analysis/audit_logger.py:75:41\n   |\n73 |     error_message: str\n74 |     stack_trace: str\n75 |     component_context: Dict[str, Any] = field(default_factory=dict)\n   |                                         ^^^^^\n76 |     recovery_attempted: bool = False\n77 |     recovery_successful: bool = False\n   |\n\nF821 Undefined name `dataclass`\n  --> canonical_flow/analysis/audit_logger.py:80:2\n   |\n80 | @dataclass \n   |  ^^^^^^^^^\n81 | class AuditEvent:\n82 |     \"\"\"Individual audit event record.\"\"\"\n   |\n\nF821 Undefined name `Optional`\n  --> canonical_flow/analysis/audit_logger.py:88:26\n   |\n86 |     event_type: AuditEventType\n87 |     timestamp: str\n88 |     performance_metrics: Optional[PerformanceMetrics] = None\n   |                          ^^^^^^^^\n89 |     input_schema: Optional[InputOutputSchema] = None\n90 |     output_schema: Optional[InputOutputSchema] = None\n   |\n\nF821 Undefined name `Optional`\n  --> canonical_flow/analysis/audit_logger.py:89:19\n   |\n87 |     timestamp: str\n88 |     performance_metrics: Optional[PerformanceMetrics] = None\n89 |     input_schema: Optional[InputOutputSchema] = None\n   |                   ^^^^^^^^\n90 |     output_schema: Optional[InputOutputSchema] = None\n91 |     error_details: Optional[ErrorDetails] = None\n   |\n\nF821 Undefined name `Optional`\n  --> canonical_flow/analysis/audit_logger.py:90:20\n   |\n88 |     performance_metrics: Optional[PerformanceMetrics] = None\n89 |     input_schema: Optional[InputOutputSchema] = None\n90 |     output_schema: Optional[InputOutputSchema] = None\n   |                    ^^^^^^^^\n91 |     error_details: Optional[ErrorDetails] = None\n92 |     context_data: Dict[str, Any] = field(default_factory=dict)\n   |\n\nF821 Undefined name `Optional`\n  --> canonical_flow/analysis/audit_logger.py:91:20\n   |\n89 |     input_schema: Optional[InputOutputSchema] = None\n90 |     output_schema: Optional[InputOutputSchema] = None\n91 |     error_details: Optional[ErrorDetails] = None\n   |                    ^^^^^^^^\n92 |     context_data: Dict[str, Any] = field(default_factory=dict)\n93 |     traceability_id: Optional[str] = None\n   |\n\nF821 Undefined name `Dict`\n  --> canonical_flow/analysis/audit_logger.py:92:19\n   |\n90 |     output_schema: Optional[InputOutputSchema] = None\n91 |     error_details: Optional[ErrorDetails] = None\n92 |     context_data: Dict[str, Any] = field(default_factory=dict)\n   |                   ^^^^\n93 |     traceability_id: Optional[str] = None\n   |\n\nF821 Undefined name `Any`\n  --> canonical_flow/analysis/audit_logger.py:92:29\n   |\n90 |     output_schema: Optional[InputOutputSchema] = None\n91 |     error_details: Optional[ErrorDetails] = None\n92 |     context_data: Dict[str, Any] = field(default_factory=dict)\n   |                             ^^^\n93 |     traceability_id: Optional[str] = None\n   |\n\nF821 Undefined name `field`\n  --> canonical_flow/analysis/audit_logger.py:92:36\n   |\n90 |     output_schema: Optional[InputOutputSchema] = None\n91 |     error_details: Optional[ErrorDetails] = None\n92 |     context_data: Dict[str, Any] = field(default_factory=dict)\n   |                                    ^^^^^\n93 |     traceability_id: Optional[str] = None\n   |\n\nF821 Undefined name `Optional`\n  --> canonical_flow/analysis/audit_logger.py:93:22\n   |\n91 |     error_details: Optional[ErrorDetails] = None\n92 |     context_data: Dict[str, Any] = field(default_factory=dict)\n93 |     traceability_id: Optional[str] = None\n   |                      ^^^^^^^^\n   |\n\nF821 Undefined name `Optional`\n   --> canonical_flow/analysis/audit_logger.py:104:41\n    |\n102 |     \"\"\"\n103 |     \n104 |     def __init__(self, audit_file_path: Optional[str] = None):\n    |                                         ^^^^^^^^\n105 |         \"\"\"Initialize the audit logger with configurable output path.\"\"\"\n106 |         if audit_file_path is None:\n    |\n\nF821 Undefined name `Path`\n   --> canonical_flow/analysis/audit_logger.py:109:32\n    |\n107 |             audit_file_path = \"canonical_flow/analysis/_audit.json\"\n108 |         \n109 |         self.audit_file_path = Path(audit_file_path)\n    |                                ^^^^\n110 |         self.audit_file_path.parent.mkdir(parents=True, exist_ok=True)\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/analysis/audit_logger.py:126:31\n    |\n124 |         # Current execution context\n125 |         self.current_execution_id = None\n126 |         self.component_stack: List[str] = []\n    |                               ^^^^\n127 |         self.events: List[AuditEvent] = []\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/analysis/audit_logger.py:127:22\n    |\n125 |         self.current_execution_id = None\n126 |         self.component_stack: List[str] = []\n127 |         self.events: List[AuditEvent] = []\n    |                      ^^^^\n128 |         \n129 |         # Performance tracking\n    |\n\nF821 Undefined name `datetime`\n   --> canonical_flow/analysis/audit_logger.py:151:16\n    |\n149 |     def _generate_timestamp(self) -> str:\n150 |         \"\"\"Generate consistent ISO 8601 timestamp with timezone.\"\"\"\n151 |         return datetime.now(timezone.utc).isoformat()\n    |                ^^^^^^^^\n152 |     \n153 |     def _capture_memory_metrics(self) -> MemoryMetrics:\n    |\n\nF821 Undefined name `timezone`\n   --> canonical_flow/analysis/audit_logger.py:151:29\n    |\n149 |     def _generate_timestamp(self) -> str:\n150 |         \"\"\"Generate consistent ISO 8601 timestamp with timezone.\"\"\"\n151 |         return datetime.now(timezone.utc).isoformat()\n    |                             ^^^^^^^^\n152 |     \n153 |     def _capture_memory_metrics(self) -> MemoryMetrics:\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/analysis/audit_logger.py:179:42\n    |\n177 |         )\n178 |     \n179 |     def _analyze_data_schema(self, data: Any, sample_size: int = 3) -> InputOutputSchema:\n    |                                          ^^^\n180 |         \"\"\"Analyze data structure and generate schema information.\"\"\"\n181 |         if data is None:\n    |\n\nF821 Undefined name `Optional`\n   --> canonical_flow/analysis/audit_logger.py:236:40\n    |\n234 |                   component_name: str,\n235 |                   event_type: AuditEventType,\n236 |                   performance_metrics: Optional[PerformanceMetrics] = None,\n    |                                        ^^^^^^^^\n237 |                   input_schema: Optional[InputOutputSchema] = None,\n238 |                   output_schema: Optional[InputOutputSchema] = None,\n    |\n\nF821 Undefined name `Optional`\n   --> canonical_flow/analysis/audit_logger.py:237:33\n    |\n235 |                   event_type: AuditEventType,\n236 |                   performance_metrics: Optional[PerformanceMetrics] = None,\n237 |                   input_schema: Optional[InputOutputSchema] = None,\n    |                                 ^^^^^^^^\n238 |                   output_schema: Optional[InputOutputSchema] = None,\n239 |                   error_details: Optional[ErrorDetails] = None,\n    |\n\nF821 Undefined name `Optional`\n   --> canonical_flow/analysis/audit_logger.py:238:34\n    |\n236 |                   performance_metrics: Optional[PerformanceMetrics] = None,\n237 |                   input_schema: Optional[InputOutputSchema] = None,\n238 |                   output_schema: Optional[InputOutputSchema] = None,\n    |                                  ^^^^^^^^\n239 |                   error_details: Optional[ErrorDetails] = None,\n240 |                   context_data: Optional[Dict[str, Any]] = None,\n    |\n\nF821 Undefined name `Optional`\n   --> canonical_flow/analysis/audit_logger.py:239:34\n    |\n237 |                   input_schema: Optional[InputOutputSchema] = None,\n238 |                   output_schema: Optional[InputOutputSchema] = None,\n239 |                   error_details: Optional[ErrorDetails] = None,\n    |                                  ^^^^^^^^\n240 |                   context_data: Optional[Dict[str, Any]] = None,\n241 |                   traceability_id: Optional[str] = None) -> str:\n    |\n\nF821 Undefined name `Optional`\n   --> canonical_flow/analysis/audit_logger.py:240:33\n    |\n238 |                   output_schema: Optional[InputOutputSchema] = None,\n239 |                   error_details: Optional[ErrorDetails] = None,\n240 |                   context_data: Optional[Dict[str, Any]] = None,\n    |                                 ^^^^^^^^\n241 |                   traceability_id: Optional[str] = None) -> str:\n242 |         \"\"\"Log an individual audit event.\"\"\"\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/analysis/audit_logger.py:240:42\n    |\n238 |                   output_schema: Optional[InputOutputSchema] = None,\n239 |                   error_details: Optional[ErrorDetails] = None,\n240 |                   context_data: Optional[Dict[str, Any]] = None,\n    |                                          ^^^^\n241 |                   traceability_id: Optional[str] = None) -> str:\n242 |         \"\"\"Log an individual audit event.\"\"\"\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/analysis/audit_logger.py:240:52\n    |\n238 |                   output_schema: Optional[InputOutputSchema] = None,\n239 |                   error_details: Optional[ErrorDetails] = None,\n240 |                   context_data: Optional[Dict[str, Any]] = None,\n    |                                                    ^^^\n241 |                   traceability_id: Optional[str] = None) -> str:\n242 |         \"\"\"Log an individual audit event.\"\"\"\n    |\n\nF821 Undefined name `Optional`\n   --> canonical_flow/analysis/audit_logger.py:241:36\n    |\n239 |                   error_details: Optional[ErrorDetails] = None,\n240 |                   context_data: Optional[Dict[str, Any]] = None,\n241 |                   traceability_id: Optional[str] = None) -> str:\n    |                                    ^^^^^^^^\n242 |         \"\"\"Log an individual audit event.\"\"\"\n    |\n\nF821 Undefined name `contextmanager`\n   --> canonical_flow/analysis/audit_logger.py:263:6\n    |\n261 |         return event_id\n262 |     \n263 |     @contextmanager\n    |      ^^^^^^^^^^^^^^\n264 |     def audit_component_execution(self, \n265 |                                  component_code: str,\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/analysis/audit_logger.py:266:46\n    |\n264 |     def audit_component_execution(self, \n265 |                                  component_code: str,\n266 |                                  input_data: Any = None,\n    |                                              ^^^\n267 |                                  context: Optional[Dict[str, Any]] = None):\n268 |         \"\"\"\n    |\n\nF821 Undefined name `Optional`\n   --> canonical_flow/analysis/audit_logger.py:267:43\n    |\n265 |                                  component_code: str,\n266 |                                  input_data: Any = None,\n267 |                                  context: Optional[Dict[str, Any]] = None):\n    |                                           ^^^^^^^^\n268 |         \"\"\"\n269 |         Context manager for automatically auditing component execution.\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/analysis/audit_logger.py:267:52\n    |\n265 |                                  component_code: str,\n266 |                                  input_data: Any = None,\n267 |                                  context: Optional[Dict[str, Any]] = None):\n    |                                                    ^^^^\n268 |         \"\"\"\n269 |         Context manager for automatically auditing component execution.\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/analysis/audit_logger.py:267:62\n    |\n265 |                                  component_code: str,\n266 |                                  input_data: Any = None,\n267 |                                  context: Optional[Dict[str, Any]] = None):\n    |                                                              ^^^\n268 |         \"\"\"\n269 |         Context manager for automatically auditing component execution.\n    |\n\nF841 Local variable `start_memory` is assigned to but never used\n   --> canonical_flow/analysis/audit_logger.py:282:9\n    |\n280 |         # Start metrics capture\n281 |         start_time = time.time()\n282 |         start_memory = self._capture_memory_metrics()\n    |         ^^^^^^^^^^^^\n283 |         \n284 |         # Analyze input data\n    |\nhelp: Remove assignment to unused variable `start_memory`\n\nF821 Undefined name `Any`\n   --> canonical_flow/analysis/audit_logger.py:303:47\n    |\n301 |                 self.additional_metrics = {}\n302 |             \n303 |             def set_output(self, output_data: Any):\n    |                                               ^^^\n304 |                 \"\"\"Set the output data for schema analysis.\"\"\"\n305 |                 self.output_data = output_data\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/analysis/audit_logger.py:307:52\n    |\n305 |                 self.output_data = output_data\n306 |             \n307 |             def add_metric(self, name: str, value: Any):\n    |                                                    ^^^\n308 |                 \"\"\"Add additional performance metric.\"\"\"\n309 |                 self.additional_metrics[name] = value\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/analysis/audit_logger.py:392:39\n    |\n390 |                 self.component_stack.pop()\n391 |     \n392 |     def serialize_audit_data(self) -> Dict[str, Any]:\n    |                                       ^^^^\n393 |         \"\"\"Serialize all audit data to a structured format.\"\"\"\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/analysis/audit_logger.py:392:49\n    |\n390 |                 self.component_stack.pop()\n391 |     \n392 |     def serialize_audit_data(self) -> Dict[str, Any]:\n    |                                                 ^^^\n393 |         \"\"\"Serialize all audit data to a structured format.\"\"\"\n    |\n\nF821 Undefined name `Optional`\n   --> canonical_flow/analysis/audit_logger.py:542:64\n    |\n540 |         return str(self.audit_file_path)\n541 |     \n542 |     def validate_audit_completeness(self, expected_components: Optional[List[str]] = None) -> Dict[str, Any]:\n    |                                                                ^^^^^^^^\n543 |         \"\"\"\n544 |         Validate that the audit data contains complete execution traces.\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/analysis/audit_logger.py:542:73\n    |\n540 |         return str(self.audit_file_path)\n541 |     \n542 |     def validate_audit_completeness(self, expected_components: Optional[List[str]] = None) -> Dict[str, Any]:\n    |                                                                         ^^^^\n543 |         \"\"\"\n544 |         Validate that the audit data contains complete execution traces.\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/analysis/audit_logger.py:542:95\n    |\n540 |         return str(self.audit_file_path)\n541 |     \n542 |     def validate_audit_completeness(self, expected_components: Optional[List[str]] = None) -> Dict[str, Any]:\n    |                                                                                               ^^^^\n543 |         \"\"\"\n544 |         Validate that the audit data contains complete execution traces.\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/analysis/audit_logger.py:542:105\n    |\n540 |         return str(self.audit_file_path)\n541 |     \n542 |     def validate_audit_completeness(self, expected_components: Optional[List[str]] = None) -> Dict[str, Any]:\n    |                                                                                                         ^^^\n543 |         \"\"\"\n544 |         Validate that the audit data contains complete execution traces.\n    |\n\nF821 Undefined name `Optional`\n   --> canonical_flow/analysis/audit_logger.py:589:23\n    |\n588 | # Singleton instance for global access\n589 | _global_audit_logger: Optional[AuditLogger] = None\n    |                       ^^^^^^^^\n590 |\n591 | def get_audit_logger() -> AuditLogger:\n    |\n\nF821 Undefined name `Path`\n  --> canonical_flow/analysis/audit_validation.py:29:32\n   |\n28 |     def __init__(self, audit_file_path: str = \"canonical_flow/analysis/_audit.json\"):\n29 |         self.audit_file_path = Path(audit_file_path)\n   |                                ^^^^\n30 |         self.expected_components = {\n31 |             \"13A\": \"adaptive_analyzer\",\n   |\n\nF821 Undefined name `Dict`\n  --> canonical_flow/analysis/audit_validation.py:45:34\n   |\n43 |         return self.audit_file_path.exists()\n44 |     \n45 |     def load_audit_data(self) -> Dict[str, Any]:\n   |                                  ^^^^\n46 | # # #         \"\"\"Load audit data from the JSON file.\"\"\"  # Module not found  # Module not found  # Module not found\n47 |         if not self.validate_audit_file_exists():\n   |\n\nF821 Undefined name `Any`\n  --> canonical_flow/analysis/audit_validation.py:45:44\n   |\n43 |         return self.audit_file_path.exists()\n44 |     \n45 |     def load_audit_data(self) -> Dict[str, Any]:\n   |                                            ^^^\n46 | # # #         \"\"\"Load audit data from the JSON file.\"\"\"  # Module not found  # Module not found  # Module not found\n47 |         if not self.validate_audit_file_exists():\n   |\n\nF821 Undefined name `Dict`\n  --> canonical_flow/analysis/audit_validation.py:58:52\n   |\n56 |             raise AuditValidationError(f\"Error reading audit file: {e}\")\n57 |     \n58 |     def validate_audit_structure(self, audit_data: Dict[str, Any]) -> List[str]:\n   |                                                    ^^^^\n59 |         \"\"\"\n60 |         Validate the basic structure of audit data.\n   |\n\nF821 Undefined name `Any`\n  --> canonical_flow/analysis/audit_validation.py:58:62\n   |\n56 |             raise AuditValidationError(f\"Error reading audit file: {e}\")\n57 |     \n58 |     def validate_audit_structure(self, audit_data: Dict[str, Any]) -> List[str]:\n   |                                                              ^^^\n59 |         \"\"\"\n60 |         Validate the basic structure of audit data.\n   |\n\nF821 Undefined name `List`\n  --> canonical_flow/analysis/audit_validation.py:58:71\n   |\n56 |             raise AuditValidationError(f\"Error reading audit file: {e}\")\n57 |     \n58 |     def validate_audit_structure(self, audit_data: Dict[str, Any]) -> List[str]:\n   |                                                                       ^^^^\n59 |         \"\"\"\n60 |         Validate the basic structure of audit data.\n   |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/analysis/audit_validation.py:100:59\n    |\n 98 |         return errors\n 99 |     \n100 |     def validate_component_completeness(self, audit_data: Dict[str, Any], \n    |                                                           ^^^^\n101 |                                        invoked_components: Optional[Set[str]] = None) -> Dict[str, Any]:\n102 |         \"\"\"\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/analysis/audit_validation.py:100:69\n    |\n 98 |         return errors\n 99 |     \n100 |     def validate_component_completeness(self, audit_data: Dict[str, Any], \n    |                                                                     ^^^\n101 |                                        invoked_components: Optional[Set[str]] = None) -> Dict[str, Any]:\n102 |         \"\"\"\n    |\n\nF821 Undefined name `Optional`\n   --> canonical_flow/analysis/audit_validation.py:101:60\n    |\n100 |     def validate_component_completeness(self, audit_data: Dict[str, Any], \n101 |                                        invoked_components: Optional[Set[str]] = None) -> Dict[str, Any]:\n    |                                                            ^^^^^^^^\n102 |         \"\"\"\n103 |         Validate that all invoked components have complete execution traces.\n    |\n\nF821 Undefined name `Set`\n   --> canonical_flow/analysis/audit_validation.py:101:69\n    |\n100 |     def validate_component_completeness(self, audit_data: Dict[str, Any], \n101 |                                        invoked_components: Optional[Set[str]] = None) -> Dict[str, Any]:\n    |                                                                     ^^^\n102 |         \"\"\"\n103 |         Validate that all invoked components have complete execution traces.\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/analysis/audit_validation.py:101:90\n    |\n100 |     def validate_component_completeness(self, audit_data: Dict[str, Any], \n101 |                                        invoked_components: Optional[Set[str]] = None) -> Dict[str, Any]:\n    |                                                                                          ^^^^\n102 |         \"\"\"\n103 |         Validate that all invoked components have complete execution traces.\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/analysis/audit_validation.py:101:100\n    |\n100 |     def validate_component_completeness(self, audit_data: Dict[str, Any], \n101 |                                        invoked_components: Optional[Set[str]] = None) -> Dict[str, Any]:\n    |                                                                                                    ^^^\n102 |         \"\"\"\n103 |         Validate that all invoked components have complete execution traces.\n    |\n\nF821 Undefined name `datetime`\n   --> canonical_flow/analysis/audit_validation.py:120:37\n    |\n118 |             \"missing_traces\": [],\n119 |             \"incomplete_traces\": [],\n120 |             \"validation_timestamp\": datetime.now().isoformat()\n    |                                     ^^^^^^^^\n121 |         }\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/analysis/audit_validation.py:158:69\n    |\n156 |         return validation_results\n157 |     \n158 |     def _validate_single_component(self, comp_code: str, comp_data: Dict[str, Any]) -> Dict[str, Any]:\n    |                                                                     ^^^^\n159 |         \"\"\"\n160 |         Validate execution trace for a single component.\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/analysis/audit_validation.py:158:79\n    |\n156 |         return validation_results\n157 |     \n158 |     def _validate_single_component(self, comp_code: str, comp_data: Dict[str, Any]) -> Dict[str, Any]:\n    |                                                                               ^^^\n159 |         \"\"\"\n160 |         Validate execution trace for a single component.\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/analysis/audit_validation.py:158:88\n    |\n156 |         return validation_results\n157 |     \n158 |     def _validate_single_component(self, comp_code: str, comp_data: Dict[str, Any]) -> Dict[str, Any]:\n    |                                                                                        ^^^^\n159 |         \"\"\"\n160 |         Validate execution trace for a single component.\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/analysis/audit_validation.py:158:98\n    |\n156 |         return validation_results\n157 |     \n158 |     def _validate_single_component(self, comp_code: str, comp_data: Dict[str, Any]) -> Dict[str, Any]:\n    |                                                                                                  ^^^\n159 |         \"\"\"\n160 |         Validate execution trace for a single component.\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/analysis/audit_validation.py:234:54\n    |\n232 |         return status\n233 |     \n234 |     def validate_execution_summary(self, audit_data: Dict[str, Any]) -> Dict[str, Any]:\n    |                                                      ^^^^\n235 |         \"\"\"\n236 |         Validate the execution summary section.\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/analysis/audit_validation.py:234:64\n    |\n232 |         return status\n233 |     \n234 |     def validate_execution_summary(self, audit_data: Dict[str, Any]) -> Dict[str, Any]:\n    |                                                                ^^^\n235 |         \"\"\"\n236 |         Validate the execution summary section.\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/analysis/audit_validation.py:234:73\n    |\n232 |         return status\n233 |     \n234 |     def validate_execution_summary(self, audit_data: Dict[str, Any]) -> Dict[str, Any]:\n    |                                                                         ^^^^\n235 |         \"\"\"\n236 |         Validate the execution summary section.\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/analysis/audit_validation.py:234:83\n    |\n232 |         return status\n233 |     \n234 |     def validate_execution_summary(self, audit_data: Dict[str, Any]) -> Dict[str, Any]:\n    |                                                                                   ^^^\n235 |         \"\"\"\n236 |         Validate the execution summary section.\n    |\n\nF821 Undefined name `Optional`\n   --> canonical_flow/analysis/audit_validation.py:282:55\n    |\n280 |         return validation\n281 |     \n282 |     def run_full_validation(self, invoked_components: Optional[Set[str]] = None) -> Dict[str, Any]:\n    |                                                       ^^^^^^^^\n283 |         \"\"\"\n284 |         Run complete audit validation including all checks.\n    |\n\nF821 Undefined name `Set`\n   --> canonical_flow/analysis/audit_validation.py:282:64\n    |\n280 |         return validation\n281 |     \n282 |     def run_full_validation(self, invoked_components: Optional[Set[str]] = None) -> Dict[str, Any]:\n    |                                                                ^^^\n283 |         \"\"\"\n284 |         Run complete audit validation including all checks.\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/analysis/audit_validation.py:282:85\n    |\n280 |         return validation\n281 |     \n282 |     def run_full_validation(self, invoked_components: Optional[Set[str]] = None) -> Dict[str, Any]:\n    |                                                                                     ^^^^\n283 |         \"\"\"\n284 |         Run complete audit validation including all checks.\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/analysis/audit_validation.py:282:95\n    |\n280 |         return validation\n281 |     \n282 |     def run_full_validation(self, invoked_components: Optional[Set[str]] = None) -> Dict[str, Any]:\n    |                                                                                               ^^^\n283 |         \"\"\"\n284 |         Run complete audit validation including all checks.\n    |\n\nF821 Undefined name `datetime`\n   --> canonical_flow/analysis/audit_validation.py:293:37\n    |\n291 |         \"\"\"\n292 |         full_report = {\n293 |             \"validation_timestamp\": datetime.now().isoformat(),\n    |                                     ^^^^^^^^\n294 |             \"audit_file_path\": str(self.audit_file_path),\n295 |             \"file_exists\": False,\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/analysis/audit_validation.py:352:62\n    |\n350 |         return full_report\n351 |     \n352 |     def generate_validation_report(self, validation_results: Dict[str, Any]) -> str:\n    |                                                              ^^^^\n353 |         \"\"\"\n354 |         Generate a human-readable validation report.\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/analysis/audit_validation.py:352:72\n    |\n350 |         return full_report\n351 |     \n352 |     def generate_validation_report(self, validation_results: Dict[str, Any]) -> str:\n    |                                                                        ^^^\n353 |         \"\"\"\n354 |         Generate a human-readable validation report.\n    |\n\nF821 Undefined name `Optional`\n   --> canonical_flow/analysis/audit_validation.py:422:50\n    |\n422 | def validate_audit_completeness(audit_file_path: Optional[str] = None,\n    |                                                  ^^^^^^^^\n423 |                                invoked_components: Optional[Set[str]] = None) -> Dict[str, Any]:\n424 |     \"\"\"\n    |\n\nF821 Undefined name `Optional`\n   --> canonical_flow/analysis/audit_validation.py:423:52\n    |\n422 | def validate_audit_completeness(audit_file_path: Optional[str] = None,\n423 |                                invoked_components: Optional[Set[str]] = None) -> Dict[str, Any]:\n    |                                                    ^^^^^^^^\n424 |     \"\"\"\n425 |     Convenience function to run full audit validation.\n    |\n\nF821 Undefined name `Set`\n   --> canonical_flow/analysis/audit_validation.py:423:61\n    |\n422 | def validate_audit_completeness(audit_file_path: Optional[str] = None,\n423 |                                invoked_components: Optional[Set[str]] = None) -> Dict[str, Any]:\n    |                                                             ^^^\n424 |     \"\"\"\n425 |     Convenience function to run full audit validation.\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/analysis/audit_validation.py:423:82\n    |\n422 | def validate_audit_completeness(audit_file_path: Optional[str] = None,\n423 |                                invoked_components: Optional[Set[str]] = None) -> Dict[str, Any]:\n    |                                                                                  ^^^^\n424 |     \"\"\"\n425 |     Convenience function to run full audit validation.\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/analysis/audit_validation.py:423:92\n    |\n422 | def validate_audit_completeness(audit_file_path: Optional[str] = None,\n423 |                                invoked_components: Optional[Set[str]] = None) -> Dict[str, Any]:\n    |                                                                                            ^^^\n424 |     \"\"\"\n425 |     Convenience function to run full audit validation.\n    |\n\nF821 Undefined name `Optional`\n   --> canonical_flow/analysis/audit_validation.py:441:52\n    |\n441 | def print_audit_validation_report(audit_file_path: Optional[str] = None,\n    |                                                    ^^^^^^^^\n442 |                                  invoked_components: Optional[Set[str]] = None) -> bool:\n443 |     \"\"\"\n    |\n\nF821 Undefined name `Optional`\n   --> canonical_flow/analysis/audit_validation.py:442:54\n    |\n441 | def print_audit_validation_report(audit_file_path: Optional[str] = None,\n442 |                                  invoked_components: Optional[Set[str]] = None) -> bool:\n    |                                                      ^^^^^^^^\n443 |     \"\"\"\n444 |     Print a formatted audit validation report to console.\n    |\n\nF821 Undefined name `Set`\n   --> canonical_flow/analysis/audit_validation.py:442:63\n    |\n441 | def print_audit_validation_report(audit_file_path: Optional[str] = None,\n442 |                                  invoked_components: Optional[Set[str]] = None) -> bool:\n    |                                                               ^^^\n443 |     \"\"\"\n444 |     Print a formatted audit validation report to console.\n    |\n\nF401 [*] `os` imported but unused\n  --> canonical_flow/analysis/dimension_aggregator.py:11:8\n   |\n 9 | import json\n10 | import logging\n11 | import os\n   |        ^^\n12 | # # # from collections import OrderedDict  # Module not found  # Module not found  # Module not found\n13 | # # # from dataclasses import dataclass, field  # Module not found  # Module not found  # Module not found\n   |\nhelp: Remove unused import: `os`\n\nF821 Undefined name `Enum`\n  --> canonical_flow/analysis/dimension_aggregator.py:26:23\n   |\n26 | class DimensionStatus(Enum):\n   |                       ^^^^\n27 |     \"\"\"Status flags for dimension processing\"\"\"\n28 |     COMPLETE = \"complete\"\n   |\n\nF821 Undefined name `Enum`\n  --> canonical_flow/analysis/dimension_aggregator.py:34:22\n   |\n34 | class CorrectionType(Enum):\n   |                      ^^^^\n35 |     \"\"\"Types of correction factors\"\"\"\n36 |     BASELINE_DEVIATION = \"baseline_deviation\"\n   |\n\nF821 Undefined name `dataclass`\n  --> canonical_flow/analysis/dimension_aggregator.py:41:2\n   |\n41 | @dataclass\n   |  ^^^^^^^^^\n42 | class CorrectionFactor:\n43 |     \"\"\"Represents a correction factor applied to scores\"\"\"\n   |\n\nF821 Undefined name `Dict`\n  --> canonical_flow/analysis/dimension_aggregator.py:50:23\n   |\n48 |     final_score: float\n49 |     reason: str\n50 |     evidence_metrics: Dict[str, Any] = field(default_factory=dict)\n   |                       ^^^^\n   |\n\nF821 Undefined name `Any`\n  --> canonical_flow/analysis/dimension_aggregator.py:50:33\n   |\n48 |     final_score: float\n49 |     reason: str\n50 |     evidence_metrics: Dict[str, Any] = field(default_factory=dict)\n   |                                 ^^^\n   |\n\nF821 Undefined name `field`\n  --> canonical_flow/analysis/dimension_aggregator.py:50:40\n   |\n48 |     final_score: float\n49 |     reason: str\n50 |     evidence_metrics: Dict[str, Any] = field(default_factory=dict)\n   |                                        ^^^^^\n   |\n\nF821 Undefined name `dataclass`\n  --> canonical_flow/analysis/dimension_aggregator.py:53:2\n   |\n53 | @dataclass\n   |  ^^^^^^^^^\n54 | class QuestionBreakdown:\n55 |     \"\"\"Question-level contribution to dimension score\"\"\"\n   |\n\nF821 Undefined name `dataclass`\n  --> canonical_flow/analysis/dimension_aggregator.py:65:2\n   |\n65 | @dataclass\n   |  ^^^^^^^^^\n66 | class DimensionResult:\n67 |     \"\"\"Aggregated result for a single dimension\"\"\"\n   |\n\nF821 Undefined name `List`\n  --> canonical_flow/analysis/dimension_aggregator.py:82:25\n   |\n81 |     # Correction factors\n82 |     correction_factors: List[CorrectionFactor] = field(default_factory=list)\n   |                         ^^^^\n83 |     question_breakdown: List[QuestionBreakdown] = field(default_factory=list)\n   |\n\nF821 Undefined name `field`\n  --> canonical_flow/analysis/dimension_aggregator.py:82:50\n   |\n81 |     # Correction factors\n82 |     correction_factors: List[CorrectionFactor] = field(default_factory=list)\n   |                                                  ^^^^^\n83 |     question_breakdown: List[QuestionBreakdown] = field(default_factory=list)\n   |\n\nF821 Undefined name `List`\n  --> canonical_flow/analysis/dimension_aggregator.py:83:25\n   |\n81 |     # Correction factors\n82 |     correction_factors: List[CorrectionFactor] = field(default_factory=list)\n83 |     question_breakdown: List[QuestionBreakdown] = field(default_factory=list)\n   |                         ^^^^\n84 |     \n85 |     # Metadata\n   |\n\nF821 Undefined name `field`\n  --> canonical_flow/analysis/dimension_aggregator.py:83:51\n   |\n81 |     # Correction factors\n82 |     correction_factors: List[CorrectionFactor] = field(default_factory=list)\n83 |     question_breakdown: List[QuestionBreakdown] = field(default_factory=list)\n   |                                                   ^^^^^\n84 |     \n85 |     # Metadata\n   |\n\nF821 Undefined name `field`\n  --> canonical_flow/analysis/dimension_aggregator.py:86:33\n   |\n85 |     # Metadata\n86 |     processing_timestamp: str = field(default_factory=lambda: datetime.utcnow().isoformat())\n   |                                 ^^^^^\n87 |     artifact_id: str = field(default_factory=lambda: str(uuid4())[:8])\n   |\n\nF821 Undefined name `datetime`\n  --> canonical_flow/analysis/dimension_aggregator.py:86:63\n   |\n85 |     # Metadata\n86 |     processing_timestamp: str = field(default_factory=lambda: datetime.utcnow().isoformat())\n   |                                                               ^^^^^^^^\n87 |     artifact_id: str = field(default_factory=lambda: str(uuid4())[:8])\n   |\n\nF821 Undefined name `field`\n  --> canonical_flow/analysis/dimension_aggregator.py:87:24\n   |\n85 |     # Metadata\n86 |     processing_timestamp: str = field(default_factory=lambda: datetime.utcnow().isoformat())\n87 |     artifact_id: str = field(default_factory=lambda: str(uuid4())[:8])\n   |                        ^^^^^\n   |\n\nF821 Undefined name `uuid4`\n  --> canonical_flow/analysis/dimension_aggregator.py:87:58\n   |\n85 |     # Metadata\n86 |     processing_timestamp: str = field(default_factory=lambda: datetime.utcnow().isoformat())\n87 |     artifact_id: str = field(default_factory=lambda: str(uuid4())[:8])\n   |                                                          ^^^^^\n   |\n\nF821 Undefined name `Path`\n   --> canonical_flow/analysis/dimension_aggregator.py:132:27\n    |\n130 |             output_dir: Directory for artifact generation\n131 |         \"\"\"\n132 |         self.output_dir = Path(output_dir)\n    |                           ^^^^\n133 |         self.output_dir.mkdir(parents=True, exist_ok=True)\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/analysis/dimension_aggregator.py:145:51\n    |\n143 |         logger.info(f\"DimensionAggregator initialized with output_dir: {self.output_dir}\")\n144 |     \n145 |     def aggregate_dimensions(self, question_data: Dict[str, Any]) -> Dict[str, DimensionResult]:\n    |                                                   ^^^^\n146 |         \"\"\"\n147 |         Aggregate question-level data into dimension-level results.\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/analysis/dimension_aggregator.py:145:61\n    |\n143 |         logger.info(f\"DimensionAggregator initialized with output_dir: {self.output_dir}\")\n144 |     \n145 |     def aggregate_dimensions(self, question_data: Dict[str, Any]) -> Dict[str, DimensionResult]:\n    |                                                             ^^^\n146 |         \"\"\"\n147 |         Aggregate question-level data into dimension-level results.\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/analysis/dimension_aggregator.py:145:70\n    |\n143 |         logger.info(f\"DimensionAggregator initialized with output_dir: {self.output_dir}\")\n144 |     \n145 |     def aggregate_dimensions(self, question_data: Dict[str, Any]) -> Dict[str, DimensionResult]:\n    |                                                                      ^^^^\n146 |         \"\"\"\n147 |         Aggregate question-level data into dimension-level results.\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/analysis/dimension_aggregator.py:192:42\n    |\n190 |         return results\n191 |     \n192 |     def _validate_input_data(self, data: Dict[str, Any]) -> None:\n    |                                          ^^^^\n193 |         \"\"\"Validate input data structure and content.\"\"\"\n194 |         if \"questions\" not in data:\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/analysis/dimension_aggregator.py:192:52\n    |\n190 |         return results\n191 |     \n192 |     def _validate_input_data(self, data: Dict[str, Any]) -> None:\n    |                                                    ^^^\n193 |         \"\"\"Validate input data structure and content.\"\"\"\n194 |         if \"questions\" not in data:\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/analysis/dimension_aggregator.py:216:56\n    |\n214 |         logger.debug(f\"Input validation passed for {len(questions)} questions\")\n215 |     \n216 |     def _group_questions_by_dimension(self, questions: List[Dict[str, Any]]) -> Dict[str, List[Dict[str, Any]]]:\n    |                                                        ^^^^\n217 |         \"\"\"Group questions by dimension ID.\"\"\"\n218 |         grouped = {dim_id: [] for dim_id in self.EXPECTED_DIMENSION_COUNTS.keys()}\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/analysis/dimension_aggregator.py:216:61\n    |\n214 |         logger.debug(f\"Input validation passed for {len(questions)} questions\")\n215 |     \n216 |     def _group_questions_by_dimension(self, questions: List[Dict[str, Any]]) -> Dict[str, List[Dict[str, Any]]]:\n    |                                                             ^^^^\n217 |         \"\"\"Group questions by dimension ID.\"\"\"\n218 |         grouped = {dim_id: [] for dim_id in self.EXPECTED_DIMENSION_COUNTS.keys()}\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/analysis/dimension_aggregator.py:216:71\n    |\n214 |         logger.debug(f\"Input validation passed for {len(questions)} questions\")\n215 |     \n216 |     def _group_questions_by_dimension(self, questions: List[Dict[str, Any]]) -> Dict[str, List[Dict[str, Any]]]:\n    |                                                                       ^^^\n217 |         \"\"\"Group questions by dimension ID.\"\"\"\n218 |         grouped = {dim_id: [] for dim_id in self.EXPECTED_DIMENSION_COUNTS.keys()}\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/analysis/dimension_aggregator.py:216:81\n    |\n214 |         logger.debug(f\"Input validation passed for {len(questions)} questions\")\n215 |     \n216 |     def _group_questions_by_dimension(self, questions: List[Dict[str, Any]]) -> Dict[str, List[Dict[str, Any]]]:\n    |                                                                                 ^^^^\n217 |         \"\"\"Group questions by dimension ID.\"\"\"\n218 |         grouped = {dim_id: [] for dim_id in self.EXPECTED_DIMENSION_COUNTS.keys()}\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/analysis/dimension_aggregator.py:216:91\n    |\n214 |         logger.debug(f\"Input validation passed for {len(questions)} questions\")\n215 |     \n216 |     def _group_questions_by_dimension(self, questions: List[Dict[str, Any]]) -> Dict[str, List[Dict[str, Any]]]:\n    |                                                                                           ^^^^\n217 |         \"\"\"Group questions by dimension ID.\"\"\"\n218 |         grouped = {dim_id: [] for dim_id in self.EXPECTED_DIMENSION_COUNTS.keys()}\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/analysis/dimension_aggregator.py:216:96\n    |\n214 |         logger.debug(f\"Input validation passed for {len(questions)} questions\")\n215 |     \n216 |     def _group_questions_by_dimension(self, questions: List[Dict[str, Any]]) -> Dict[str, List[Dict[str, Any]]]:\n    |                                                                                                ^^^^\n217 |         \"\"\"Group questions by dimension ID.\"\"\"\n218 |         grouped = {dim_id: [] for dim_id in self.EXPECTED_DIMENSION_COUNTS.keys()}\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/analysis/dimension_aggregator.py:216:106\n    |\n214 |         logger.debug(f\"Input validation passed for {len(questions)} questions\")\n215 |     \n216 |     def _group_questions_by_dimension(self, questions: List[Dict[str, Any]]) -> Dict[str, List[Dict[str, Any]]]:\n    |                                                                                                          ^^^\n217 |         \"\"\"Group questions by dimension ID.\"\"\"\n218 |         grouped = {dim_id: [] for dim_id in self.EXPECTED_DIMENSION_COUNTS.keys()}\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/analysis/dimension_aggregator.py:232:64\n    |\n230 |         return grouped\n231 |     \n232 |     def _process_dimension(self, dimension_id: str, questions: List[Dict[str, Any]]) -> DimensionResult:\n    |                                                                ^^^^\n233 |         \"\"\"Process a single dimension and compute aggregated metrics.\"\"\"\n234 |         logger.debug(f\"Processing dimension {dimension_id} with {len(questions)} questions\")\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/analysis/dimension_aggregator.py:232:69\n    |\n230 |         return grouped\n231 |     \n232 |     def _process_dimension(self, dimension_id: str, questions: List[Dict[str, Any]]) -> DimensionResult:\n    |                                                                     ^^^^\n233 |         \"\"\"Process a single dimension and compute aggregated metrics.\"\"\"\n234 |         logger.debug(f\"Processing dimension {dimension_id} with {len(questions)} questions\")\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/analysis/dimension_aggregator.py:232:79\n    |\n230 |         return grouped\n231 |     \n232 |     def _process_dimension(self, dimension_id: str, questions: List[Dict[str, Any]]) -> DimensionResult:\n    |                                                                               ^^^\n233 |         \"\"\"Process a single dimension and compute aggregated metrics.\"\"\"\n234 |         logger.debug(f\"Processing dimension {dimension_id} with {len(questions)} questions\")\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/analysis/dimension_aggregator.py:280:49\n    |\n278 |         return result\n279 |     \n280 |     def _compute_weighted_mean(self, questions: List[Dict[str, Any]]) -> Tuple[float, List[QuestionBreakdown]]:\n    |                                                 ^^^^\n281 |         \"\"\"Compute weighted mean score and generate question breakdown.\"\"\"\n282 |         if not questions:\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/analysis/dimension_aggregator.py:280:54\n    |\n278 |         return result\n279 |     \n280 |     def _compute_weighted_mean(self, questions: List[Dict[str, Any]]) -> Tuple[float, List[QuestionBreakdown]]:\n    |                                                      ^^^^\n281 |         \"\"\"Compute weighted mean score and generate question breakdown.\"\"\"\n282 |         if not questions:\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/analysis/dimension_aggregator.py:280:64\n    |\n278 |         return result\n279 |     \n280 |     def _compute_weighted_mean(self, questions: List[Dict[str, Any]]) -> Tuple[float, List[QuestionBreakdown]]:\n    |                                                                ^^^\n281 |         \"\"\"Compute weighted mean score and generate question breakdown.\"\"\"\n282 |         if not questions:\n    |\n\nF821 Undefined name `Tuple`\n   --> canonical_flow/analysis/dimension_aggregator.py:280:74\n    |\n278 |         return result\n279 |     \n280 |     def _compute_weighted_mean(self, questions: List[Dict[str, Any]]) -> Tuple[float, List[QuestionBreakdown]]:\n    |                                                                          ^^^^^\n281 |         \"\"\"Compute weighted mean score and generate question breakdown.\"\"\"\n282 |         if not questions:\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/analysis/dimension_aggregator.py:280:87\n    |\n278 |         return result\n279 |     \n280 |     def _compute_weighted_mean(self, questions: List[Dict[str, Any]]) -> Tuple[float, List[QuestionBreakdown]]:\n    |                                                                                       ^^^^\n281 |         \"\"\"Compute weighted mean score and generate question breakdown.\"\"\"\n282 |         if not questions:\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/analysis/dimension_aggregator.py:316:54\n    |\n314 |         return weighted_mean, breakdown\n315 |     \n316 |     def _calculate_evidence_density(self, questions: List[Dict[str, Any]]) -> float:\n    |                                                      ^^^^\n317 |         \"\"\"Calculate evidence density metric.\"\"\"\n318 |         if not questions:\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/analysis/dimension_aggregator.py:316:59\n    |\n314 |         return weighted_mean, breakdown\n315 |     \n316 |     def _calculate_evidence_density(self, questions: List[Dict[str, Any]]) -> float:\n    |                                                           ^^^^\n317 |         \"\"\"Calculate evidence density metric.\"\"\"\n318 |         if not questions:\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/analysis/dimension_aggregator.py:316:69\n    |\n314 |         return weighted_mean, breakdown\n315 |     \n316 |     def _calculate_evidence_density(self, questions: List[Dict[str, Any]]) -> float:\n    |                                                                     ^^^\n317 |         \"\"\"Calculate evidence density metric.\"\"\"\n318 |         if not questions:\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/analysis/dimension_aggregator.py:324:62\n    |\n322 |         return total_evidence / len(questions)\n323 |     \n324 |     def _calculate_average_evidence_quality(self, questions: List[Dict[str, Any]]) -> float:\n    |                                                              ^^^^\n325 |         \"\"\"Calculate average evidence quality.\"\"\"\n326 |         if not questions:\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/analysis/dimension_aggregator.py:324:67\n    |\n322 |         return total_evidence / len(questions)\n323 |     \n324 |     def _calculate_average_evidence_quality(self, questions: List[Dict[str, Any]]) -> float:\n    |                                                                   ^^^^\n325 |         \"\"\"Calculate average evidence quality.\"\"\"\n326 |         if not questions:\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/analysis/dimension_aggregator.py:324:77\n    |\n322 |         return total_evidence / len(questions)\n323 |     \n324 |     def _calculate_average_evidence_quality(self, questions: List[Dict[str, Any]]) -> float:\n    |                                                                             ^^^\n325 |         \"\"\"Calculate average evidence quality.\"\"\"\n326 |         if not questions:\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/analysis/dimension_aggregator.py:332:77\n    |\n330 |         return total_quality / len(questions)\n331 |     \n332 |     def _apply_correction_factors(self, result: DimensionResult, questions: List[Dict[str, Any]]) -> None:\n    |                                                                             ^^^^\n333 |         \"\"\"Apply correction factors to dimension score.\"\"\"\n334 |         original_score = result.adjusted_score\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/analysis/dimension_aggregator.py:332:82\n    |\n330 |         return total_quality / len(questions)\n331 |     \n332 |     def _apply_correction_factors(self, result: DimensionResult, questions: List[Dict[str, Any]]) -> None:\n    |                                                                                  ^^^^\n333 |         \"\"\"Apply correction factors to dimension score.\"\"\"\n334 |         original_score = result.adjusted_score\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/analysis/dimension_aggregator.py:332:92\n    |\n330 |         return total_quality / len(questions)\n331 |     \n332 |     def _apply_correction_factors(self, result: DimensionResult, questions: List[Dict[str, Any]]) -> None:\n    |                                                                                            ^^^\n333 |         \"\"\"Apply correction factors to dimension score.\"\"\"\n334 |         original_score = result.adjusted_score\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/analysis/dimension_aggregator.py:386:78\n    |\n384 |         logger.debug(f\"Applied {len(result.correction_factors)} correction factors to {result.dimension_id}\")\n385 |     \n386 |     def _detect_baseline_deviation(self, result: DimensionResult, questions: List[Dict[str, Any]]) -> float:\n    |                                                                              ^^^^\n387 |         \"\"\"\n388 |         Detect baseline deviation for causal correction.\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/analysis/dimension_aggregator.py:386:83\n    |\n384 |         logger.debug(f\"Applied {len(result.correction_factors)} correction factors to {result.dimension_id}\")\n385 |     \n386 |     def _detect_baseline_deviation(self, result: DimensionResult, questions: List[Dict[str, Any]]) -> float:\n    |                                                                                   ^^^^\n387 |         \"\"\"\n388 |         Detect baseline deviation for causal correction.\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/analysis/dimension_aggregator.py:386:93\n    |\n384 |         logger.debug(f\"Applied {len(result.correction_factors)} correction factors to {result.dimension_id}\")\n385 |     \n386 |     def _detect_baseline_deviation(self, result: DimensionResult, questions: List[Dict[str, Any]]) -> float:\n    |                                                                                             ^^^\n387 |         \"\"\"\n388 |         Detect baseline deviation for causal correction.\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/analysis/dimension_aggregator.py:407:44\n    |\n405 |         return deviation\n406 |     \n407 |     def _generate_artifacts(self, results: Dict[str, DimensionResult], metadata: Dict[str, Any]) -> None:\n    |                                            ^^^^\n408 |         \"\"\"Generate deterministic JSON artifacts.\"\"\"\n409 |         timestamp = datetime.utcnow().strftime(\"%Y%m%d_%H%M%S\")\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/analysis/dimension_aggregator.py:407:82\n    |\n405 |         return deviation\n406 |     \n407 |     def _generate_artifacts(self, results: Dict[str, DimensionResult], metadata: Dict[str, Any]) -> None:\n    |                                                                                  ^^^^\n408 |         \"\"\"Generate deterministic JSON artifacts.\"\"\"\n409 |         timestamp = datetime.utcnow().strftime(\"%Y%m%d_%H%M%S\")\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/analysis/dimension_aggregator.py:407:92\n    |\n405 |         return deviation\n406 |     \n407 |     def _generate_artifacts(self, results: Dict[str, DimensionResult], metadata: Dict[str, Any]) -> None:\n    |                                                                                            ^^^\n408 |         \"\"\"Generate deterministic JSON artifacts.\"\"\"\n409 |         timestamp = datetime.utcnow().strftime(\"%Y%m%d_%H%M%S\")\n    |\n\nF821 Undefined name `datetime`\n   --> canonical_flow/analysis/dimension_aggregator.py:409:21\n    |\n407 |     def _generate_artifacts(self, results: Dict[str, DimensionResult], metadata: Dict[str, Any]) -> None:\n408 |         \"\"\"Generate deterministic JSON artifacts.\"\"\"\n409 |         timestamp = datetime.utcnow().strftime(\"%Y%m%d_%H%M%S\")\n    |                     ^^^^^^^^\n410 |         \n411 |         # Create comprehensive artifact\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/analysis/dimension_aggregator.py:449:53\n    |\n447 |         logger.info(f\"Generated artifacts: {artifact_path}\")\n448 |     \n449 |     def _generate_validation_summary(self, results: Dict[str, DimensionResult]) -> Dict[str, Any]:\n    |                                                     ^^^^\n450 |         \"\"\"Generate validation summary for dimension counts.\"\"\"\n451 |         summary = {\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/analysis/dimension_aggregator.py:449:84\n    |\n447 |         logger.info(f\"Generated artifacts: {artifact_path}\")\n448 |     \n449 |     def _generate_validation_summary(self, results: Dict[str, DimensionResult]) -> Dict[str, Any]:\n    |                                                                                    ^^^^\n450 |         \"\"\"Generate validation summary for dimension counts.\"\"\"\n451 |         summary = {\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/analysis/dimension_aggregator.py:449:94\n    |\n447 |         logger.info(f\"Generated artifacts: {artifact_path}\")\n448 |     \n449 |     def _generate_validation_summary(self, results: Dict[str, DimensionResult]) -> Dict[str, Any]:\n    |                                                                                              ^^^\n450 |         \"\"\"Generate validation summary for dimension counts.\"\"\"\n451 |         summary = {\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/analysis/dimension_aggregator.py:471:43\n    |\n469 |         return summary\n470 |     \n471 |     def _serialize_results(self, results: Dict[str, DimensionResult]) -> Dict[str, Any]:\n    |                                           ^^^^\n472 |         \"\"\"Serialize results with deterministic ordering.\"\"\"\n473 |         serialized = OrderedDict()\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/analysis/dimension_aggregator.py:471:74\n    |\n469 |         return summary\n470 |     \n471 |     def _serialize_results(self, results: Dict[str, DimensionResult]) -> Dict[str, Any]:\n    |                                                                          ^^^^\n472 |         \"\"\"Serialize results with deterministic ordering.\"\"\"\n473 |         serialized = OrderedDict()\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/analysis/dimension_aggregator.py:471:84\n    |\n469 |         return summary\n470 |     \n471 |     def _serialize_results(self, results: Dict[str, DimensionResult]) -> Dict[str, Any]:\n    |                                                                                    ^^^\n472 |         \"\"\"Serialize results with deterministic ordering.\"\"\"\n473 |         serialized = OrderedDict()\n    |\n\nF821 Undefined name `OrderedDict`\n   --> canonical_flow/analysis/dimension_aggregator.py:473:22\n    |\n471 |     def _serialize_results(self, results: Dict[str, DimensionResult]) -> Dict[str, Any]:\n472 |         \"\"\"Serialize results with deterministic ordering.\"\"\"\n473 |         serialized = OrderedDict()\n    |                      ^^^^^^^^^^^\n474 |         \n475 |         for dim_id in sorted(results.keys()):\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/analysis/dimension_aggregator.py:480:68\n    |\n478 |         return serialized\n479 |     \n480 |     def _serialize_single_result(self, result: DimensionResult) -> Dict[str, Any]:\n    |                                                                    ^^^^\n481 |         \"\"\"Serialize a single dimension result.\"\"\"\n482 |         return {\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/analysis/dimension_aggregator.py:480:78\n    |\n478 |         return serialized\n479 |     \n480 |     def _serialize_single_result(self, result: DimensionResult) -> Dict[str, Any]:\n    |                                                                              ^^^\n481 |         \"\"\"Serialize a single dimension result.\"\"\"\n482 |         return {\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/analysis/dimension_aggregator.py:528:49\n    |\n526 |         }\n527 |     \n528 |     def _compute_quality_metrics(self, results: Dict[str, DimensionResult]) -> Dict[str, Any]:\n    |                                                 ^^^^\n529 |         \"\"\"Compute aggregate quality metrics across all dimensions.\"\"\"\n530 |         metrics = {\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/analysis/dimension_aggregator.py:528:80\n    |\n526 |         }\n527 |     \n528 |     def _compute_quality_metrics(self, results: Dict[str, DimensionResult]) -> Dict[str, Any]:\n    |                                                                                ^^^^\n529 |         \"\"\"Compute aggregate quality metrics across all dimensions.\"\"\"\n530 |         metrics = {\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/analysis/dimension_aggregator.py:528:90\n    |\n526 |         }\n527 |     \n528 |     def _compute_quality_metrics(self, results: Dict[str, DimensionResult]) -> Dict[str, Any]:\n    |                                                                                          ^^^\n529 |         \"\"\"Compute aggregate quality metrics across all dimensions.\"\"\"\n530 |         metrics = {\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/analysis/dimension_aggregator.py:556:47\n    |\n554 |         return metrics\n555 |     \n556 |     def _summarize_corrections(self, results: Dict[str, DimensionResult]) -> Dict[str, Any]:\n    |                                               ^^^^\n557 |         \"\"\"Summarize correction factors applied.\"\"\"\n558 |         summary = {\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/analysis/dimension_aggregator.py:556:78\n    |\n554 |         return metrics\n555 |     \n556 |     def _summarize_corrections(self, results: Dict[str, DimensionResult]) -> Dict[str, Any]:\n    |                                                                              ^^^^\n557 |         \"\"\"Summarize correction factors applied.\"\"\"\n558 |         summary = {\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/analysis/dimension_aggregator.py:556:88\n    |\n554 |         return metrics\n555 |     \n556 |     def _summarize_corrections(self, results: Dict[str, DimensionResult]) -> Dict[str, Any]:\n    |                                                                                        ^^^\n557 |         \"\"\"Summarize correction factors applied.\"\"\"\n558 |         summary = {\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/analysis/dimension_aggregator.py:583:38\n    |\n583 | def create_sample_question_data() -> Dict[str, Any]:\n    |                                      ^^^^\n584 |     \"\"\"Create sample question data for testing.\"\"\"\n585 |     questions = []\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/analysis/dimension_aggregator.py:583:48\n    |\n583 | def create_sample_question_data() -> Dict[str, Any]:\n    |                                                ^^^\n584 |     \"\"\"Create sample question data for testing.\"\"\"\n585 |     questions = []\n    |\n\nF821 Undefined name `datetime`\n   --> canonical_flow/analysis/dimension_aggregator.py:635:26\n    |\n633 |         \"metadata\": {\n634 |             \"source\": \"sample_data\",\n635 |             \"timestamp\": datetime.utcnow().isoformat(),\n    |                          ^^^^^^^^\n636 |             \"total_questions\": len(questions)\n637 |         }\n    |\n\nF401 [*] `json` imported but unused\n  --> canonical_flow/analysis/dnp_causal_correction_system.py:8:8\n   |\n 6 | \"\"\"\n 7 |\n 8 | import json\n   |        ^^^^\n 9 | import uuid\n10 | # # # from datetime import datetime, timezone  # Module not found  # Module not found  # Module not found\n   |\nhelp: Remove unused import: `json`\n\nF821 Undefined name `Enum`\n  --> canonical_flow/analysis/dnp_causal_correction_system.py:21:31\n   |\n19 | logger = logging.getLogger(__name__)\n20 |\n21 | class CorrectionCategory(str, Enum):\n   |                               ^^^^\n22 |     CONSTITUTIONAL = \"constitutional\"\n23 |     ETHICAL = \"ethical\"\n   |\n\nF821 Undefined name `Enum`\n  --> canonical_flow/analysis/dnp_causal_correction_system.py:28:32\n   |\n26 |     TECHNICAL = \"technical\"\n27 |\n28 | class HumanRightsStandard(str, Enum):\n   |                                ^^^^\n29 |     DIGNITY = \"dignity\"\n30 |     EQUALITY = \"equality\"\n   |\n\nF821 Undefined name `dataclass`\n  --> canonical_flow/analysis/dnp_causal_correction_system.py:36:2\n   |\n34 |     PROPORTIONALITY = \"proportionality\"\n35 |\n36 | @dataclass\n   |  ^^^^^^^^^\n37 | class BaselineDeviation:\n38 |     metric_name: str\n   |\n\nF821 Undefined name `field`\n  --> canonical_flow/analysis/dnp_causal_correction_system.py:42:27\n   |\n40 |     measured_value: float\n41 |     category: CorrectionCategory\n42 |     measurement_id: str = field(default_factory=lambda: str(uuid.uuid4()))\n   |                           ^^^^^\n43 |     \n44 |     def __post_init__(self):\n   |\n\nF821 Undefined name `dataclass`\n  --> canonical_flow/analysis/dnp_causal_correction_system.py:63:2\n   |\n61 |             return \"critical\"\n62 |\n63 | @dataclass\n   |  ^^^^^^^^^\n64 | class CorrectionFactor:\n65 |     deviation_id: str\n   |\n\nF821 Undefined name `Tuple`\n  --> canonical_flow/analysis/dnp_causal_correction_system.py:69:26\n   |\n67 |     multiplicative_factor: float\n68 |     additive_adjustment: float\n69 |     confidence_interval: Tuple[float, float]\n   |                          ^^^^^\n70 |     factor_id: str = field(default_factory=lambda: str(uuid.uuid4()))\n   |\n\nF821 Undefined name `field`\n  --> canonical_flow/analysis/dnp_causal_correction_system.py:70:22\n   |\n68 |     additive_adjustment: float\n69 |     confidence_interval: Tuple[float, float]\n70 |     factor_id: str = field(default_factory=lambda: str(uuid.uuid4()))\n   |                      ^^^^^\n71 |     \n72 |     @classmethod\n   |\n\nF821 Undefined name `dataclass`\n   --> canonical_flow/analysis/dnp_causal_correction_system.py:99:2\n    |\n 97 |         )\n 98 |\n 99 | @dataclass\n    |  ^^^^^^^^^\n100 | class RobustnessScore:\n101 |     correction_factor_id: str\n    |\n\nF821 Undefined name `field`\n   --> canonical_flow/analysis/dnp_causal_correction_system.py:104:26\n    |\n102 |     overall_robustness: float\n103 |     is_reliable: bool\n104 |     robustness_id: str = field(default_factory=lambda: str(uuid.uuid4()))\n    |                          ^^^^^\n105 |     \n106 |     @classmethod\n    |\n\nF821 Undefined name `dataclass`\n   --> canonical_flow/analysis/dnp_causal_correction_system.py:131:2\n    |\n129 |         )\n130 |\n131 | @dataclass\n    |  ^^^^^^^^^\n132 | class HumanRightsAlignment:\n133 |     overall_alignment_score: float\n    |\n\nF821 Undefined name `field`\n   --> canonical_flow/analysis/dnp_causal_correction_system.py:136:25\n    |\n134 |     alignment_status: str\n135 |     is_compliant: bool\n136 |     alignment_id: str = field(default_factory=lambda: str(uuid.uuid4()))\n    |                         ^^^^^\n137 |     \n138 |     @classmethod\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/analysis/dnp_causal_correction_system.py:139:54\n    |\n138 |     @classmethod\n139 |     def validate_correction_system(cls, corrections: List[CorrectionFactor], robustness: List[RobustnessScore]) -> 'HumanRightsAlignm\u2026\n    |                                                      ^^^^\n140 |         # Simplified human rights validation\n141 |         dignity_score = 0.9\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/analysis/dnp_causal_correction_system.py:139:90\n    |\n138 |     @classmethod\n139 |     def validate_correction_system(cls, corrections: List[CorrectionFactor], robustness: List[RobustnessScore]) -> 'HumanRightsAlignm\u2026\n    |                                                                                          ^^^^\n140 |         # Simplified human rights validation\n141 |         dignity_score = 0.9\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/analysis/dnp_causal_correction_system.py:159:54\n    |\n157 |         self.system_id = str(uuid.uuid4())[:8]\n158 |         \n159 |     def process_correction_cycle(self, measurements: Dict[str, float], baselines: Dict[str, float], \n    |                                                      ^^^^\n160 |                                categories: Optional[Dict[str, CorrectionCategory]] = None) -> Dict[str, Any]:\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/analysis/dnp_causal_correction_system.py:159:83\n    |\n157 |         self.system_id = str(uuid.uuid4())[:8]\n158 |         \n159 |     def process_correction_cycle(self, measurements: Dict[str, float], baselines: Dict[str, float], \n    |                                                                                   ^^^^\n160 |                                categories: Optional[Dict[str, CorrectionCategory]] = None) -> Dict[str, Any]:\n    |\n\nF821 Undefined name `Optional`\n   --> canonical_flow/analysis/dnp_causal_correction_system.py:160:44\n    |\n159 |     def process_correction_cycle(self, measurements: Dict[str, float], baselines: Dict[str, float], \n160 |                                categories: Optional[Dict[str, CorrectionCategory]] = None) -> Dict[str, Any]:\n    |                                            ^^^^^^^^\n161 |         \n162 |         # Step 1: Measure deviations\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/analysis/dnp_causal_correction_system.py:160:53\n    |\n159 |     def process_correction_cycle(self, measurements: Dict[str, float], baselines: Dict[str, float], \n160 |                                categories: Optional[Dict[str, CorrectionCategory]] = None) -> Dict[str, Any]:\n    |                                                     ^^^^\n161 |         \n162 |         # Step 1: Measure deviations\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/analysis/dnp_causal_correction_system.py:160:95\n    |\n159 |     def process_correction_cycle(self, measurements: Dict[str, float], baselines: Dict[str, float], \n160 |                                categories: Optional[Dict[str, CorrectionCategory]] = None) -> Dict[str, Any]:\n    |                                                                                               ^^^^\n161 |         \n162 |         # Step 1: Measure deviations\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/analysis/dnp_causal_correction_system.py:160:105\n    |\n159 |     def process_correction_cycle(self, measurements: Dict[str, float], baselines: Dict[str, float], \n160 |                                categories: Optional[Dict[str, CorrectionCategory]] = None) -> Dict[str, Any]:\n    |                                                                                                         ^^^\n161 |         \n162 |         # Step 1: Measure deviations\n    |\n\ninvalid-syntax: Unexpected indentation\n  --> canonical_flow/analysis/integration_demo.py:17:1\n   |\n16 | # # # from canonical_flow.analysis.artifact_generator import (  # Module not found  # Module not found  # Module not found\n17 |     ArtifactGenerator, QuestionEvaluation, DimensionSummary, PointSummary,\n   | ^^^^\n18 |     MesoClusterAnalysis, MacroAlignment, EvidenceReference\n19 | )\n   |\n\ninvalid-syntax: Expected a statement\n  --> canonical_flow/analysis/integration_demo.py:19:1\n   |\n17 |     ArtifactGenerator, QuestionEvaluation, DimensionSummary, PointSummary,\n18 |     MesoClusterAnalysis, MacroAlignment, EvidenceReference\n19 | )\n   | ^\n   |\n\ninvalid-syntax: Expected a statement\n  --> canonical_flow/analysis/integration_demo.py:19:2\n   |\n17 |     ArtifactGenerator, QuestionEvaluation, DimensionSummary, PointSummary,\n18 |     MesoClusterAnalysis, MacroAlignment, EvidenceReference\n19 | )\n   |  ^\n   |\n\nF821 Undefined name `Enum`\n  --> canonical_flow/analysis/macro_alignment.py:34:23\n   |\n34 | class ComplianceLevel(Enum):\n   |                       ^^^^\n35 |     \"\"\"Compliance levels based on threshold analysis\"\"\"\n36 |     CUMPLE = \"CUMPLE\"\n   |\n\nF821 Undefined name `Enum`\n  --> canonical_flow/analysis/macro_alignment.py:41:25\n   |\n41 | class ContradictionType(Enum):\n   |                         ^^^^\n42 |     \"\"\"Types of systemic contradictions\"\"\"\n43 |     SCORE_CONFLICT = \"score_conflict\"\n   |\n\nF821 Undefined name `dataclass`\n  --> canonical_flow/analysis/macro_alignment.py:49:2\n   |\n49 | @dataclass\n   |  ^^^^^^^^^\n50 | class PointScore:\n51 |     \"\"\"Individual point-level score with metadata\"\"\"\n   |\n\nF821 Undefined name `List`\n  --> canonical_flow/analysis/macro_alignment.py:57:20\n   |\n55 |     confidence: float\n56 |     evidence_count: int\n57 |     evidence_gaps: List[str] = field(default_factory=list)\n   |                    ^^^^\n58 |     cluster_id: int = 1\n59 |     timestamp: str = field(default_factory=lambda: datetime.now(timezone.utc).isoformat())\n   |\n\nF821 Undefined name `field`\n  --> canonical_flow/analysis/macro_alignment.py:57:32\n   |\n55 |     confidence: float\n56 |     evidence_count: int\n57 |     evidence_gaps: List[str] = field(default_factory=list)\n   |                                ^^^^^\n58 |     cluster_id: int = 1\n59 |     timestamp: str = field(default_factory=lambda: datetime.now(timezone.utc).isoformat())\n   |\n\nF821 Undefined name `field`\n  --> canonical_flow/analysis/macro_alignment.py:59:22\n   |\n57 |     evidence_gaps: List[str] = field(default_factory=list)\n58 |     cluster_id: int = 1\n59 |     timestamp: str = field(default_factory=lambda: datetime.now(timezone.utc).isoformat())\n   |                      ^^^^^\n60 |     \n61 |     def __post_init__(self):\n   |\n\nF821 Undefined name `datetime`\n  --> canonical_flow/analysis/macro_alignment.py:59:52\n   |\n57 |     evidence_gaps: List[str] = field(default_factory=list)\n58 |     cluster_id: int = 1\n59 |     timestamp: str = field(default_factory=lambda: datetime.now(timezone.utc).isoformat())\n   |                                                    ^^^^^^^^\n60 |     \n61 |     def __post_init__(self):\n   |\n\nF821 Undefined name `timezone`\n  --> canonical_flow/analysis/macro_alignment.py:59:65\n   |\n57 |     evidence_gaps: List[str] = field(default_factory=list)\n58 |     cluster_id: int = 1\n59 |     timestamp: str = field(default_factory=lambda: datetime.now(timezone.utc).isoformat())\n   |                                                                 ^^^^^^^^\n60 |     \n61 |     def __post_init__(self):\n   |\n\nF821 Undefined name `dataclass`\n  --> canonical_flow/analysis/macro_alignment.py:65:2\n   |\n65 | @dataclass\n   |  ^^^^^^^^^\n66 | class SystemicContradiction:\n67 |     \"\"\"Systemic contradiction detection result\"\"\"\n   |\n\nF821 Undefined name `List`\n  --> canonical_flow/analysis/macro_alignment.py:70:22\n   |\n68 |     contradiction_id: str\n69 |     contradiction_type: ContradictionType\n70 |     affected_points: List[str]\n   |                      ^^^^\n71 |     cluster_id: int\n72 |     severity: str  # \"high\", \"medium\", \"low\"\n   |\n\nF821 Undefined name `List`\n  --> canonical_flow/analysis/macro_alignment.py:74:27\n   |\n72 |     severity: str  # \"high\", \"medium\", \"low\"\n73 |     description: str\n74 |     conflicting_evidence: List[str] = field(default_factory=list)\n   |                           ^^^^\n75 |     confidence_score: float = 0.0\n   |\n\nF821 Undefined name `field`\n  --> canonical_flow/analysis/macro_alignment.py:74:39\n   |\n72 |     severity: str  # \"high\", \"medium\", \"low\"\n73 |     description: str\n74 |     conflicting_evidence: List[str] = field(default_factory=list)\n   |                                       ^^^^^\n75 |     confidence_score: float = 0.0\n   |\n\nF821 Undefined name `dataclass`\n  --> canonical_flow/analysis/macro_alignment.py:82:2\n   |\n82 | @dataclass\n   |  ^^^^^^^^^\n83 | class ActionRecommendation:\n84 |     \"\"\"Prioritized action recommendation\"\"\"\n   |\n\nF821 Undefined name `List`\n  --> canonical_flow/analysis/macro_alignment.py:90:23\n   |\n88 |     current_score: float\n89 |     improvement_potential: float\n90 |     required_actions: List[str]\n   |                       ^^^^\n91 |     evidence_gaps: List[str]\n92 |     estimated_impact: float\n   |\n\nF821 Undefined name `List`\n  --> canonical_flow/analysis/macro_alignment.py:91:20\n   |\n89 |     improvement_potential: float\n90 |     required_actions: List[str]\n91 |     evidence_gaps: List[str]\n   |                    ^^^^\n92 |     estimated_impact: float\n93 |     implementation_complexity: str  # \"low\", \"medium\", \"high\"\n   |\n\nF821 Undefined name `dataclass`\n   --> canonical_flow/analysis/macro_alignment.py:100:2\n    |\n100 | @dataclass\n    |  ^^^^^^^^^\n101 | class MacroAlignmentResult:\n102 |     \"\"\"Complete macro alignment analysis result\"\"\"\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/analysis/macro_alignment.py:105:22\n    |\n103 |     global_decalogo_score: float\n104 |     compliance_level: ComplianceLevel\n105 |     weighted_scores: Dict[str, float]\n    |                      ^^^^\n106 |     point_scores: Dict[str, PointScore]\n107 |     systemic_contradictions: List[SystemicContradiction]\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/analysis/macro_alignment.py:106:19\n    |\n104 |     compliance_level: ComplianceLevel\n105 |     weighted_scores: Dict[str, float]\n106 |     point_scores: Dict[str, PointScore]\n    |                   ^^^^\n107 |     systemic_contradictions: List[SystemicContradiction]\n108 |     action_recommendations: List[ActionRecommendation]\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/analysis/macro_alignment.py:107:30\n    |\n105 |     weighted_scores: Dict[str, float]\n106 |     point_scores: Dict[str, PointScore]\n107 |     systemic_contradictions: List[SystemicContradiction]\n    |                              ^^^^\n108 |     action_recommendations: List[ActionRecommendation]\n109 |     cluster_analysis: Dict[int, Dict[str, float]]\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/analysis/macro_alignment.py:108:29\n    |\n106 |     point_scores: Dict[str, PointScore]\n107 |     systemic_contradictions: List[SystemicContradiction]\n108 |     action_recommendations: List[ActionRecommendation]\n    |                             ^^^^\n109 |     cluster_analysis: Dict[int, Dict[str, float]]\n110 |     audit_trail: Dict[str, Any]\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/analysis/macro_alignment.py:109:23\n    |\n107 |     systemic_contradictions: List[SystemicContradiction]\n108 |     action_recommendations: List[ActionRecommendation]\n109 |     cluster_analysis: Dict[int, Dict[str, float]]\n    |                       ^^^^\n110 |     audit_trail: Dict[str, Any]\n111 |     processing_metadata: Dict[str, Any]\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/analysis/macro_alignment.py:109:33\n    |\n107 |     systemic_contradictions: List[SystemicContradiction]\n108 |     action_recommendations: List[ActionRecommendation]\n109 |     cluster_analysis: Dict[int, Dict[str, float]]\n    |                                 ^^^^\n110 |     audit_trail: Dict[str, Any]\n111 |     processing_metadata: Dict[str, Any]\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/analysis/macro_alignment.py:110:18\n    |\n108 |     action_recommendations: List[ActionRecommendation]\n109 |     cluster_analysis: Dict[int, Dict[str, float]]\n110 |     audit_trail: Dict[str, Any]\n    |                  ^^^^\n111 |     processing_metadata: Dict[str, Any]\n112 |     timestamp: str = field(default_factory=lambda: datetime.now(timezone.utc).isoformat())\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/analysis/macro_alignment.py:110:28\n    |\n108 |     action_recommendations: List[ActionRecommendation]\n109 |     cluster_analysis: Dict[int, Dict[str, float]]\n110 |     audit_trail: Dict[str, Any]\n    |                            ^^^\n111 |     processing_metadata: Dict[str, Any]\n112 |     timestamp: str = field(default_factory=lambda: datetime.now(timezone.utc).isoformat())\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/analysis/macro_alignment.py:111:26\n    |\n109 |     cluster_analysis: Dict[int, Dict[str, float]]\n110 |     audit_trail: Dict[str, Any]\n111 |     processing_metadata: Dict[str, Any]\n    |                          ^^^^\n112 |     timestamp: str = field(default_factory=lambda: datetime.now(timezone.utc).isoformat())\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/analysis/macro_alignment.py:111:36\n    |\n109 |     cluster_analysis: Dict[int, Dict[str, float]]\n110 |     audit_trail: Dict[str, Any]\n111 |     processing_metadata: Dict[str, Any]\n    |                                    ^^^\n112 |     timestamp: str = field(default_factory=lambda: datetime.now(timezone.utc).isoformat())\n    |\n\nF821 Undefined name `field`\n   --> canonical_flow/analysis/macro_alignment.py:112:22\n    |\n110 |     audit_trail: Dict[str, Any]\n111 |     processing_metadata: Dict[str, Any]\n112 |     timestamp: str = field(default_factory=lambda: datetime.now(timezone.utc).isoformat())\n    |                      ^^^^^\n113 |     \n114 |     def __post_init__(self):\n    |\n\nF821 Undefined name `datetime`\n   --> canonical_flow/analysis/macro_alignment.py:112:52\n    |\n110 |     audit_trail: Dict[str, Any]\n111 |     processing_metadata: Dict[str, Any]\n112 |     timestamp: str = field(default_factory=lambda: datetime.now(timezone.utc).isoformat())\n    |                                                    ^^^^^^^^\n113 |     \n114 |     def __post_init__(self):\n    |\n\nF821 Undefined name `timezone`\n   --> canonical_flow/analysis/macro_alignment.py:112:65\n    |\n110 |     audit_trail: Dict[str, Any]\n111 |     processing_metadata: Dict[str, Any]\n112 |     timestamp: str = field(default_factory=lambda: datetime.now(timezone.utc).isoformat())\n    |                                                                 ^^^^^^^^\n113 |     \n114 |     def __post_init__(self):\n    |\n\nF821 Undefined name `Optional`\n   --> canonical_flow/analysis/macro_alignment.py:176:33\n    |\n174 |     }\n175 |     \n176 |     def __init__(self, weights: Optional[Dict[str, float]] = None):\n    |                                 ^^^^^^^^\n177 |         \"\"\"Initialize macro aggregation system with optional custom weights\"\"\"\n178 |         self.weights = weights or self.DEFAULT_WEIGHTS.copy()\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/analysis/macro_alignment.py:176:42\n    |\n174 |     }\n175 |     \n176 |     def __init__(self, weights: Optional[Dict[str, float]] = None):\n    |                                          ^^^^\n177 |         \"\"\"Initialize macro aggregation system with optional custom weights\"\"\"\n178 |         self.weights = weights or self.DEFAULT_WEIGHTS.copy()\n    |\n\nF821 Undefined name `datetime`\n   --> canonical_flow/analysis/macro_alignment.py:200:22\n    |\n198 |     def _generate_processing_id(self) -> str:\n199 |         \"\"\"Generate deterministic processing ID\"\"\"\n200 |         content = f\"{datetime.now(timezone.utc).date()}_{id(self)}\"\n    |                      ^^^^^^^^\n201 |         return f\"macro_agg_{hashlib.md5(content.encode()).hexdigest()[:8]}\"\n    |\n\nF821 Undefined name `timezone`\n   --> canonical_flow/analysis/macro_alignment.py:200:35\n    |\n198 |     def _generate_processing_id(self) -> str:\n199 |         \"\"\"Generate deterministic processing ID\"\"\"\n200 |         content = f\"{datetime.now(timezone.utc).date()}_{id(self)}\"\n    |                                   ^^^^^^^^\n201 |         return f\"macro_agg_{hashlib.md5(content.encode()).hexdigest()[:8]}\"\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/analysis/macro_alignment.py:203:55\n    |\n201 |         return f\"macro_agg_{hashlib.md5(content.encode()).hexdigest()[:8]}\"\n202 |     \n203 |     def _log_audit_event(self, event_type: str, data: Dict[str, Any]) -> None:\n    |                                                       ^^^^\n204 |         \"\"\"Log audit event with timestamp\"\"\"\n205 |         self.audit_trail.append({\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/analysis/macro_alignment.py:203:65\n    |\n201 |         return f\"macro_agg_{hashlib.md5(content.encode()).hexdigest()[:8]}\"\n202 |     \n203 |     def _log_audit_event(self, event_type: str, data: Dict[str, Any]) -> None:\n    |                                                                 ^^^\n204 |         \"\"\"Log audit event with timestamp\"\"\"\n205 |         self.audit_trail.append({\n    |\n\nF821 Undefined name `datetime`\n   --> canonical_flow/analysis/macro_alignment.py:207:26\n    |\n205 |         self.audit_trail.append({\n206 |             \"event_type\": event_type,\n207 |             \"timestamp\": datetime.now(timezone.utc).isoformat(),\n    |                          ^^^^^^^^\n208 |             \"data\": data\n209 |         })\n    |\n\nF821 Undefined name `timezone`\n   --> canonical_flow/analysis/macro_alignment.py:207:39\n    |\n205 |         self.audit_trail.append({\n206 |             \"event_type\": event_type,\n207 |             \"timestamp\": datetime.now(timezone.utc).isoformat(),\n    |                                       ^^^^^^^^\n208 |             \"data\": data\n209 |         })\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/analysis/macro_alignment.py:220:53\n    |\n218 |             return ComplianceLevel.NO_CUMPLE\n219 |     \n220 |     def _detect_score_conflicts(self, point_scores: Dict[str, PointScore]) -> List[SystemicContradiction]:\n    |                                                     ^^^^\n221 |         \"\"\"Detect score conflicts within clusters\"\"\"\n222 |         contradictions = []\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/analysis/macro_alignment.py:220:79\n    |\n218 |             return ComplianceLevel.NO_CUMPLE\n219 |     \n220 |     def _detect_score_conflicts(self, point_scores: Dict[str, PointScore]) -> List[SystemicContradiction]:\n    |                                                                               ^^^^\n221 |         \"\"\"Detect score conflicts within clusters\"\"\"\n222 |         contradictions = []\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/analysis/macro_alignment.py:260:56\n    |\n258 |         return contradictions\n259 |     \n260 |     def _detect_evidence_conflicts(self, point_scores: Dict[str, PointScore]) -> List[SystemicContradiction]:\n    |                                                        ^^^^\n261 |         \"\"\"Detect evidence conflicts and gaps\"\"\"\n262 |         contradictions = []\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/analysis/macro_alignment.py:260:82\n    |\n258 |         return contradictions\n259 |     \n260 |     def _detect_evidence_conflicts(self, point_scores: Dict[str, PointScore]) -> List[SystemicContradiction]:\n    |                                                                                  ^^^^\n261 |         \"\"\"Detect evidence conflicts and gaps\"\"\"\n262 |         contradictions = []\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/analysis/macro_alignment.py:283:61\n    |\n281 |         return contradictions\n282 |     \n283 |     def _detect_cluster_inconsistencies(self, point_scores: Dict[str, PointScore]) -> List[SystemicContradiction]:\n    |                                                             ^^^^\n284 |         \"\"\"Detect inconsistencies within clusters\"\"\"\n285 |         contradictions = []\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/analysis/macro_alignment.py:283:87\n    |\n281 |         return contradictions\n282 |     \n283 |     def _detect_cluster_inconsistencies(self, point_scores: Dict[str, PointScore]) -> List[SystemicContradiction]:\n    |                                                                                       ^^^^\n284 |         \"\"\"Detect inconsistencies within clusters\"\"\"\n285 |         contradictions = []\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/analysis/macro_alignment.py:327:62\n    |\n325 |         return contradictions\n326 |     \n327 |     def _generate_action_recommendations(self, point_scores: Dict[str, PointScore]) -> List[ActionRecommendation]:\n    |                                                              ^^^^\n328 |         \"\"\"Generate prioritized action recommendations\"\"\"\n329 |         recommendations = []\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/analysis/macro_alignment.py:327:88\n    |\n325 |         return contradictions\n326 |     \n327 |     def _generate_action_recommendations(self, point_scores: Dict[str, PointScore]) -> List[ActionRecommendation]:\n    |                                                                                        ^^^^\n328 |         \"\"\"Generate prioritized action recommendations\"\"\"\n329 |         recommendations = []\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/analysis/macro_alignment.py:374:47\n    |\n372 |         return recommendations\n373 |     \n374 |     def _analyze_clusters(self, point_scores: Dict[str, PointScore]) -> Dict[int, Dict[str, float]]:\n    |                                               ^^^^\n375 |         \"\"\"Analyze cluster-level performance\"\"\"\n376 |         cluster_analysis = {}\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/analysis/macro_alignment.py:374:73\n    |\n372 |         return recommendations\n373 |     \n374 |     def _analyze_clusters(self, point_scores: Dict[str, PointScore]) -> Dict[int, Dict[str, float]]:\n    |                                                                         ^^^^\n375 |         \"\"\"Analyze cluster-level performance\"\"\"\n376 |         cluster_analysis = {}\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/analysis/macro_alignment.py:374:83\n    |\n372 |         return recommendations\n373 |     \n374 |     def _analyze_clusters(self, point_scores: Dict[str, PointScore]) -> Dict[int, Dict[str, float]]:\n    |                                                                                   ^^^^\n375 |         \"\"\"Analyze cluster-level performance\"\"\"\n376 |         cluster_analysis = {}\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/analysis/macro_alignment.py:419:27\n    |\n417 |     def compute_macro_alignment(\n418 |         self,\n419 |         point_level_data: Dict[str, Any],\n    |                           ^^^^\n420 |         custom_weights: Optional[Dict[str, float]] = None\n421 |     ) -> MacroAlignmentResult:\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/analysis/macro_alignment.py:419:37\n    |\n417 |     def compute_macro_alignment(\n418 |         self,\n419 |         point_level_data: Dict[str, Any],\n    |                                     ^^^\n420 |         custom_weights: Optional[Dict[str, float]] = None\n421 |     ) -> MacroAlignmentResult:\n    |\n\nF821 Undefined name `Optional`\n   --> canonical_flow/analysis/macro_alignment.py:420:25\n    |\n418 |         self,\n419 |         point_level_data: Dict[str, Any],\n420 |         custom_weights: Optional[Dict[str, float]] = None\n    |                         ^^^^^^^^\n421 |     ) -> MacroAlignmentResult:\n422 |         \"\"\"\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/analysis/macro_alignment.py:420:34\n    |\n418 |         self,\n419 |         point_level_data: Dict[str, Any],\n420 |         custom_weights: Optional[Dict[str, float]] = None\n    |                                  ^^^^\n421 |     ) -> MacroAlignmentResult:\n422 |         \"\"\"\n    |\n\nF821 Undefined name `datetime`\n   --> canonical_flow/analysis/macro_alignment.py:432:22\n    |\n430 |             Complete macro alignment result with audit trail\n431 |         \"\"\"\n432 |         start_time = datetime.now(timezone.utc)\n    |                      ^^^^^^^^\n433 |         \n434 |         # Apply custom weights if provided\n    |\n\nF821 Undefined name `timezone`\n   --> canonical_flow/analysis/macro_alignment.py:432:35\n    |\n430 |             Complete macro alignment result with audit trail\n431 |         \"\"\"\n432 |         start_time = datetime.now(timezone.utc)\n    |                                   ^^^^^^^^\n433 |         \n434 |         # Apply custom weights if provided\n    |\n\nF821 Undefined name `datetime`\n   --> canonical_flow/analysis/macro_alignment.py:505:20\n    |\n504 |         # Create processing metadata\n505 |         end_time = datetime.now(timezone.utc)\n    |                    ^^^^^^^^\n506 |         processing_metadata = {\n507 |             \"processing_id\": self.processing_id,\n    |\n\nF821 Undefined name `timezone`\n   --> canonical_flow/analysis/macro_alignment.py:505:33\n    |\n504 |         # Create processing metadata\n505 |         end_time = datetime.now(timezone.utc)\n    |                                 ^^^^^^^^\n506 |         processing_metadata = {\n507 |             \"processing_id\": self.processing_id,\n    |\n\nF821 Undefined name `Optional`\n   --> canonical_flow/analysis/macro_alignment.py:538:71\n    |\n536 |         return result\n537 |     \n538 |     def save_results(self, result: MacroAlignmentResult, output_path: Optional[str] = None) -> str:\n    |                                                                       ^^^^^^^^\n539 |         \"\"\"\n540 |         Save macro alignment results to JSON file with deterministic ordering.\n    |\n\nF821 Undefined name `Path`\n   --> canonical_flow/analysis/macro_alignment.py:553:23\n    |\n552 |         # Ensure output directory exists\n553 |         output_file = Path(output_path)\n    |                       ^^^^\n554 |         output_file.parent.mkdir(parents=True, exist_ok=True)\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/analysis/macro_alignment.py:571:66\n    |\n569 |         return str(output_file)\n570 |     \n571 |     def _serialize_result(self, result: MacroAlignmentResult) -> Dict[str, Any]:\n    |                                                                  ^^^^\n572 |         \"\"\"Serialize result with custom handling for complex types\"\"\"\n573 |         result_dict = asdict(result)\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/analysis/macro_alignment.py:571:76\n    |\n569 |         return str(output_file)\n570 |     \n571 |     def _serialize_result(self, result: MacroAlignmentResult) -> Dict[str, Any]:\n    |                                                                            ^^^\n572 |         \"\"\"Serialize result with custom handling for complex types\"\"\"\n573 |         result_dict = asdict(result)\n    |\n\nF821 Undefined name `asdict`\n   --> canonical_flow/analysis/macro_alignment.py:573:23\n    |\n571 |     def _serialize_result(self, result: MacroAlignmentResult) -> Dict[str, Any]:\n572 |         \"\"\"Serialize result with custom handling for complex types\"\"\"\n573 |         result_dict = asdict(result)\n    |                       ^^^^^^\n574 |         \n575 |         # Convert enums to strings\n    |\n\nF821 Undefined name `asdict`\n   --> canonical_flow/analysis/macro_alignment.py:581:49\n    |\n579 |         point_scores_serialized = {}\n580 |         for point_id, point_score in result.point_scores.items():\n581 |             point_scores_serialized[point_id] = asdict(point_score)\n    |                                                 ^^^^^^\n582 |         result_dict[\"point_scores\"] = point_scores_serialized\n    |\n\nF821 Undefined name `asdict`\n   --> canonical_flow/analysis/macro_alignment.py:587:34\n    |\n585 |         contradictions_serialized = []\n586 |         for contradiction in result.systemic_contradictions:\n587 |             contradiction_dict = asdict(contradiction)\n    |                                  ^^^^^^\n588 |             contradiction_dict[\"contradiction_type\"] = contradiction.contradiction_type.value\n589 |             contradictions_serialized.append(contradiction_dict)\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/analysis/macro_alignment.py:595:29\n    |\n595 | def create_sample_data() -> Dict[str, Any]:\n    |                             ^^^^\n596 |     \"\"\"Create sample point-level data for testing\"\"\"\n597 |     sample_data = {}\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/analysis/macro_alignment.py:595:39\n    |\n595 | def create_sample_data() -> Dict[str, Any]:\n    |                                       ^^^\n596 |     \"\"\"Create sample point-level data for testing\"\"\"\n597 |     sample_data = {}\n    |\n\nF541 [*] f-string without any placeholders\n   --> canonical_flow/analysis/macro_alignment.py:643:11\n    |\n641 |     output_path = system.save_results(result)\n642 |     \n643 |     print(f\"Macro alignment analysis complete!\")\n    |           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n644 |     print(f\"Global Dec\u00e1logo Score: {result.global_decalogo_score:.3f}\")\n645 |     print(f\"Compliance Level: {result.compliance_level.value}\")\n    |\nhelp: Remove extraneous `f` prefix\n\nF821 Undefined name `Enum`\n  --> canonical_flow/analysis/per_point_scoring_system.py:31:23\n   |\n29 | logger = logging.getLogger(__name__)\n30 |\n31 | class ComplianceLevel(Enum):\n   |                       ^^^^\n32 |     \"\"\"Compliance classification levels.\"\"\"\n33 |     CUMPLE = \"CUMPLE\"\n   |\n\nF821 Undefined name `dataclass`\n  --> canonical_flow/analysis/per_point_scoring_system.py:37:2\n   |\n35 |     NO_CUMPLE = \"NO_CUMPLE\"\n36 |\n37 | @dataclass\n   |  ^^^^^^^^^\n38 | class DimensionScore:\n39 |     \"\"\"Score for a single dimension.\"\"\"\n   |\n\nF821 Undefined name `List`\n  --> canonical_flow/analysis/per_point_scoring_system.py:46:23\n   |\n44 |     question_count: int\n45 |     questions_answered: int\n46 |     top_contributors: List[Dict[str, Any]]\n   |                       ^^^^\n47 |\n48 | @dataclass  \n   |\n\nF821 Undefined name `Dict`\n  --> canonical_flow/analysis/per_point_scoring_system.py:46:28\n   |\n44 |     question_count: int\n45 |     questions_answered: int\n46 |     top_contributors: List[Dict[str, Any]]\n   |                            ^^^^\n47 |\n48 | @dataclass  \n   |\n\nF821 Undefined name `Any`\n  --> canonical_flow/analysis/per_point_scoring_system.py:46:38\n   |\n44 |     question_count: int\n45 |     questions_answered: int\n46 |     top_contributors: List[Dict[str, Any]]\n   |                                      ^^^\n47 |\n48 | @dataclass  \n   |\n\nF821 Undefined name `dataclass`\n  --> canonical_flow/analysis/per_point_scoring_system.py:48:2\n   |\n46 |     top_contributors: List[Dict[str, Any]]\n47 |\n48 | @dataclass  \n   |  ^^^^^^^^^\n49 | class PointScoreResult:\n50 |     \"\"\"Complete scoring result for a Dec\u00e1logo point.\"\"\"\n   |\n\nF821 Undefined name `Dict`\n  --> canonical_flow/analysis/per_point_scoring_system.py:54:23\n   |\n52 |     final_score: float\n53 |     compliance_level: ComplianceLevel\n54 |     dimension_scores: Dict[str, DimensionScore]\n   |                       ^^^^\n55 |     total_questions: int\n56 |     total_answered: int\n   |\n\nF821 Undefined name `List`\n  --> canonical_flow/analysis/per_point_scoring_system.py:58:33\n   |\n56 |     total_answered: int\n57 |     completion_rate: float\n58 |     top_contributing_questions: List[Dict[str, Any]]\n   |                                 ^^^^\n59 |     evidence_summary: Dict[str, Any]\n60 |     calculation_metadata: Dict[str, Any]\n   |\n\nF821 Undefined name `Dict`\n  --> canonical_flow/analysis/per_point_scoring_system.py:58:38\n   |\n56 |     total_answered: int\n57 |     completion_rate: float\n58 |     top_contributing_questions: List[Dict[str, Any]]\n   |                                      ^^^^\n59 |     evidence_summary: Dict[str, Any]\n60 |     calculation_metadata: Dict[str, Any]\n   |\n\nF821 Undefined name `Any`\n  --> canonical_flow/analysis/per_point_scoring_system.py:58:48\n   |\n56 |     total_answered: int\n57 |     completion_rate: float\n58 |     top_contributing_questions: List[Dict[str, Any]]\n   |                                                ^^^\n59 |     evidence_summary: Dict[str, Any]\n60 |     calculation_metadata: Dict[str, Any]\n   |\n\nF821 Undefined name `Dict`\n  --> canonical_flow/analysis/per_point_scoring_system.py:59:23\n   |\n57 |     completion_rate: float\n58 |     top_contributing_questions: List[Dict[str, Any]]\n59 |     evidence_summary: Dict[str, Any]\n   |                       ^^^^\n60 |     calculation_metadata: Dict[str, Any]\n   |\n\nF821 Undefined name `Any`\n  --> canonical_flow/analysis/per_point_scoring_system.py:59:33\n   |\n57 |     completion_rate: float\n58 |     top_contributing_questions: List[Dict[str, Any]]\n59 |     evidence_summary: Dict[str, Any]\n   |                                 ^^^\n60 |     calculation_metadata: Dict[str, Any]\n   |\n\nF821 Undefined name `Dict`\n  --> canonical_flow/analysis/per_point_scoring_system.py:60:27\n   |\n58 |     top_contributing_questions: List[Dict[str, Any]]\n59 |     evidence_summary: Dict[str, Any]\n60 |     calculation_metadata: Dict[str, Any]\n   |                           ^^^^\n61 |\n62 | class PerPointScoringSystem:\n   |\n\nF821 Undefined name `Any`\n  --> canonical_flow/analysis/per_point_scoring_system.py:60:37\n   |\n58 |     top_contributing_questions: List[Dict[str, Any]]\n59 |     evidence_summary: Dict[str, Any]\n60 |     calculation_metadata: Dict[str, Any]\n   |                                     ^^^\n61 |\n62 | class PerPointScoringSystem:\n   |\n\nF821 Undefined name `Decimal`\n  --> canonical_flow/analysis/per_point_scoring_system.py:72:17\n   |\n70 |     # Dimension weights as specified\n71 |     DIMENSION_WEIGHTS = {\n72 |         \"DE-1\": Decimal(\"0.30\"),\n   |                 ^^^^^^^\n73 |         \"DE-2\": Decimal(\"0.25\"), \n74 |         \"DE-3\": Decimal(\"0.25\"),\n   |\n\nF821 Undefined name `Decimal`\n  --> canonical_flow/analysis/per_point_scoring_system.py:73:17\n   |\n71 |     DIMENSION_WEIGHTS = {\n72 |         \"DE-1\": Decimal(\"0.30\"),\n73 |         \"DE-2\": Decimal(\"0.25\"), \n   |                 ^^^^^^^\n74 |         \"DE-3\": Decimal(\"0.25\"),\n75 |         \"DE-4\": Decimal(\"0.20\")\n   |\n\nF821 Undefined name `Decimal`\n  --> canonical_flow/analysis/per_point_scoring_system.py:74:17\n   |\n72 |         \"DE-1\": Decimal(\"0.30\"),\n73 |         \"DE-2\": Decimal(\"0.25\"), \n74 |         \"DE-3\": Decimal(\"0.25\"),\n   |                 ^^^^^^^\n75 |         \"DE-4\": Decimal(\"0.20\")\n76 |     }\n   |\n\nF821 Undefined name `Decimal`\n  --> canonical_flow/analysis/per_point_scoring_system.py:75:17\n   |\n73 |         \"DE-2\": Decimal(\"0.25\"), \n74 |         \"DE-3\": Decimal(\"0.25\"),\n75 |         \"DE-4\": Decimal(\"0.20\")\n   |                 ^^^^^^^\n76 |     }\n   |\n\nF821 Undefined name `Decimal`\n  --> canonical_flow/analysis/per_point_scoring_system.py:80:33\n   |\n78 |     # Compliance thresholds\n79 |     COMPLIANCE_THRESHOLDS = {\n80 |         ComplianceLevel.CUMPLE: Decimal(\"0.80\"),\n   |                                 ^^^^^^^\n81 |         ComplianceLevel.CUMPLE_PARCIAL: Decimal(\"0.50\")\n82 |     }\n   |\n\nF821 Undefined name `Decimal`\n  --> canonical_flow/analysis/per_point_scoring_system.py:81:41\n   |\n79 |     COMPLIANCE_THRESHOLDS = {\n80 |         ComplianceLevel.CUMPLE: Decimal(\"0.80\"),\n81 |         ComplianceLevel.CUMPLE_PARCIAL: Decimal(\"0.50\")\n   |                                         ^^^^^^^\n82 |     }\n   |\n\nF821 Undefined name `Path`\n  --> canonical_flow/analysis/per_point_scoring_system.py:95:27\n   |\n93 |     def __init__(self, output_dir: str = \"canonical_flow/analysis/\"):\n94 |         \"\"\"Initialize the scoring system.\"\"\"\n95 |         self.output_dir = Path(output_dir)\n   |                           ^^^^\n96 |         self.output_dir.mkdir(parents=True, exist_ok=True)\n   |\n\nF821 Undefined name `Decimal`\n   --> canonical_flow/analysis/per_point_scoring_system.py:100:17\n    |\n 98 |         # Validate dimension weights sum to 1.0\n 99 |         weight_sum = sum(self.DIMENSION_WEIGHTS.values())\n100 |         if not (Decimal(\"0.999\") <= weight_sum <= Decimal(\"1.001\")):\n    |                 ^^^^^^^\n101 |             raise ValueError(f\"Dimension weights must sum to 1.0, got {weight_sum}\")\n    |\n\nF821 Undefined name `Decimal`\n   --> canonical_flow/analysis/per_point_scoring_system.py:100:51\n    |\n 98 |         # Validate dimension weights sum to 1.0\n 99 |         weight_sum = sum(self.DIMENSION_WEIGHTS.values())\n100 |         if not (Decimal(\"0.999\") <= weight_sum <= Decimal(\"1.001\")):\n    |                                                   ^^^^^^^\n101 |             raise ValueError(f\"Dimension weights must sum to 1.0, got {weight_sum}\")\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/analysis/per_point_scoring_system.py:108:27\n    |\n106 |         self,\n107 |         point_id: int,\n108 |         dimension_scores: Dict[str, Dict[str, Any]],\n    |                           ^^^^\n109 |         evidence_data: Optional[Dict[str, Any]] = None\n110 |     ) -> PointScoreResult:\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/analysis/per_point_scoring_system.py:108:37\n    |\n106 |         self,\n107 |         point_id: int,\n108 |         dimension_scores: Dict[str, Dict[str, Any]],\n    |                                     ^^^^\n109 |         evidence_data: Optional[Dict[str, Any]] = None\n110 |     ) -> PointScoreResult:\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/analysis/per_point_scoring_system.py:108:47\n    |\n106 |         self,\n107 |         point_id: int,\n108 |         dimension_scores: Dict[str, Dict[str, Any]],\n    |                                               ^^^\n109 |         evidence_data: Optional[Dict[str, Any]] = None\n110 |     ) -> PointScoreResult:\n    |\n\nF821 Undefined name `Optional`\n   --> canonical_flow/analysis/per_point_scoring_system.py:109:24\n    |\n107 |         point_id: int,\n108 |         dimension_scores: Dict[str, Dict[str, Any]],\n109 |         evidence_data: Optional[Dict[str, Any]] = None\n    |                        ^^^^^^^^\n110 |     ) -> PointScoreResult:\n111 |         \"\"\"\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/analysis/per_point_scoring_system.py:109:33\n    |\n107 |         point_id: int,\n108 |         dimension_scores: Dict[str, Dict[str, Any]],\n109 |         evidence_data: Optional[Dict[str, Any]] = None\n    |                                 ^^^^\n110 |     ) -> PointScoreResult:\n111 |         \"\"\"\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/analysis/per_point_scoring_system.py:109:43\n    |\n107 |         point_id: int,\n108 |         dimension_scores: Dict[str, Dict[str, Any]],\n109 |         evidence_data: Optional[Dict[str, Any]] = None\n    |                                           ^^^\n110 |     ) -> PointScoreResult:\n111 |         \"\"\"\n    |\n\nF821 Undefined name `Decimal`\n   --> canonical_flow/analysis/per_point_scoring_system.py:129:32\n    |\n127 |         # Calculate dimension-level scores\n128 |         calculated_dimensions = {}\n129 |         total_weighted_score = Decimal(\"0\")\n    |                                ^^^^^^^\n130 |         total_questions = 0\n131 |         total_answered = 0\n    |\n\nF821 Undefined name `Decimal`\n   --> canonical_flow/analysis/per_point_scoring_system.py:144:37\n    |\n143 |             # Accumulate weighted score\n144 |             weighted_contribution = Decimal(str(dim_score.raw_score)) * weight\n    |                                     ^^^^^^^\n145 |             total_weighted_score += weighted_contribution\n    |\n\nF821 Undefined name `Decimal`\n   --> canonical_flow/analysis/per_point_scoring_system.py:155:59\n    |\n154 |         # Calculate final score\n155 |         final_score = float(total_weighted_score.quantize(Decimal(\"0.0001\"), rounding=ROUND_HALF_UP))\n    |                                                           ^^^^^^^\n156 |         \n157 |         # Determine compliance level\n    |\n\nF821 Undefined name `ROUND_HALF_UP`\n   --> canonical_flow/analysis/per_point_scoring_system.py:155:87\n    |\n154 |         # Calculate final score\n155 |         final_score = float(total_weighted_score.quantize(Decimal(\"0.0001\"), rounding=ROUND_HALF_UP))\n    |                                                                                       ^^^^^^^^^^^^^\n156 |         \n157 |         # Determine compliance level\n    |\n\nF821 Undefined name `datetime`\n   --> canonical_flow/analysis/per_point_scoring_system.py:171:38\n    |\n169 |         # Calculation metadata\n170 |         calculation_metadata = {\n171 |             \"calculation_timestamp\": datetime.now().isoformat(),\n    |                                      ^^^^^^^^\n172 |             \"dimension_weights\": {k: float(v) for k, v in self.DIMENSION_WEIGHTS.items()},\n173 |             \"compliance_thresholds\": {\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/analysis/per_point_scoring_system.py:200:19\n    |\n198 |         self,\n199 |         dimension: str,\n200 |         dim_data: Dict[str, Any]\n    |                   ^^^^\n201 |     ) -> DimensionScore:\n202 |         \"\"\"Calculate score for a single dimension.\"\"\"\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/analysis/per_point_scoring_system.py:200:29\n    |\n198 |         self,\n199 |         dimension: str,\n200 |         dim_data: Dict[str, Any]\n    |                             ^^^\n201 |     ) -> DimensionScore:\n202 |         \"\"\"Calculate score for a single dimension.\"\"\"\n    |\n\nF821 Undefined name `Decimal`\n   --> canonical_flow/analysis/per_point_scoring_system.py:209:27\n    |\n207 |         if questions:\n208 | # # #             # Sum scores from answered questions  # Module not found  # Module not found  # Module not found\n209 |             total_score = Decimal(\"0\")\n    |                           ^^^^^^^\n210 |             answered_count = 0\n211 |             contributors = []\n    |\n\nF821 Undefined name `Decimal`\n   --> canonical_flow/analysis/per_point_scoring_system.py:221:38\n    |\n219 |                 if q_score is not None:\n220 |                     normalized_score = max(0, min(1, float(q_score)))\n221 |                     weighted_score = Decimal(str(normalized_score)) * Decimal(str(q_weight))\n    |                                      ^^^^^^^\n222 |                     total_score += weighted_score\n223 |                     answered_count += 1\n    |\n\nF821 Undefined name `Decimal`\n   --> canonical_flow/analysis/per_point_scoring_system.py:221:71\n    |\n219 |                 if q_score is not None:\n220 |                     normalized_score = max(0, min(1, float(q_score)))\n221 |                     weighted_score = Decimal(str(normalized_score)) * Decimal(str(q_weight))\n    |                                                                       ^^^^^^^\n222 |                     total_score += weighted_score\n223 |                     answered_count += 1\n    |\n\nF821 Undefined name `Decimal`\n   --> canonical_flow/analysis/per_point_scoring_system.py:245:62\n    |\n244 |         # Apply dimension weight\n245 |         weight = float(self.DIMENSION_WEIGHTS.get(dimension, Decimal(\"0\")))\n    |                                                              ^^^^^^^\n246 |         weighted_score = raw_score * weight\n    |\n\nF821 Undefined name `Decimal`\n   --> canonical_flow/analysis/per_point_scoring_system.py:263:25\n    |\n261 |     def _classify_compliance(self, score: float) -> ComplianceLevel:\n262 |         \"\"\"Classify compliance level based on score.\"\"\"\n263 |         score_decimal = Decimal(str(score))\n    |                         ^^^^^^^\n264 |         \n265 |         if score_decimal >= self.COMPLIANCE_THRESHOLDS[ComplianceLevel.CUMPLE]:\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/analysis/per_point_scoring_system.py:272:48\n    |\n270 |             return ComplianceLevel.NO_CUMPLE\n271 |     \n272 |     def _rank_contributors(self, contributors: List[Dict[str, Any]], limit: int = 10) -> List[Dict[str, Any]]:\n    |                                                ^^^^\n273 |         \"\"\"Rank and return top contributing questions.\"\"\"\n274 |         sorted_contributors = sorted(contributors, key=lambda x: x.get(\"contribution\", 0), reverse=True)\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/analysis/per_point_scoring_system.py:272:53\n    |\n270 |             return ComplianceLevel.NO_CUMPLE\n271 |     \n272 |     def _rank_contributors(self, contributors: List[Dict[str, Any]], limit: int = 10) -> List[Dict[str, Any]]:\n    |                                                     ^^^^\n273 |         \"\"\"Rank and return top contributing questions.\"\"\"\n274 |         sorted_contributors = sorted(contributors, key=lambda x: x.get(\"contribution\", 0), reverse=True)\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/analysis/per_point_scoring_system.py:272:63\n    |\n270 |             return ComplianceLevel.NO_CUMPLE\n271 |     \n272 |     def _rank_contributors(self, contributors: List[Dict[str, Any]], limit: int = 10) -> List[Dict[str, Any]]:\n    |                                                               ^^^\n273 |         \"\"\"Rank and return top contributing questions.\"\"\"\n274 |         sorted_contributors = sorted(contributors, key=lambda x: x.get(\"contribution\", 0), reverse=True)\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/analysis/per_point_scoring_system.py:272:90\n    |\n270 |             return ComplianceLevel.NO_CUMPLE\n271 |     \n272 |     def _rank_contributors(self, contributors: List[Dict[str, Any]], limit: int = 10) -> List[Dict[str, Any]]:\n    |                                                                                          ^^^^\n273 |         \"\"\"Rank and return top contributing questions.\"\"\"\n274 |         sorted_contributors = sorted(contributors, key=lambda x: x.get(\"contribution\", 0), reverse=True)\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/analysis/per_point_scoring_system.py:272:95\n    |\n270 |             return ComplianceLevel.NO_CUMPLE\n271 |     \n272 |     def _rank_contributors(self, contributors: List[Dict[str, Any]], limit: int = 10) -> List[Dict[str, Any]]:\n    |                                                                                               ^^^^\n273 |         \"\"\"Rank and return top contributing questions.\"\"\"\n274 |         sorted_contributors = sorted(contributors, key=lambda x: x.get(\"contribution\", 0), reverse=True)\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/analysis/per_point_scoring_system.py:272:105\n    |\n270 |             return ComplianceLevel.NO_CUMPLE\n271 |     \n272 |     def _rank_contributors(self, contributors: List[Dict[str, Any]], limit: int = 10) -> List[Dict[str, Any]]:\n    |                                                                                                         ^^^\n273 |         \"\"\"Rank and return top contributing questions.\"\"\"\n274 |         sorted_contributors = sorted(contributors, key=lambda x: x.get(\"contribution\", 0), reverse=True)\n    |\n\nF821 Undefined name `Optional`\n   --> canonical_flow/analysis/per_point_scoring_system.py:279:24\n    |\n277 |     def _generate_evidence_summary(\n278 |         self,\n279 |         evidence_data: Optional[Dict[str, Any]],\n    |                        ^^^^^^^^\n280 |         dimension_scores: Dict[str, Dict[str, Any]]\n281 |     ) -> Dict[str, Any]:\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/analysis/per_point_scoring_system.py:279:33\n    |\n277 |     def _generate_evidence_summary(\n278 |         self,\n279 |         evidence_data: Optional[Dict[str, Any]],\n    |                                 ^^^^\n280 |         dimension_scores: Dict[str, Dict[str, Any]]\n281 |     ) -> Dict[str, Any]:\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/analysis/per_point_scoring_system.py:279:43\n    |\n277 |     def _generate_evidence_summary(\n278 |         self,\n279 |         evidence_data: Optional[Dict[str, Any]],\n    |                                           ^^^\n280 |         dimension_scores: Dict[str, Dict[str, Any]]\n281 |     ) -> Dict[str, Any]:\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/analysis/per_point_scoring_system.py:280:27\n    |\n278 |         self,\n279 |         evidence_data: Optional[Dict[str, Any]],\n280 |         dimension_scores: Dict[str, Dict[str, Any]]\n    |                           ^^^^\n281 |     ) -> Dict[str, Any]:\n282 |         \"\"\"Generate summary of evidence supporting the scores.\"\"\"\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/analysis/per_point_scoring_system.py:280:37\n    |\n278 |         self,\n279 |         evidence_data: Optional[Dict[str, Any]],\n280 |         dimension_scores: Dict[str, Dict[str, Any]]\n    |                                     ^^^^\n281 |     ) -> Dict[str, Any]:\n282 |         \"\"\"Generate summary of evidence supporting the scores.\"\"\"\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/analysis/per_point_scoring_system.py:280:47\n    |\n278 |         self,\n279 |         evidence_data: Optional[Dict[str, Any]],\n280 |         dimension_scores: Dict[str, Dict[str, Any]]\n    |                                               ^^^\n281 |     ) -> Dict[str, Any]:\n282 |         \"\"\"Generate summary of evidence supporting the scores.\"\"\"\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/analysis/per_point_scoring_system.py:281:10\n    |\n279 |         evidence_data: Optional[Dict[str, Any]],\n280 |         dimension_scores: Dict[str, Dict[str, Any]]\n281 |     ) -> Dict[str, Any]:\n    |          ^^^^\n282 |         \"\"\"Generate summary of evidence supporting the scores.\"\"\"\n283 |         summary = {\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/analysis/per_point_scoring_system.py:281:20\n    |\n279 |         evidence_data: Optional[Dict[str, Any]],\n280 |         dimension_scores: Dict[str, Dict[str, Any]]\n281 |     ) -> Dict[str, Any]:\n    |                    ^^^\n282 |         \"\"\"Generate summary of evidence supporting the scores.\"\"\"\n283 |         summary = {\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/analysis/per_point_scoring_system.py:314:22\n    |\n312 |     def process_all_points(\n313 |         self,\n314 |         points_data: Dict[int, Dict[str, Any]],\n    |                      ^^^^\n315 |         output_filename: Optional[str] = None\n316 |     ) -> Dict[int, PointScoreResult]:\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/analysis/per_point_scoring_system.py:314:32\n    |\n312 |     def process_all_points(\n313 |         self,\n314 |         points_data: Dict[int, Dict[str, Any]],\n    |                                ^^^^\n315 |         output_filename: Optional[str] = None\n316 |     ) -> Dict[int, PointScoreResult]:\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/analysis/per_point_scoring_system.py:314:42\n    |\n312 |     def process_all_points(\n313 |         self,\n314 |         points_data: Dict[int, Dict[str, Any]],\n    |                                          ^^^\n315 |         output_filename: Optional[str] = None\n316 |     ) -> Dict[int, PointScoreResult]:\n    |\n\nF821 Undefined name `Optional`\n   --> canonical_flow/analysis/per_point_scoring_system.py:315:26\n    |\n313 |         self,\n314 |         points_data: Dict[int, Dict[str, Any]],\n315 |         output_filename: Optional[str] = None\n    |                          ^^^^^^^^\n316 |     ) -> Dict[int, PointScoreResult]:\n317 |         \"\"\"\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/analysis/per_point_scoring_system.py:316:10\n    |\n314 |         points_data: Dict[int, Dict[str, Any]],\n315 |         output_filename: Optional[str] = None\n316 |     ) -> Dict[int, PointScoreResult]:\n    |          ^^^^\n317 |         \"\"\"\n318 |         Process scoring for all 10 Dec\u00e1logo points.\n    |\n\nF821 Undefined name `datetime`\n   --> canonical_flow/analysis/per_point_scoring_system.py:334:37\n    |\n332 |             \"compliance_distribution\": {level.value: 0 for level in ComplianceLevel},\n333 |             \"average_score\": 0.0,\n334 |             \"processing_timestamp\": datetime.now().isoformat()\n    |                                     ^^^^^^^^\n335 |         }\n    |\n\nF821 Undefined name `datetime`\n   --> canonical_flow/analysis/per_point_scoring_system.py:366:25\n    |\n364 |         # Save results to JSON\n365 |         if output_filename is None:\n366 |             timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    |                         ^^^^^^^^\n367 |             output_filename = f\"per_point_scores_{timestamp}.json\"\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/analysis/per_point_scoring_system.py:377:18\n    |\n375 |     def _save_results(\n376 |         self,\n377 |         results: Dict[int, PointScoreResult], \n    |                  ^^^^\n378 |         summary: Dict[str, Any],\n379 |         output_path: Path\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/analysis/per_point_scoring_system.py:378:18\n    |\n376 |         self,\n377 |         results: Dict[int, PointScoreResult], \n378 |         summary: Dict[str, Any],\n    |                  ^^^^\n379 |         output_path: Path\n380 |     ) -> None:\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/analysis/per_point_scoring_system.py:378:28\n    |\n376 |         self,\n377 |         results: Dict[int, PointScoreResult], \n378 |         summary: Dict[str, Any],\n    |                            ^^^\n379 |         output_path: Path\n380 |     ) -> None:\n    |\n\nF821 Undefined name `Path`\n   --> canonical_flow/analysis/per_point_scoring_system.py:379:22\n    |\n377 |         results: Dict[int, PointScoreResult], \n378 |         summary: Dict[str, Any],\n379 |         output_path: Path\n    |                      ^^^^\n380 |     ) -> None:\n381 |         \"\"\"Save scoring results to JSON file.\"\"\"\n    |\n\nF821 Undefined name `asdict`\n   --> canonical_flow/analysis/per_point_scoring_system.py:403:26\n    |\n401 |                 \"compliance_level\": result.compliance_level.value,\n402 |                 \"dimension_scores\": {\n403 |                     dim: asdict(score) for dim, score in result.dimension_scores.items()\n    |                          ^^^^^^\n404 |                 },\n405 |                 \"total_questions\": result.total_questions,\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/analysis/per_point_scoring_system.py:423:18\n    |\n421 |     def generate_explainability_report(\n422 |         self,\n423 |         results: Dict[int, PointScoreResult],\n    |                  ^^^^\n424 |         output_filename: Optional[str] = None\n425 |     ) -> Path:\n    |\n\nF821 Undefined name `Optional`\n   --> canonical_flow/analysis/per_point_scoring_system.py:424:26\n    |\n422 |         self,\n423 |         results: Dict[int, PointScoreResult],\n424 |         output_filename: Optional[str] = None\n    |                          ^^^^^^^^\n425 |     ) -> Path:\n426 |         \"\"\"\n    |\n\nF821 Undefined name `Path`\n   --> canonical_flow/analysis/per_point_scoring_system.py:425:10\n    |\n423 |         results: Dict[int, PointScoreResult],\n424 |         output_filename: Optional[str] = None\n425 |     ) -> Path:\n    |          ^^^^\n426 |         \"\"\"\n427 |         Generate detailed explainability report for all points.\n    |\n\nF821 Undefined name `datetime`\n   --> canonical_flow/analysis/per_point_scoring_system.py:437:25\n    |\n435 |         \"\"\"\n436 |         if output_filename is None:\n437 |             timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    |                         ^^^^^^^^\n438 |             output_filename = f\"explainability_report_{timestamp}.json\"\n    |\n\nF821 Undefined name `datetime`\n   --> canonical_flow/analysis/per_point_scoring_system.py:444:41\n    |\n442 |         explainability_data = {\n443 |             \"report_metadata\": {\n444 |                 \"generation_timestamp\": datetime.now().isoformat(),\n    |                                         ^^^^^^^^\n445 |                 \"report_type\": \"per_point_explainability\",\n446 |                 \"points_analyzed\": len(results)\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/analysis/per_point_scoring_system.py:473:72\n    |\n471 |             raise\n472 |     \n473 |     def _generate_point_explanation(self, result: PointScoreResult) -> Dict[str, Any]:\n    |                                                                        ^^^^\n474 |         \"\"\"Generate detailed explanation for a single point.\"\"\"\n475 |         explanation = {\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/analysis/per_point_scoring_system.py:473:82\n    |\n471 |             raise\n472 |     \n473 |     def _generate_point_explanation(self, result: PointScoreResult) -> Dict[str, Any]:\n    |                                                                                  ^^^\n474 |         \"\"\"Generate detailed explanation for a single point.\"\"\"\n475 |         explanation = {\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/analysis/per_point_scoring_system.py:517:70\n    |\n515 |         return explanation\n516 |     \n517 |     def _generate_recommendations(self, result: PointScoreResult) -> List[str]:\n    |                                                                      ^^^^\n518 |         \"\"\"Generate recommendations based on scoring results.\"\"\"\n519 |         recommendations = []\n    |\n\nF541 [*] f-string without any placeholders\n   --> canonical_flow/analysis/per_point_scoring_system.py:607:11\n    |\n606 |     print(f\"\u2705 Processed {len(results)} points\")\n607 |     print(f\"\ud83d\udcca Results saved to: canonical_flow/analysis/\")\n    |           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n608 |     print(f\"\ud83d\udccb Explainability report: {report_path}\")\n    |\nhelp: Remove extraneous `f` prefix\n\nF541 [*] f-string without any placeholders\n   --> canonical_flow/analysis/per_point_scoring_system.py:616:11\n    |\n614 |         compliance_counts[level] = compliance_counts.get(level, 0) + 1\n615 |     \n616 |     print(f\"\\n\ud83d\udcc8 Compliance Distribution:\")\n    |           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n617 |     for level, count in compliance_counts.items():\n618 |         print(f\"  {level}: {count} points\")\n    |\nhelp: Remove extraneous `f` prefix\n\ninvalid-syntax: Unexpected indentation\n  --> canonical_flow/analysis/test_artifact_generator.py:14:1\n   |\n13 | # # # from canonical_flow.analysis.artifact_generator import (  # Module not found  # Module not found  # Module not found\n14 |     ArtifactGenerator, create_sample_data, QuestionEvaluation, EvidenceReference\n   | ^^^^\n15 | )\n   |\n\ninvalid-syntax: Expected a statement\n  --> canonical_flow/analysis/test_artifact_generator.py:15:1\n   |\n13 | # # # from canonical_flow.analysis.artifact_generator import (  # Module not found  # Module not found  # Module not found\n14 |     ArtifactGenerator, create_sample_data, QuestionEvaluation, EvidenceReference\n15 | )\n   | ^\n   |\n\ninvalid-syntax: Expected a statement\n  --> canonical_flow/analysis/test_artifact_generator.py:15:2\n   |\n13 | # # # from canonical_flow.analysis.artifact_generator import (  # Module not found  # Module not found  # Module not found\n14 |     ArtifactGenerator, create_sample_data, QuestionEvaluation, EvidenceReference\n15 | )\n   |  ^\n   |\n\ninvalid-syntax: Unexpected indentation\n  --> canonical_flow/analysis/test_unit_artifact_generator.py:18:1\n   |\n17 | # # # from canonical_flow.analysis.artifact_generator import (  # Module not found  # Module not found  # Module not found\n18 |     ArtifactGenerator, QuestionEvaluation, DimensionSummary, PointSummary,\n   | ^^^^\n19 |     MesoClusterAnalysis, MacroAlignment, EvidenceReference, create_sample_data\n20 | )\n   |\n\ninvalid-syntax: Expected a statement\n  --> canonical_flow/analysis/test_unit_artifact_generator.py:20:1\n   |\n18 |     ArtifactGenerator, QuestionEvaluation, DimensionSummary, PointSummary,\n19 |     MesoClusterAnalysis, MacroAlignment, EvidenceReference, create_sample_data\n20 | )\n   | ^\n   |\n\ninvalid-syntax: Expected a statement\n  --> canonical_flow/analysis/test_unit_artifact_generator.py:20:2\n   |\n18 |     ArtifactGenerator, QuestionEvaluation, DimensionSummary, PointSummary,\n19 |     MesoClusterAnalysis, MacroAlignment, EvidenceReference, create_sample_data\n20 | )\n   |  ^\n   |\n\ninvalid-syntax: Unexpected indentation\n  --> canonical_flow/calibration/__init__.py:10:1\n   |\n 8 | # # # from .calibration_dashboard import CalibrationDashboard  # Module not found  # Module not found  # Module not found\n 9 | # # # from .calibration_artifacts import (  # Module not found  # Module not found  # Module not found\n10 |     CalibrationArtifact,\n   | ^^^^\n11 |     RetrievalCalibrationArtifact,\n12 |     ConfidenceCalibrationArtifact,\n   |\n\ninvalid-syntax: Expected a statement\n  --> canonical_flow/calibration/__init__.py:14:1\n   |\n12 |     ConfidenceCalibrationArtifact,\n13 |     AggregationCalibrationArtifact\n14 | )\n   | ^\n15 |\n16 | __all__ = [\n   |\n\ninvalid-syntax: Expected a statement\n  --> canonical_flow/calibration/__init__.py:14:2\n   |\n12 |     ConfidenceCalibrationArtifact,\n13 |     AggregationCalibrationArtifact\n14 | )\n   |  ^\n15 |\n16 | __all__ = [\n   |\n\nF821 Undefined name `dataclass`\n  --> canonical_flow/calibration/calibration_artifacts.py:14:2\n   |\n14 | @dataclass\n   |  ^^^^^^^^^\n15 | class CalibrationArtifact:\n16 |     \"\"\"Base class for all calibration artifacts with common fields.\"\"\"\n   |\n\nF821 Undefined name `Dict`\n  --> canonical_flow/calibration/calibration_artifacts.py:27:29\n   |\n25 |     timestamp: str\n26 |     stage_version: str\n27 |     calibration_parameters: Dict[str, Any]\n   |                             ^^^^\n28 |     \n29 |     # Optional quality gate thresholds\n   |\n\nF821 Undefined name `Any`\n  --> canonical_flow/calibration/calibration_artifacts.py:27:39\n   |\n25 |     timestamp: str\n26 |     stage_version: str\n27 |     calibration_parameters: Dict[str, Any]\n   |                                       ^^^\n28 |     \n29 |     # Optional quality gate thresholds\n   |\n\nF821 Undefined name `Optional`\n  --> canonical_flow/calibration/calibration_artifacts.py:30:25\n   |\n29 |     # Optional quality gate thresholds\n30 |     quality_thresholds: Optional[Dict[str, float]] = None\n   |                         ^^^^^^^^\n31 |     \n32 |     def to_dict(self) -> Dict[str, Any]:\n   |\n\nF821 Undefined name `Dict`\n  --> canonical_flow/calibration/calibration_artifacts.py:30:34\n   |\n29 |     # Optional quality gate thresholds\n30 |     quality_thresholds: Optional[Dict[str, float]] = None\n   |                                  ^^^^\n31 |     \n32 |     def to_dict(self) -> Dict[str, Any]:\n   |\n\nF821 Undefined name `Dict`\n  --> canonical_flow/calibration/calibration_artifacts.py:32:26\n   |\n30 |     quality_thresholds: Optional[Dict[str, float]] = None\n31 |     \n32 |     def to_dict(self) -> Dict[str, Any]:\n   |                          ^^^^\n33 |         \"\"\"Convert artifact to dictionary for JSON serialization.\"\"\"\n34 |         return asdict(self)\n   |\n\nF821 Undefined name `Any`\n  --> canonical_flow/calibration/calibration_artifacts.py:32:36\n   |\n30 |     quality_thresholds: Optional[Dict[str, float]] = None\n31 |     \n32 |     def to_dict(self) -> Dict[str, Any]:\n   |                                    ^^^\n33 |         \"\"\"Convert artifact to dictionary for JSON serialization.\"\"\"\n34 |         return asdict(self)\n   |\n\nF821 Undefined name `asdict`\n  --> canonical_flow/calibration/calibration_artifacts.py:34:16\n   |\n32 |     def to_dict(self) -> Dict[str, Any]:\n33 |         \"\"\"Convert artifact to dictionary for JSON serialization.\"\"\"\n34 |         return asdict(self)\n   |                ^^^^^^\n35 |     \n36 |     def save(self, filepath: Union[str, Path]) -> None:\n   |\n\nF821 Undefined name `Union`\n  --> canonical_flow/calibration/calibration_artifacts.py:36:30\n   |\n34 |         return asdict(self)\n35 |     \n36 |     def save(self, filepath: Union[str, Path]) -> None:\n   |                              ^^^^^\n37 |         \"\"\"Save artifact to JSON file.\"\"\"\n38 |         filepath_obj: Path = Path(filepath)\n   |\n\nF821 Undefined name `Path`\n  --> canonical_flow/calibration/calibration_artifacts.py:36:41\n   |\n34 |         return asdict(self)\n35 |     \n36 |     def save(self, filepath: Union[str, Path]) -> None:\n   |                                         ^^^^\n37 |         \"\"\"Save artifact to JSON file.\"\"\"\n38 |         filepath_obj: Path = Path(filepath)\n   |\n\nF821 Undefined name `Path`\n  --> canonical_flow/calibration/calibration_artifacts.py:38:23\n   |\n36 |     def save(self, filepath: Union[str, Path]) -> None:\n37 |         \"\"\"Save artifact to JSON file.\"\"\"\n38 |         filepath_obj: Path = Path(filepath)\n   |                       ^^^^\n39 |         filepath_obj.parent.mkdir(parents=True, exist_ok=True)\n   |\n\nF821 Undefined name `Path`\n  --> canonical_flow/calibration/calibration_artifacts.py:38:30\n   |\n36 |     def save(self, filepath: Union[str, Path]) -> None:\n37 |         \"\"\"Save artifact to JSON file.\"\"\"\n38 |         filepath_obj: Path = Path(filepath)\n   |                              ^^^^\n39 |         filepath_obj.parent.mkdir(parents=True, exist_ok=True)\n   |\n\nF821 Undefined name `Dict`\n  --> canonical_flow/calibration/calibration_artifacts.py:45:30\n   |\n44 |     @classmethod\n45 |     def from_dict(cls, data: Dict[str, Any]) -> 'CalibrationArtifact':\n   |                              ^^^^\n46 | # # #         \"\"\"Create artifact from dictionary.\"\"\"  # Module not found  # Module not found  # Module not found\n47 |         return cls(**data)\n   |\n\nF821 Undefined name `Any`\n  --> canonical_flow/calibration/calibration_artifacts.py:45:40\n   |\n44 |     @classmethod\n45 |     def from_dict(cls, data: Dict[str, Any]) -> 'CalibrationArtifact':\n   |                                        ^^^\n46 | # # #         \"\"\"Create artifact from dictionary.\"\"\"  # Module not found  # Module not found  # Module not found\n47 |         return cls(**data)\n   |\n\nF821 Undefined name `Union`\n  --> canonical_flow/calibration/calibration_artifacts.py:50:29\n   |\n49 |     @classmethod\n50 |     def load(cls, filepath: Union[str, Path]) -> 'CalibrationArtifact':\n   |                             ^^^^^\n51 | # # #         \"\"\"Load artifact from JSON file.\"\"\"  # Module not found  # Module not found  # Module not found\n52 |         with open(filepath, 'r') as f:\n   |\n\nF821 Undefined name `Path`\n  --> canonical_flow/calibration/calibration_artifacts.py:50:40\n   |\n49 |     @classmethod\n50 |     def load(cls, filepath: Union[str, Path]) -> 'CalibrationArtifact':\n   |                                        ^^^^\n51 | # # #         \"\"\"Load artifact from JSON file.\"\"\"  # Module not found  # Module not found  # Module not found\n52 |         with open(filepath, 'r') as f:\n   |\n\nF821 Undefined name `Dict`\n  --> canonical_flow/calibration/calibration_artifacts.py:53:19\n   |\n51 | # # #         \"\"\"Load artifact from JSON file.\"\"\"  # Module not found  # Module not found  # Module not found\n52 |         with open(filepath, 'r') as f:\n53 |             data: Dict[str, Any] = json.load(f)\n   |                   ^^^^\n54 |         return cls.from_dict(data)\n   |\n\nF821 Undefined name `Any`\n  --> canonical_flow/calibration/calibration_artifacts.py:53:29\n   |\n51 | # # #         \"\"\"Load artifact from JSON file.\"\"\"  # Module not found  # Module not found  # Module not found\n52 |         with open(filepath, 'r') as f:\n53 |             data: Dict[str, Any] = json.load(f)\n   |                             ^^^\n54 |         return cls.from_dict(data)\n   |\n\nF821 Undefined name `dataclass`\n  --> canonical_flow/calibration/calibration_artifacts.py:61:2\n   |\n61 | @dataclass\n   |  ^^^^^^^^^\n62 | class RetrievalCalibrationArtifact(CalibrationArtifact):\n63 |     \"\"\"Calibration artifact for retrieval stage with temperature stability metrics.\"\"\"\n   |\n\nF821 Undefined name `Optional`\n  --> canonical_flow/calibration/calibration_artifacts.py:74:21\n   |\n72 |     # Stage parameters\n73 |     temperature_parameter: float = 1.0\n74 |     fusion_weights: Optional[List[float]] = None\n   |                     ^^^^^^^^\n75 |     retrieval_k_values: Optional[List[int]] = None\n   |\n\nF821 Undefined name `List`\n  --> canonical_flow/calibration/calibration_artifacts.py:74:30\n   |\n72 |     # Stage parameters\n73 |     temperature_parameter: float = 1.0\n74 |     fusion_weights: Optional[List[float]] = None\n   |                              ^^^^\n75 |     retrieval_k_values: Optional[List[int]] = None\n   |\n\nF821 Undefined name `Optional`\n  --> canonical_flow/calibration/calibration_artifacts.py:75:25\n   |\n73 |     temperature_parameter: float = 1.0\n74 |     fusion_weights: Optional[List[float]] = None\n75 |     retrieval_k_values: Optional[List[int]] = None\n   |                         ^^^^^^^^\n76 |     \n77 |     # Additional retrieval metrics\n   |\n\nF821 Undefined name `List`\n  --> canonical_flow/calibration/calibration_artifacts.py:75:34\n   |\n73 |     temperature_parameter: float = 1.0\n74 |     fusion_weights: Optional[List[float]] = None\n75 |     retrieval_k_values: Optional[List[int]] = None\n   |                                  ^^^^\n76 |     \n77 |     # Additional retrieval metrics\n   |\n\nF821 Undefined name `dataclass`\n  --> canonical_flow/calibration/calibration_artifacts.py:89:2\n   |\n89 | @dataclass\n   |  ^^^^^^^^^\n90 | class ConfidenceCalibrationArtifact(CalibrationArtifact):\n91 |     \"\"\"Calibration artifact for confidence estimation stage with interval coverage.\"\"\"\n   |\n\nF821 Undefined name `Optional`\n   --> canonical_flow/calibration/calibration_artifacts.py:103:32\n    |\n101 |     adaptive_coverage_gap: float = 0.0\n102 |     prediction_set_size_avg: float = 0.0\n103 |     nonconformity_score_stats: Optional[Dict[str, float]] = None\n    |                                ^^^^^^^^\n104 |     \n105 |     # Stage parameters\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/calibration/calibration_artifacts.py:103:41\n    |\n101 |     adaptive_coverage_gap: float = 0.0\n102 |     prediction_set_size_avg: float = 0.0\n103 |     nonconformity_score_stats: Optional[Dict[str, float]] = None\n    |                                         ^^^^\n104 |     \n105 |     # Stage parameters\n    |\n\nF821 Undefined name `Optional`\n   --> canonical_flow/calibration/calibration_artifacts.py:113:26\n    |\n111 |     distribution_shift_detected: bool = False\n112 |     ks_test_p_value: float = 1.0\n113 |     bootstrap_intervals: Optional[Dict[str, List[float]]] = None\n    |                          ^^^^^^^^\n114 |     \n115 |     def __post_init__(self) -> None:\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/calibration/calibration_artifacts.py:113:35\n    |\n111 |     distribution_shift_detected: bool = False\n112 |     ks_test_p_value: float = 1.0\n113 |     bootstrap_intervals: Optional[Dict[str, List[float]]] = None\n    |                                   ^^^^\n114 |     \n115 |     def __post_init__(self) -> None:\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/calibration/calibration_artifacts.py:113:45\n    |\n111 |     distribution_shift_detected: bool = False\n112 |     ks_test_p_value: float = 1.0\n113 |     bootstrap_intervals: Optional[Dict[str, List[float]]] = None\n    |                                             ^^^^\n114 |     \n115 |     def __post_init__(self) -> None:\n    |\n\nF821 Undefined name `dataclass`\n   --> canonical_flow/calibration/calibration_artifacts.py:122:2\n    |\n122 | @dataclass\n    |  ^^^^^^^^^\n123 | class AggregationCalibrationArtifact(CalibrationArtifact):\n124 |     \"\"\"Calibration artifact for aggregation stage with convergence metrics.\"\"\"\n    |\n\ninvalid-syntax: Expected a statement\n  --> canonical_flow/calibration/calibration_dashboard.py:33:1\n   |\n31 |     ConfidenceCalibrationArtifact,\n32 |     AggregationCalibrationArtifact\n33 | )\n   | ^\n   |\n\ninvalid-syntax: Expected a statement\n  --> canonical_flow/calibration/calibration_dashboard.py:33:2\n   |\n31 |     ConfidenceCalibrationArtifact,\n32 |     AggregationCalibrationArtifact\n33 | )\n   |  ^\n   |\n\nF821 Undefined name `dataclass`\n  --> canonical_flow/calibration_dashboard.py:20:2\n   |\n20 | @dataclass\n   |  ^^^^^^^^^\n21 | class CalibrationReport:\n22 |     \"\"\"Schema for calibration reports\"\"\"\n   |\n\nF821 Undefined name `Optional`\n  --> canonical_flow/calibration_dashboard.py:31:20\n   |\n30 |     # Additional metadata\n31 |     error_message: Optional[str] = None\n   |                    ^^^^^^^^\n32 |     metadata: Optional[Dict[str, Any]] = None\n   |\n\nF821 Undefined name `Optional`\n  --> canonical_flow/calibration_dashboard.py:32:15\n   |\n30 |     # Additional metadata\n31 |     error_message: Optional[str] = None\n32 |     metadata: Optional[Dict[str, Any]] = None\n   |               ^^^^^^^^\n   |\n\nF821 Undefined name `Dict`\n  --> canonical_flow/calibration_dashboard.py:32:24\n   |\n30 |     # Additional metadata\n31 |     error_message: Optional[str] = None\n32 |     metadata: Optional[Dict[str, Any]] = None\n   |                        ^^^^\n   |\n\nF821 Undefined name `Any`\n  --> canonical_flow/calibration_dashboard.py:32:34\n   |\n30 |     # Additional metadata\n31 |     error_message: Optional[str] = None\n32 |     metadata: Optional[Dict[str, Any]] = None\n   |                                  ^^^\n   |\n\nF821 Undefined name `Path`\n  --> canonical_flow/calibration_dashboard.py:48:33\n   |\n46 |             output_directory: Directory to write calibration reports\n47 |         \"\"\"\n48 |         self.output_directory = Path(output_directory)\n   |                                 ^^^^\n49 |         self.output_directory.mkdir(parents=True, exist_ok=True)\n   |\n\nF821 Undefined name `Dict`\n  --> canonical_flow/calibration_dashboard.py:58:66\n   |\n56 |         }\n57 |         \n58 |     def generate_report(self, stage_name: str, calibration_data: Dict[str, Any]) -> CalibrationReport:\n   |                                                                  ^^^^\n59 |         \"\"\"\n60 |         Generate a calibration report for a given stage.\n   |\n\nF821 Undefined name `Any`\n  --> canonical_flow/calibration_dashboard.py:58:76\n   |\n56 |         }\n57 |         \n58 |     def generate_report(self, stage_name: str, calibration_data: Dict[str, Any]) -> CalibrationReport:\n   |                                                                            ^^^\n59 |         \"\"\"\n60 |         Generate a calibration report for a given stage.\n   |\n\nF821 Undefined name `datetime`\n  --> canonical_flow/calibration_dashboard.py:77:27\n   |\n76 |             report = CalibrationReport(\n77 |                 timestamp=datetime.now().isoformat(),\n   |                           ^^^^^^^^\n78 |                 stage_name=stage_name,\n79 |                 quality_score=quality_score,\n   |\n\nF821 Undefined name `datetime`\n  --> canonical_flow/calibration_dashboard.py:93:27\n   |\n91 |             # Return error report\n92 |             return CalibrationReport(\n93 |                 timestamp=datetime.now().isoformat(),\n   |                           ^^^^^^^^\n94 |                 stage_name=stage_name,\n95 |                 quality_score=0.0,\n   |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/calibration_dashboard.py:103:44\n    |\n101 |             )\n102 |     \n103 |     def _extract_quality_score(self, data: Dict[str, Any]) -> float:\n    |                                            ^^^^\n104 | # # #         \"\"\"Extract quality score from calibration data with fallbacks\"\"\"  # Module not found  # Module not found  # Module not \u2026\n105 |         # Try multiple possible field names\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/calibration_dashboard.py:103:54\n    |\n101 |             )\n102 |     \n103 |     def _extract_quality_score(self, data: Dict[str, Any]) -> float:\n    |                                                      ^^^\n104 | # # #         \"\"\"Extract quality score from calibration data with fallbacks\"\"\"  # Module not found  # Module not found  # Module not \u2026\n105 |         # Try multiple possible field names\n    |\n\nF541 [*] f-string without any placeholders\n   --> canonical_flow/calibration_dashboard.py:124:24\n    |\n123 |         # Default fallback\n124 |         logger.warning(f\"No valid quality score found in calibration data, using 0.0\")\n    |                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n125 |         return 0.0\n    |\nhelp: Remove extraneous `f` prefix\n\nF821 Undefined name `Dict`\n   --> canonical_flow/calibration_dashboard.py:127:51\n    |\n125 |         return 0.0\n126 |     \n127 |     def _extract_calibration_decision(self, data: Dict[str, Any]) -> str:\n    |                                                   ^^^^\n128 | # # #         \"\"\"Extract calibration decision from data\"\"\"  # Module not found  # Module not found  # Module not found\n129 |         decision_fields = ['calibration_decision', 'decision', 'action', 'recommendation']\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/calibration_dashboard.py:127:61\n    |\n125 |         return 0.0\n126 |     \n127 |     def _extract_calibration_decision(self, data: Dict[str, Any]) -> str:\n    |                                                             ^^^\n128 | # # #         \"\"\"Extract calibration decision from data\"\"\"  # Module not found  # Module not found  # Module not found\n129 |         decision_fields = ['calibration_decision', 'decision', 'action', 'recommendation']\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/calibration_dashboard.py:152:50\n    |\n150 |         return 'unknown'\n151 |     \n152 |     def _extract_coverage_percentage(self, data: Dict[str, Any]) -> float:\n    |                                                  ^^^^\n153 | # # #         \"\"\"Extract coverage percentage from data\"\"\"  # Module not found  # Module not found  # Module not found\n154 |         coverage_fields = ['coverage_percentage', 'coverage', 'percent_covered', 'completeness']\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/calibration_dashboard.py:152:60\n    |\n150 |         return 'unknown'\n151 |     \n152 |     def _extract_coverage_percentage(self, data: Dict[str, Any]) -> float:\n    |                                                            ^^^\n153 | # # #         \"\"\"Extract coverage percentage from data\"\"\"  # Module not found  # Module not found  # Module not found\n154 |         coverage_fields = ['coverage_percentage', 'coverage', 'percent_covered', 'completeness']\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/calibration_dashboard.py:174:50\n    |\n172 |         return 0.0\n173 |     \n174 |     def _extract_quality_gate_status(self, data: Dict[str, Any]) -> bool:\n    |                                                  ^^^^\n175 |         \"\"\"Extract quality gate pass/fail status\"\"\"\n176 |         gate_fields = ['quality_gate_passed', 'passed', 'gate_passed', 'validation_passed']\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/calibration_dashboard.py:174:60\n    |\n172 |         return 0.0\n173 |     \n174 |     def _extract_quality_gate_status(self, data: Dict[str, Any]) -> bool:\n    |                                                            ^^^\n175 |         \"\"\"Extract quality gate pass/fail status\"\"\"\n176 |         gate_fields = ['quality_gate_passed', 'passed', 'gate_passed', 'validation_passed']\n    |\n\nF821 Undefined name `Path`\n   --> canonical_flow/calibration_dashboard.py:188:75\n    |\n186 |         return quality_score >= threshold\n187 |     \n188 |     def write_report(self, stage_name: str, report: CalibrationReport) -> Path:\n    |                                                                           ^^^^\n189 |         \"\"\"\n190 |         Write calibration report to disk in JSON format.\n    |\n\nF821 Undefined name `asdict`\n   --> canonical_flow/calibration_dashboard.py:205:27\n    |\n203 |         try:\n204 |             # Convert to dict and ensure UTF-8 encoding\n205 |             report_data = asdict(report)\n    |                           ^^^^^^\n206 |             \n207 |             with open(file_path, 'w', encoding='utf-8') as f:\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/calibration_dashboard.py:217:76\n    |\n215 |             raise\n216 |     \n217 |     def generate_and_write_report(self, stage_name: str, calibration_data: Dict[str, Any]) -> Path:\n    |                                                                            ^^^^\n218 |         \"\"\"\n219 |         Generate and write a calibration report in one step.\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/calibration_dashboard.py:217:86\n    |\n215 |             raise\n216 |     \n217 |     def generate_and_write_report(self, stage_name: str, calibration_data: Dict[str, Any]) -> Path:\n    |                                                                                      ^^^\n218 |         \"\"\"\n219 |         Generate and write a calibration report in one step.\n    |\n\nF821 Undefined name `Path`\n   --> canonical_flow/calibration_dashboard.py:217:95\n    |\n215 |             raise\n216 |     \n217 |     def generate_and_write_report(self, stage_name: str, calibration_data: Dict[str, Any]) -> Path:\n    |                                                                                               ^^^^\n218 |         \"\"\"\n219 |         Generate and write a calibration report in one step.\n    |\n\nF821 Undefined name `Optional`\n   --> canonical_flow/calibration_dashboard.py:231:47\n    |\n229 |         return self.write_report(stage_name, report)\n230 |     \n231 |     def read_report(self, stage_name: str) -> Optional[CalibrationReport]:\n    |                                               ^^^^^^^^\n232 |         \"\"\"\n233 | # # #         Read an existing calibration report from disk.  # Module not found  # Module not found  # Module not found\n    |\n\nF841 [*] Local variable `e` is assigned to but never used\n   --> canonical_flow/calibration_dashboard.py:253:29\n    |\n251 |             return CalibrationReport(**report_data)\n252 |             \n253 |         except Exception as e:\n    |                             ^\n254 | # # #             logger.error(f\"Failed to read calibration report from {file_path}: {e}\")  # Module not found  # Module not found  #\u2026\n255 |             return None\n    |\nhelp: Remove assignment to unused variable `e`\n\nF821 Undefined name `List`\n   --> canonical_flow/calibration_dashboard.py:257:31\n    |\n255 |             return None\n256 |     \n257 |     def list_reports(self) -> List[str]:\n    |                               ^^^^\n258 |         \"\"\"\n259 |         List all available calibration reports.\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/calibration_dashboard.py:273:30\n    |\n271 |         return reports\n272 |     \n273 |     def get_summary(self) -> Dict[str, Any]:\n    |                              ^^^^\n274 |         \"\"\"\n275 |         Get a summary of all calibration reports.\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/calibration_dashboard.py:273:40\n    |\n271 |         return reports\n272 |     \n273 |     def get_summary(self) -> Dict[str, Any]:\n    |                                        ^^^\n274 |         \"\"\"\n275 |         Get a summary of all calibration reports.\n    |\n\nF821 Undefined name `datetime`\n   --> canonical_flow/calibration_dashboard.py:281:26\n    |\n279 |         \"\"\"\n280 |         summary = {\n281 |             \"timestamp\": datetime.now().isoformat(),\n    |                          ^^^^^^^^\n282 |             \"total_reports\": 0,\n283 |             \"stages\": {},\n    |\n\nF821 Undefined name `dataclass`\n  --> canonical_flow/evaluation/audit_logger.py:21:2\n   |\n21 | @dataclass\n   |  ^^^^^^^^^\n22 | class ComponentTrace:\n23 |     \"\"\"Trace information for a single component execution.\"\"\"\n   |\n\nF821 Undefined name `Optional`\n  --> canonical_flow/evaluation/audit_logger.py:26:21\n   |\n24 |     component_name: str\n25 |     entry_timestamp: float\n26 |     exit_timestamp: Optional[float] = None\n   |                     ^^^^^^^^\n27 |     duration_ms: Optional[float] = None\n28 |     status: str = \"running\"\n   |\n\nF821 Undefined name `Optional`\n  --> canonical_flow/evaluation/audit_logger.py:27:18\n   |\n25 |     entry_timestamp: float\n26 |     exit_timestamp: Optional[float] = None\n27 |     duration_ms: Optional[float] = None\n   |                  ^^^^^^^^\n28 |     status: str = \"running\"\n29 |     inputs: Dict[str, Any] = field(default_factory=dict)\n   |\n\nF821 Undefined name `Dict`\n  --> canonical_flow/evaluation/audit_logger.py:29:13\n   |\n27 |     duration_ms: Optional[float] = None\n28 |     status: str = \"running\"\n29 |     inputs: Dict[str, Any] = field(default_factory=dict)\n   |             ^^^^\n30 |     outputs: Dict[str, Any] = field(default_factory=dict)\n31 |     errors: List[str] = field(default_factory=list)\n   |\n\nF821 Undefined name `Any`\n  --> canonical_flow/evaluation/audit_logger.py:29:23\n   |\n27 |     duration_ms: Optional[float] = None\n28 |     status: str = \"running\"\n29 |     inputs: Dict[str, Any] = field(default_factory=dict)\n   |                       ^^^\n30 |     outputs: Dict[str, Any] = field(default_factory=dict)\n31 |     errors: List[str] = field(default_factory=list)\n   |\n\nF821 Undefined name `field`\n  --> canonical_flow/evaluation/audit_logger.py:29:30\n   |\n27 |     duration_ms: Optional[float] = None\n28 |     status: str = \"running\"\n29 |     inputs: Dict[str, Any] = field(default_factory=dict)\n   |                              ^^^^^\n30 |     outputs: Dict[str, Any] = field(default_factory=dict)\n31 |     errors: List[str] = field(default_factory=list)\n   |\n\nF821 Undefined name `Dict`\n  --> canonical_flow/evaluation/audit_logger.py:30:14\n   |\n28 |     status: str = \"running\"\n29 |     inputs: Dict[str, Any] = field(default_factory=dict)\n30 |     outputs: Dict[str, Any] = field(default_factory=dict)\n   |              ^^^^\n31 |     errors: List[str] = field(default_factory=list)\n32 |     metadata: Dict[str, Any] = field(default_factory=dict)\n   |\n\nF821 Undefined name `Any`\n  --> canonical_flow/evaluation/audit_logger.py:30:24\n   |\n28 |     status: str = \"running\"\n29 |     inputs: Dict[str, Any] = field(default_factory=dict)\n30 |     outputs: Dict[str, Any] = field(default_factory=dict)\n   |                        ^^^\n31 |     errors: List[str] = field(default_factory=list)\n32 |     metadata: Dict[str, Any] = field(default_factory=dict)\n   |\n\nF821 Undefined name `field`\n  --> canonical_flow/evaluation/audit_logger.py:30:31\n   |\n28 |     status: str = \"running\"\n29 |     inputs: Dict[str, Any] = field(default_factory=dict)\n30 |     outputs: Dict[str, Any] = field(default_factory=dict)\n   |                               ^^^^^\n31 |     errors: List[str] = field(default_factory=list)\n32 |     metadata: Dict[str, Any] = field(default_factory=dict)\n   |\n\nF821 Undefined name `List`\n  --> canonical_flow/evaluation/audit_logger.py:31:13\n   |\n29 |     inputs: Dict[str, Any] = field(default_factory=dict)\n30 |     outputs: Dict[str, Any] = field(default_factory=dict)\n31 |     errors: List[str] = field(default_factory=list)\n   |             ^^^^\n32 |     metadata: Dict[str, Any] = field(default_factory=dict)\n   |\n\nF821 Undefined name `field`\n  --> canonical_flow/evaluation/audit_logger.py:31:25\n   |\n29 |     inputs: Dict[str, Any] = field(default_factory=dict)\n30 |     outputs: Dict[str, Any] = field(default_factory=dict)\n31 |     errors: List[str] = field(default_factory=list)\n   |                         ^^^^^\n32 |     metadata: Dict[str, Any] = field(default_factory=dict)\n   |\n\nF821 Undefined name `Dict`\n  --> canonical_flow/evaluation/audit_logger.py:32:15\n   |\n30 |     outputs: Dict[str, Any] = field(default_factory=dict)\n31 |     errors: List[str] = field(default_factory=list)\n32 |     metadata: Dict[str, Any] = field(default_factory=dict)\n   |               ^^^^\n33 |\n34 |     def complete(self, status: str = \"completed\", outputs: Optional[Dict[str, Any]] = None, \n   |\n\nF821 Undefined name `Any`\n  --> canonical_flow/evaluation/audit_logger.py:32:25\n   |\n30 |     outputs: Dict[str, Any] = field(default_factory=dict)\n31 |     errors: List[str] = field(default_factory=list)\n32 |     metadata: Dict[str, Any] = field(default_factory=dict)\n   |                         ^^^\n33 |\n34 |     def complete(self, status: str = \"completed\", outputs: Optional[Dict[str, Any]] = None, \n   |\n\nF821 Undefined name `field`\n  --> canonical_flow/evaluation/audit_logger.py:32:32\n   |\n30 |     outputs: Dict[str, Any] = field(default_factory=dict)\n31 |     errors: List[str] = field(default_factory=list)\n32 |     metadata: Dict[str, Any] = field(default_factory=dict)\n   |                                ^^^^^\n33 |\n34 |     def complete(self, status: str = \"completed\", outputs: Optional[Dict[str, Any]] = None, \n   |\n\nF821 Undefined name `Optional`\n  --> canonical_flow/evaluation/audit_logger.py:34:60\n   |\n32 |     metadata: Dict[str, Any] = field(default_factory=dict)\n33 |\n34 |     def complete(self, status: str = \"completed\", outputs: Optional[Dict[str, Any]] = None, \n   |                                                            ^^^^^^^^\n35 |                  errors: Optional[List[str]] = None):\n36 |         \"\"\"Mark component execution as completed.\"\"\"\n   |\n\nF821 Undefined name `Dict`\n  --> canonical_flow/evaluation/audit_logger.py:34:69\n   |\n32 |     metadata: Dict[str, Any] = field(default_factory=dict)\n33 |\n34 |     def complete(self, status: str = \"completed\", outputs: Optional[Dict[str, Any]] = None, \n   |                                                                     ^^^^\n35 |                  errors: Optional[List[str]] = None):\n36 |         \"\"\"Mark component execution as completed.\"\"\"\n   |\n\nF821 Undefined name `Any`\n  --> canonical_flow/evaluation/audit_logger.py:34:79\n   |\n32 |     metadata: Dict[str, Any] = field(default_factory=dict)\n33 |\n34 |     def complete(self, status: str = \"completed\", outputs: Optional[Dict[str, Any]] = None, \n   |                                                                               ^^^\n35 |                  errors: Optional[List[str]] = None):\n36 |         \"\"\"Mark component execution as completed.\"\"\"\n   |\n\nF821 Undefined name `Optional`\n  --> canonical_flow/evaluation/audit_logger.py:35:26\n   |\n34 |     def complete(self, status: str = \"completed\", outputs: Optional[Dict[str, Any]] = None, \n35 |                  errors: Optional[List[str]] = None):\n   |                          ^^^^^^^^\n36 |         \"\"\"Mark component execution as completed.\"\"\"\n37 |         self.exit_timestamp = time.time()\n   |\n\nF821 Undefined name `List`\n  --> canonical_flow/evaluation/audit_logger.py:35:35\n   |\n34 |     def complete(self, status: str = \"completed\", outputs: Optional[Dict[str, Any]] = None, \n35 |                  errors: Optional[List[str]] = None):\n   |                                   ^^^^\n36 |         \"\"\"Mark component execution as completed.\"\"\"\n37 |         self.exit_timestamp = time.time()\n   |\n\nF821 Undefined name `dataclass`\n  --> canonical_flow/evaluation/audit_logger.py:46:2\n   |\n46 | @dataclass\n   |  ^^^^^^^^^\n47 | class DecalogoPointTrace:\n48 |     \"\"\"Comprehensive trace for a single Dec\u00e1logo point evaluation.\"\"\"\n   |\n\nF821 Undefined name `Optional`\n  --> canonical_flow/evaluation/audit_logger.py:53:20\n   |\n51 |     evaluation_id: str\n52 |     start_timestamp: float\n53 |     end_timestamp: Optional[float] = None\n   |                    ^^^^^^^^\n54 |     total_duration_ms: Optional[float] = None\n55 |     components: Dict[str, ComponentTrace] = field(default_factory=dict)\n   |\n\nF821 Undefined name `Optional`\n  --> canonical_flow/evaluation/audit_logger.py:54:24\n   |\n52 |     start_timestamp: float\n53 |     end_timestamp: Optional[float] = None\n54 |     total_duration_ms: Optional[float] = None\n   |                        ^^^^^^^^\n55 |     components: Dict[str, ComponentTrace] = field(default_factory=dict)\n56 |     question_processing: Dict[str, float] = field(default_factory=dict)\n   |\n\nF821 Undefined name `Dict`\n  --> canonical_flow/evaluation/audit_logger.py:55:17\n   |\n53 |     end_timestamp: Optional[float] = None\n54 |     total_duration_ms: Optional[float] = None\n55 |     components: Dict[str, ComponentTrace] = field(default_factory=dict)\n   |                 ^^^^\n56 |     question_processing: Dict[str, float] = field(default_factory=dict)\n57 |     evidence_validation: Dict[str, float] = field(default_factory=dict)\n   |\n\nF821 Undefined name `field`\n  --> canonical_flow/evaluation/audit_logger.py:55:45\n   |\n53 |     end_timestamp: Optional[float] = None\n54 |     total_duration_ms: Optional[float] = None\n55 |     components: Dict[str, ComponentTrace] = field(default_factory=dict)\n   |                                             ^^^^^\n56 |     question_processing: Dict[str, float] = field(default_factory=dict)\n57 |     evidence_validation: Dict[str, float] = field(default_factory=dict)\n   |\n\nF821 Undefined name `Dict`\n  --> canonical_flow/evaluation/audit_logger.py:56:26\n   |\n54 |     total_duration_ms: Optional[float] = None\n55 |     components: Dict[str, ComponentTrace] = field(default_factory=dict)\n56 |     question_processing: Dict[str, float] = field(default_factory=dict)\n   |                          ^^^^\n57 |     evidence_validation: Dict[str, float] = field(default_factory=dict)\n58 |     scoring_computation: Dict[str, Any] = field(default_factory=dict)\n   |\n\nF821 Undefined name `field`\n  --> canonical_flow/evaluation/audit_logger.py:56:45\n   |\n54 |     total_duration_ms: Optional[float] = None\n55 |     components: Dict[str, ComponentTrace] = field(default_factory=dict)\n56 |     question_processing: Dict[str, float] = field(default_factory=dict)\n   |                                             ^^^^^\n57 |     evidence_validation: Dict[str, float] = field(default_factory=dict)\n58 |     scoring_computation: Dict[str, Any] = field(default_factory=dict)\n   |\n\nF821 Undefined name `Dict`\n  --> canonical_flow/evaluation/audit_logger.py:57:26\n   |\n55 |     components: Dict[str, ComponentTrace] = field(default_factory=dict)\n56 |     question_processing: Dict[str, float] = field(default_factory=dict)\n57 |     evidence_validation: Dict[str, float] = field(default_factory=dict)\n   |                          ^^^^\n58 |     scoring_computation: Dict[str, Any] = field(default_factory=dict)\n59 |     status: str = \"running\"\n   |\n\nF821 Undefined name `field`\n  --> canonical_flow/evaluation/audit_logger.py:57:45\n   |\n55 |     components: Dict[str, ComponentTrace] = field(default_factory=dict)\n56 |     question_processing: Dict[str, float] = field(default_factory=dict)\n57 |     evidence_validation: Dict[str, float] = field(default_factory=dict)\n   |                                             ^^^^^\n58 |     scoring_computation: Dict[str, Any] = field(default_factory=dict)\n59 |     status: str = \"running\"\n   |\n\nF821 Undefined name `Dict`\n  --> canonical_flow/evaluation/audit_logger.py:58:26\n   |\n56 |     question_processing: Dict[str, float] = field(default_factory=dict)\n57 |     evidence_validation: Dict[str, float] = field(default_factory=dict)\n58 |     scoring_computation: Dict[str, Any] = field(default_factory=dict)\n   |                          ^^^^\n59 |     status: str = \"running\"\n60 |     errors: List[str] = field(default_factory=list)\n   |\n\nF821 Undefined name `Any`\n  --> canonical_flow/evaluation/audit_logger.py:58:36\n   |\n56 |     question_processing: Dict[str, float] = field(default_factory=dict)\n57 |     evidence_validation: Dict[str, float] = field(default_factory=dict)\n58 |     scoring_computation: Dict[str, Any] = field(default_factory=dict)\n   |                                    ^^^\n59 |     status: str = \"running\"\n60 |     errors: List[str] = field(default_factory=list)\n   |\n\nF821 Undefined name `field`\n  --> canonical_flow/evaluation/audit_logger.py:58:43\n   |\n56 |     question_processing: Dict[str, float] = field(default_factory=dict)\n57 |     evidence_validation: Dict[str, float] = field(default_factory=dict)\n58 |     scoring_computation: Dict[str, Any] = field(default_factory=dict)\n   |                                           ^^^^^\n59 |     status: str = \"running\"\n60 |     errors: List[str] = field(default_factory=list)\n   |\n\nF821 Undefined name `List`\n  --> canonical_flow/evaluation/audit_logger.py:60:13\n   |\n58 |     scoring_computation: Dict[str, Any] = field(default_factory=dict)\n59 |     status: str = \"running\"\n60 |     errors: List[str] = field(default_factory=list)\n   |             ^^^^\n61 |\n62 |     def complete(self):\n   |\n\nF821 Undefined name `field`\n  --> canonical_flow/evaluation/audit_logger.py:60:25\n   |\n58 |     scoring_computation: Dict[str, Any] = field(default_factory=dict)\n59 |     status: str = \"running\"\n60 |     errors: List[str] = field(default_factory=list)\n   |                         ^^^^^\n61 |\n62 |     def complete(self):\n   |\n\nF821 Undefined name `dataclass`\n  --> canonical_flow/evaluation/audit_logger.py:69:2\n   |\n69 | @dataclass\n   |  ^^^^^^^^^\n70 | class CoverageValidationResult:\n71 |     \"\"\"Coverage validation results for question-dimension mapping.\"\"\"\n   |\n\nF821 Undefined name `List`\n  --> canonical_flow/evaluation/audit_logger.py:75:27\n   |\n73 |     expected_question_count: int\n74 |     actual_question_count: int\n75 |     missing_question_ids: List[str] = field(default_factory=list)\n   |                           ^^^^\n76 |     extra_question_ids: List[str] = field(default_factory=list)\n77 |     coverage_percentage: float = 0.0\n   |\n\nF821 Undefined name `field`\n  --> canonical_flow/evaluation/audit_logger.py:75:39\n   |\n73 |     expected_question_count: int\n74 |     actual_question_count: int\n75 |     missing_question_ids: List[str] = field(default_factory=list)\n   |                                       ^^^^^\n76 |     extra_question_ids: List[str] = field(default_factory=list)\n77 |     coverage_percentage: float = 0.0\n   |\n\nF821 Undefined name `List`\n  --> canonical_flow/evaluation/audit_logger.py:76:25\n   |\n74 |     actual_question_count: int\n75 |     missing_question_ids: List[str] = field(default_factory=list)\n76 |     extra_question_ids: List[str] = field(default_factory=list)\n   |                         ^^^^\n77 |     coverage_percentage: float = 0.0\n78 |     incomplete_evidence_count: int = 0\n   |\n\nF821 Undefined name `field`\n  --> canonical_flow/evaluation/audit_logger.py:76:37\n   |\n74 |     actual_question_count: int\n75 |     missing_question_ids: List[str] = field(default_factory=list)\n76 |     extra_question_ids: List[str] = field(default_factory=list)\n   |                                     ^^^^^\n77 |     coverage_percentage: float = 0.0\n78 |     incomplete_evidence_count: int = 0\n   |\n\nF821 Undefined name `List`\n  --> canonical_flow/evaluation/audit_logger.py:79:30\n   |\n77 |     coverage_percentage: float = 0.0\n78 |     incomplete_evidence_count: int = 0\n79 |     missing_page_references: List[Dict[str, Any]] = field(default_factory=list)\n   |                              ^^^^\n80 |     validation_status: str = \"unknown\"\n   |\n\nF821 Undefined name `Dict`\n  --> canonical_flow/evaluation/audit_logger.py:79:35\n   |\n77 |     coverage_percentage: float = 0.0\n78 |     incomplete_evidence_count: int = 0\n79 |     missing_page_references: List[Dict[str, Any]] = field(default_factory=list)\n   |                                   ^^^^\n80 |     validation_status: str = \"unknown\"\n   |\n\nF821 Undefined name `Any`\n  --> canonical_flow/evaluation/audit_logger.py:79:45\n   |\n77 |     coverage_percentage: float = 0.0\n78 |     incomplete_evidence_count: int = 0\n79 |     missing_page_references: List[Dict[str, Any]] = field(default_factory=list)\n   |                                             ^^^\n80 |     validation_status: str = \"unknown\"\n   |\n\nF821 Undefined name `field`\n  --> canonical_flow/evaluation/audit_logger.py:79:53\n   |\n77 |     coverage_percentage: float = 0.0\n78 |     incomplete_evidence_count: int = 0\n79 |     missing_page_references: List[Dict[str, Any]] = field(default_factory=list)\n   |                                                     ^^^^^\n80 |     validation_status: str = \"unknown\"\n   |\n\nF821 Undefined name `dataclass`\n  --> canonical_flow/evaluation/audit_logger.py:97:2\n   |\n97 | @dataclass\n   |  ^^^^^^^^^\n98 | class ScoringConsistencyResult:\n99 |     \"\"\"Scoring consistency validation results.\"\"\"\n   |\n\nF821 Undefined name `List`\n   --> canonical_flow/evaluation/audit_logger.py:101:17\n    |\n 99 |     \"\"\"Scoring consistency validation results.\"\"\"\n100 |     dimension: str\n101 |     raw_scores: List[float] = field(default_factory=list)\n    |                 ^^^^\n102 |     weighted_scores: List[float] = field(default_factory=list)\n103 |     final_classification: str = \"\"\n    |\n\nF821 Undefined name `field`\n   --> canonical_flow/evaluation/audit_logger.py:101:31\n    |\n 99 |     \"\"\"Scoring consistency validation results.\"\"\"\n100 |     dimension: str\n101 |     raw_scores: List[float] = field(default_factory=list)\n    |                               ^^^^^\n102 |     weighted_scores: List[float] = field(default_factory=list)\n103 |     final_classification: str = \"\"\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/evaluation/audit_logger.py:102:22\n    |\n100 |     dimension: str\n101 |     raw_scores: List[float] = field(default_factory=list)\n102 |     weighted_scores: List[float] = field(default_factory=list)\n    |                      ^^^^\n103 |     final_classification: str = \"\"\n104 |     expected_classification: str = \"\"\n    |\n\nF821 Undefined name `field`\n   --> canonical_flow/evaluation/audit_logger.py:102:36\n    |\n100 |     dimension: str\n101 |     raw_scores: List[float] = field(default_factory=list)\n102 |     weighted_scores: List[float] = field(default_factory=list)\n    |                                    ^^^^^\n103 |     final_classification: str = \"\"\n104 |     expected_classification: str = \"\"\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/evaluation/audit_logger.py:106:22\n    |\n104 |     expected_classification: str = \"\"\n105 |     score_variance: float = 0.0\n106 |     inconsistencies: List[Dict[str, Any]] = field(default_factory=list)\n    |                      ^^^^\n107 |     diagnostic_info: Dict[str, Any] = field(default_factory=dict)\n108 |     consistency_status: str = \"unknown\"\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/evaluation/audit_logger.py:106:27\n    |\n104 |     expected_classification: str = \"\"\n105 |     score_variance: float = 0.0\n106 |     inconsistencies: List[Dict[str, Any]] = field(default_factory=list)\n    |                           ^^^^\n107 |     diagnostic_info: Dict[str, Any] = field(default_factory=dict)\n108 |     consistency_status: str = \"unknown\"\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/evaluation/audit_logger.py:106:37\n    |\n104 |     expected_classification: str = \"\"\n105 |     score_variance: float = 0.0\n106 |     inconsistencies: List[Dict[str, Any]] = field(default_factory=list)\n    |                                     ^^^\n107 |     diagnostic_info: Dict[str, Any] = field(default_factory=dict)\n108 |     consistency_status: str = \"unknown\"\n    |\n\nF821 Undefined name `field`\n   --> canonical_flow/evaluation/audit_logger.py:106:45\n    |\n104 |     expected_classification: str = \"\"\n105 |     score_variance: float = 0.0\n106 |     inconsistencies: List[Dict[str, Any]] = field(default_factory=list)\n    |                                             ^^^^^\n107 |     diagnostic_info: Dict[str, Any] = field(default_factory=dict)\n108 |     consistency_status: str = \"unknown\"\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/evaluation/audit_logger.py:107:22\n    |\n105 |     score_variance: float = 0.0\n106 |     inconsistencies: List[Dict[str, Any]] = field(default_factory=list)\n107 |     diagnostic_info: Dict[str, Any] = field(default_factory=dict)\n    |                      ^^^^\n108 |     consistency_status: str = \"unknown\"\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/evaluation/audit_logger.py:107:32\n    |\n105 |     score_variance: float = 0.0\n106 |     inconsistencies: List[Dict[str, Any]] = field(default_factory=list)\n107 |     diagnostic_info: Dict[str, Any] = field(default_factory=dict)\n    |                                ^^^\n108 |     consistency_status: str = \"unknown\"\n    |\n\nF821 Undefined name `field`\n   --> canonical_flow/evaluation/audit_logger.py:107:39\n    |\n105 |     score_variance: float = 0.0\n106 |     inconsistencies: List[Dict[str, Any]] = field(default_factory=list)\n107 |     diagnostic_info: Dict[str, Any] = field(default_factory=dict)\n    |                                       ^^^^^\n108 |     consistency_status: str = \"unknown\"\n    |\n\nF821 Undefined name `Optional`\n   --> canonical_flow/evaluation/audit_logger.py:135:36\n    |\n133 |     \"\"\"\n134 |     \n135 |     def __init__(self, audit_file: Optional[str] = None):\n    |                                    ^^^^^^^^\n136 |         \"\"\"Initialize audit logger with optional custom audit file path.\"\"\"\n137 |         self.audit_file = audit_file or \"canonical_flow/evaluation/_audit.json\"\n    |\n\nF821 Undefined name `Path`\n   --> canonical_flow/evaluation/audit_logger.py:138:9\n    |\n136 |         \"\"\"Initialize audit logger with optional custom audit file path.\"\"\"\n137 |         self.audit_file = audit_file or \"canonical_flow/evaluation/_audit.json\"\n138 |         Path(self.audit_file).parent.mkdir(parents=True, exist_ok=True)\n    |         ^^^^\n139 |         \n140 |         # Core audit data structures\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/evaluation/audit_logger.py:142:31\n    |\n140 |         # Core audit data structures\n141 |         self.session_id = self._generate_session_id()\n142 |         self.decalogo_traces: Dict[str, DecalogoPointTrace] = {}\n    |                               ^^^^\n143 |         self.coverage_results: Dict[str, CoverageValidationResult] = {}\n144 |         self.scoring_results: Dict[str, ScoringConsistencyResult] = {}\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/evaluation/audit_logger.py:143:32\n    |\n141 |         self.session_id = self._generate_session_id()\n142 |         self.decalogo_traces: Dict[str, DecalogoPointTrace] = {}\n143 |         self.coverage_results: Dict[str, CoverageValidationResult] = {}\n    |                                ^^^^\n144 |         self.scoring_results: Dict[str, ScoringConsistencyResult] = {}\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/evaluation/audit_logger.py:144:31\n    |\n142 |         self.decalogo_traces: Dict[str, DecalogoPointTrace] = {}\n143 |         self.coverage_results: Dict[str, CoverageValidationResult] = {}\n144 |         self.scoring_results: Dict[str, ScoringConsistencyResult] = {}\n    |                               ^^^^\n145 |         \n146 |         # Metrics tracking\n    |\n\nF821 Undefined name `Optional`\n   --> canonical_flow/evaluation/audit_logger.py:157:32\n    |\n156 |         # Internal state\n157 |         self._active_point_id: Optional[str] = None\n    |                                ^^^^^^^^\n158 |         self._start_time = time.time()\n    |\n\nF821 Undefined name `datetime`\n   --> canonical_flow/evaluation/audit_logger.py:162:21\n    |\n160 |     def _generate_session_id(self) -> str:\n161 |         \"\"\"Generate unique session identifier.\"\"\"\n162 |         timestamp = datetime.now(timezone.utc).isoformat()\n    |                     ^^^^^^^^\n163 |         hash_input = f\"{timestamp}_{id(self)}\".encode()\n164 |         return hashlib.md5(hash_input).hexdigest()[:16]\n    |\n\nF821 Undefined name `timezone`\n   --> canonical_flow/evaluation/audit_logger.py:162:34\n    |\n160 |     def _generate_session_id(self) -> str:\n161 |         \"\"\"Generate unique session identifier.\"\"\"\n162 |         timestamp = datetime.now(timezone.utc).isoformat()\n    |                                  ^^^^^^^^\n163 |         hash_input = f\"{timestamp}_{id(self)}\".encode()\n164 |         return hashlib.md5(hash_input).hexdigest()[:16]\n    |\n\nF821 Undefined name `Optional`\n   --> canonical_flow/evaluation/audit_logger.py:193:64\n    |\n191 |         return evaluation_id\n192 |\n193 |     def log_component_entry(self, component_name: str, inputs: Optional[Dict[str, Any]] = None,\n    |                                                                ^^^^^^^^\n194 |                            evaluation_id: Optional[str] = None) -> str:\n195 |         \"\"\"\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/evaluation/audit_logger.py:193:73\n    |\n191 |         return evaluation_id\n192 |\n193 |     def log_component_entry(self, component_name: str, inputs: Optional[Dict[str, Any]] = None,\n    |                                                                         ^^^^\n194 |                            evaluation_id: Optional[str] = None) -> str:\n195 |         \"\"\"\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/evaluation/audit_logger.py:193:83\n    |\n191 |         return evaluation_id\n192 |\n193 |     def log_component_entry(self, component_name: str, inputs: Optional[Dict[str, Any]] = None,\n    |                                                                                   ^^^\n194 |                            evaluation_id: Optional[str] = None) -> str:\n195 |         \"\"\"\n    |\n\nF821 Undefined name `Optional`\n   --> canonical_flow/evaluation/audit_logger.py:194:43\n    |\n193 |     def log_component_entry(self, component_name: str, inputs: Optional[Dict[str, Any]] = None,\n194 |                            evaluation_id: Optional[str] = None) -> str:\n    |                                           ^^^^^^^^\n195 |         \"\"\"\n196 |         Log entry into a component during evaluation.\n    |\n\nF821 Undefined name `Optional`\n   --> canonical_flow/evaluation/audit_logger.py:223:64\n    |\n221 |         return component_trace_id\n222 |\n223 |     def log_component_exit(self, component_name: str, outputs: Optional[Dict[str, Any]] = None,\n    |                                                                ^^^^^^^^\n224 |                           errors: Optional[List[str]] = None, evaluation_id: Optional[str] = None):\n225 |         \"\"\"\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/evaluation/audit_logger.py:223:73\n    |\n221 |         return component_trace_id\n222 |\n223 |     def log_component_exit(self, component_name: str, outputs: Optional[Dict[str, Any]] = None,\n    |                                                                         ^^^^\n224 |                           errors: Optional[List[str]] = None, evaluation_id: Optional[str] = None):\n225 |         \"\"\"\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/evaluation/audit_logger.py:223:83\n    |\n221 |         return component_trace_id\n222 |\n223 |     def log_component_exit(self, component_name: str, outputs: Optional[Dict[str, Any]] = None,\n    |                                                                                   ^^^\n224 |                           errors: Optional[List[str]] = None, evaluation_id: Optional[str] = None):\n225 |         \"\"\"\n    |\n\nF821 Undefined name `Optional`\n   --> canonical_flow/evaluation/audit_logger.py:224:35\n    |\n223 |     def log_component_exit(self, component_name: str, outputs: Optional[Dict[str, Any]] = None,\n224 |                           errors: Optional[List[str]] = None, evaluation_id: Optional[str] = None):\n    |                                   ^^^^^^^^\n225 |         \"\"\"\n226 | # # #         Log exit from a component during evaluation.  # Module not found  # Module not found  # Module not found\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/evaluation/audit_logger.py:224:44\n    |\n223 |     def log_component_exit(self, component_name: str, outputs: Optional[Dict[str, Any]] = None,\n224 |                           errors: Optional[List[str]] = None, evaluation_id: Optional[str] = None):\n    |                                            ^^^^\n225 |         \"\"\"\n226 | # # #         Log exit from a component during evaluation.  # Module not found  # Module not found  # Module not found\n    |\n\nF821 Undefined name `Optional`\n   --> canonical_flow/evaluation/audit_logger.py:224:78\n    |\n223 |     def log_component_exit(self, component_name: str, outputs: Optional[Dict[str, Any]] = None,\n224 |                           errors: Optional[List[str]] = None, evaluation_id: Optional[str] = None):\n    |                                                                              ^^^^^^^^\n225 |         \"\"\"\n226 | # # #         Log exit from a component during evaluation.  # Module not found  # Module not found  # Module not found\n    |\n\nF821 Undefined name `Optional`\n   --> canonical_flow/evaluation/audit_logger.py:259:47\n    |\n258 |     def log_question_processing(self, question_id: str, duration_ms: float,\n259 |                                question_text: Optional[str] = None,\n    |                                               ^^^^^^^^\n260 |                                evaluation_id: Optional[str] = None):\n261 |         \"\"\"\n    |\n\nF821 Undefined name `Optional`\n   --> canonical_flow/evaluation/audit_logger.py:260:47\n    |\n258 |     def log_question_processing(self, question_id: str, duration_ms: float,\n259 |                                question_text: Optional[str] = None,\n260 |                                evaluation_id: Optional[str] = None):\n    |                                               ^^^^^^^^\n261 |         \"\"\"\n262 |         Log question processing timing and details.\n    |\n\nF821 Undefined name `Optional`\n   --> canonical_flow/evaluation/audit_logger.py:281:49\n    |\n280 |     def log_evidence_validation(self, evidence_id: str, validation_time_ms: float,\n281 |                                page_references: Optional[List[str]] = None,\n    |                                                 ^^^^^^^^\n282 |                                validation_status: str = \"completed\",\n283 |                                evaluation_id: Optional[str] = None):\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/evaluation/audit_logger.py:281:58\n    |\n280 |     def log_evidence_validation(self, evidence_id: str, validation_time_ms: float,\n281 |                                page_references: Optional[List[str]] = None,\n    |                                                          ^^^^\n282 |                                validation_status: str = \"completed\",\n283 |                                evaluation_id: Optional[str] = None):\n    |\n\nF821 Undefined name `Optional`\n   --> canonical_flow/evaluation/audit_logger.py:283:47\n    |\n281 |                                page_references: Optional[List[str]] = None,\n282 |                                validation_status: str = \"completed\",\n283 |                                evaluation_id: Optional[str] = None):\n    |                                               ^^^^^^^^\n284 |         \"\"\"\n285 |         Log evidence validation timing and details.\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/evaluation/audit_logger.py:317:67\n    |\n315 |         logger.debug(f\"Evidence {evidence_id} validated in {validation_time_ms:.2f}ms\")\n316 |\n317 |     def log_scoring_computation(self, dimension: str, raw_scores: List[float],\n    |                                                                   ^^^^\n318 |                                weighted_scores: List[float], weights: List[float],\n319 |                                final_score: float, classification: str,\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/evaluation/audit_logger.py:318:49\n    |\n317 |     def log_scoring_computation(self, dimension: str, raw_scores: List[float],\n318 |                                weighted_scores: List[float], weights: List[float],\n    |                                                 ^^^^\n319 |                                final_score: float, classification: str,\n320 |                                evaluation_id: Optional[str] = None):\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/evaluation/audit_logger.py:318:71\n    |\n317 |     def log_scoring_computation(self, dimension: str, raw_scores: List[float],\n318 |                                weighted_scores: List[float], weights: List[float],\n    |                                                                       ^^^^\n319 |                                final_score: float, classification: str,\n320 |                                evaluation_id: Optional[str] = None):\n    |\n\nF821 Undefined name `Optional`\n   --> canonical_flow/evaluation/audit_logger.py:320:47\n    |\n318 |                                weighted_scores: List[float], weights: List[float],\n319 |                                final_score: float, classification: str,\n320 |                                evaluation_id: Optional[str] = None):\n    |                                               ^^^^^^^^\n321 |         \"\"\"\n322 |         Log scoring computation details with diagnostic information.\n    |\n\nF821 Undefined name `Optional`\n   --> canonical_flow/evaluation/audit_logger.py:372:56\n    |\n370 |         logger.debug(f\"Scoring computed for {dimension}: {final_score:.3f} -> {classification}\")\n371 |\n372 |     def complete_point_evaluation(self, evaluation_id: Optional[str] = None):\n    |                                                        ^^^^^^^^\n373 |         \"\"\"\n374 |         Mark a Dec\u00e1logo point evaluation as completed.\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/evaluation/audit_logger.py:391:69\n    |\n389 |         logger.info(f\"Completed evaluation {eval_id}\")\n390 |\n391 |     def validate_coverage(self, dimension: str, expected_questions: List[str],\n    |                                                                     ^^^^\n392 |                          actual_questions: List[str]) -> CoverageValidationResult:\n393 |         \"\"\"\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/evaluation/audit_logger.py:392:44\n    |\n391 |     def validate_coverage(self, dimension: str, expected_questions: List[str],\n392 |                          actual_questions: List[str]) -> CoverageValidationResult:\n    |                                            ^^^^\n393 |         \"\"\"\n394 |         Validate question coverage for a dimension.\n    |\n\nF821 Undefined name `Optional`\n   --> canonical_flow/evaluation/audit_logger.py:477:61\n    |\n475 |         return result\n476 |\n477 |     def _get_current_dimension(self, evaluation_id: str) -> Optional[str]:\n    |                                                             ^^^^^^^^\n478 |         \"\"\"Get the current dimension being evaluated.\"\"\"\n479 |         if evaluation_id not in self.decalogo_traces:\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/evaluation/audit_logger.py:490:40\n    |\n488 |         return None\n489 |\n490 |     def generate_audit_report(self) -> Dict[str, Any]:\n    |                                        ^^^^\n491 |         \"\"\"\n492 |         Generate comprehensive audit report.\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/evaluation/audit_logger.py:490:50\n    |\n488 |         return None\n489 |\n490 |     def generate_audit_report(self) -> Dict[str, Any]:\n    |                                                  ^^^\n491 |         \"\"\"\n492 |         Generate comprehensive audit report.\n    |\n\nF821 Undefined name `datetime`\n   --> canonical_flow/evaluation/audit_logger.py:509:32\n    |\n507 |         report = {\n508 |             \"session_id\": self.session_id,\n509 |             \"audit_timestamp\": datetime.now(timezone.utc).isoformat(),\n    |                                ^^^^^^^^\n510 |             \"session_duration_s\": time.time() - self._start_time,\n    |\n\nF821 Undefined name `timezone`\n   --> canonical_flow/evaluation/audit_logger.py:509:45\n    |\n507 |         report = {\n508 |             \"session_id\": self.session_id,\n509 |             \"audit_timestamp\": datetime.now(timezone.utc).isoformat(),\n    |                                             ^^^^^^^^\n510 |             \"session_duration_s\": time.time() - self._start_time,\n    |\n\nF821 Undefined name `asdict`\n   --> canonical_flow/evaluation/audit_logger.py:514:26\n    |\n512 |             # Core tracking data\n513 |             \"decalogo_traces\": {\n514 |                 eval_id: asdict(trace) \n    |                          ^^^^^^\n515 |                 for eval_id, trace in self.decalogo_traces.items()\n516 |             },\n    |\n\nF821 Undefined name `asdict`\n   --> canonical_flow/evaluation/audit_logger.py:519:22\n    |\n518 |             \"coverage_validation\": {\n519 |                 dim: asdict(result) \n    |                      ^^^^^^\n520 |                 for dim, result in self.coverage_results.items()\n521 |             },\n    |\n\nF821 Undefined name `asdict`\n   --> canonical_flow/evaluation/audit_logger.py:524:22\n    |\n523 |             \"scoring_consistency\": {\n524 |                 dim: asdict(result) \n    |                      ^^^^^^\n525 |                 for dim, result in self.scoring_results.items()\n526 |             },\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/evaluation/audit_logger.py:538:45\n    |\n536 |         return report\n537 |\n538 |     def _generate_summary_analysis(self) -> Dict[str, Any]:\n    |                                             ^^^^\n539 |         \"\"\"Generate high-level summary analysis.\"\"\"\n540 |         summary = {\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/evaluation/audit_logger.py:538:55\n    |\n536 |         return report\n537 |\n538 |     def _generate_summary_analysis(self) -> Dict[str, Any]:\n    |                                                       ^^^\n539 |         \"\"\"Generate high-level summary analysis.\"\"\"\n540 |         summary = {\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/evaluation/audit_logger.py:573:44\n    |\n571 |         return summary\n572 |\n573 |     def _generate_recommendations(self) -> List[Dict[str, Any]]:\n    |                                            ^^^^\n574 |         \"\"\"Generate actionable recommendations based on audit findings.\"\"\"\n575 |         recommendations = []\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/evaluation/audit_logger.py:573:49\n    |\n571 |         return summary\n572 |\n573 |     def _generate_recommendations(self) -> List[Dict[str, Any]]:\n    |                                                 ^^^^\n574 |         \"\"\"Generate actionable recommendations based on audit findings.\"\"\"\n575 |         recommendations = []\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/evaluation/audit_logger.py:573:59\n    |\n571 |         return summary\n572 |\n573 |     def _generate_recommendations(self) -> List[Dict[str, Any]]:\n    |                                                           ^^^\n574 |         \"\"\"Generate actionable recommendations based on audit findings.\"\"\"\n575 |         recommendations = []\n    |\n\nF821 Undefined name `Optional`\n   --> canonical_flow/evaluation/audit_logger.py:612:37\n    |\n610 |         return recommendations\n611 |\n612 |     def save_audit(self, file_path: Optional[str] = None) -> str:\n    |                                     ^^^^^^^^\n613 |         \"\"\"\n614 |         Save audit report to JSON file.\n    |\n\nF821 Undefined name `Path`\n   --> canonical_flow/evaluation/audit_logger.py:623:9\n    |\n621 |         \"\"\"\n622 |         audit_path = file_path or self.audit_file\n623 |         Path(audit_path).parent.mkdir(parents=True, exist_ok=True)\n    |         ^^^^\n624 |         \n625 |         report = self.generate_audit_report()\n    |\n\nF821 Undefined name `Optional`\n   --> canonical_flow/evaluation/audit_logger.py:633:37\n    |\n631 |         return audit_path\n632 |\n633 |     def load_audit(self, file_path: Optional[str] = None) -> Dict[str, Any]:\n    |                                     ^^^^^^^^\n634 |         \"\"\"\n635 | # # #         Load previous audit report from JSON file.  # Module not found  # Module not found  # Module not found\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/evaluation/audit_logger.py:633:62\n    |\n631 |         return audit_path\n632 |\n633 |     def load_audit(self, file_path: Optional[str] = None) -> Dict[str, Any]:\n    |                                                              ^^^^\n634 |         \"\"\"\n635 | # # #         Load previous audit report from JSON file.  # Module not found  # Module not found  # Module not found\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/evaluation/audit_logger.py:633:72\n    |\n631 |         return audit_path\n632 |\n633 |     def load_audit(self, file_path: Optional[str] = None) -> Dict[str, Any]:\n    |                                                                        ^^^\n634 |         \"\"\"\n635 | # # #         Load previous audit report from JSON file.  # Module not found  # Module not found  # Module not found\n    |\n\nF821 Undefined name `Optional`\n   --> canonical_flow/evaluation/audit_logger.py:657:23\n    |\n656 | # Convenience functions for global audit logging\n657 | _global_audit_logger: Optional[AuditLogger] = None\n    |                       ^^^^^^^^\n    |\n\nF821 Undefined name `Optional`\n   --> canonical_flow/evaluation/audit_logger.py:673:54\n    |\n673 | def log_component_entry(component_name: str, inputs: Optional[Dict[str, Any]] = None) -> str:\n    |                                                      ^^^^^^^^\n674 |     \"\"\"Log component entry (global).\"\"\"\n675 |     return get_audit_logger().log_component_entry(component_name, inputs)\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/evaluation/audit_logger.py:673:63\n    |\n673 | def log_component_entry(component_name: str, inputs: Optional[Dict[str, Any]] = None) -> str:\n    |                                                               ^^^^\n674 |     \"\"\"Log component entry (global).\"\"\"\n675 |     return get_audit_logger().log_component_entry(component_name, inputs)\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/evaluation/audit_logger.py:673:73\n    |\n673 | def log_component_entry(component_name: str, inputs: Optional[Dict[str, Any]] = None) -> str:\n    |                                                                         ^^^\n674 |     \"\"\"Log component entry (global).\"\"\"\n675 |     return get_audit_logger().log_component_entry(component_name, inputs)\n    |\n\nF821 Undefined name `Optional`\n   --> canonical_flow/evaluation/audit_logger.py:678:54\n    |\n678 | def log_component_exit(component_name: str, outputs: Optional[Dict[str, Any]] = None,\n    |                                                      ^^^^^^^^\n679 |                       errors: Optional[List[str]] = None):\n680 |     \"\"\"Log component exit (global).\"\"\"\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/evaluation/audit_logger.py:678:63\n    |\n678 | def log_component_exit(component_name: str, outputs: Optional[Dict[str, Any]] = None,\n    |                                                               ^^^^\n679 |                       errors: Optional[List[str]] = None):\n680 |     \"\"\"Log component exit (global).\"\"\"\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/evaluation/audit_logger.py:678:73\n    |\n678 | def log_component_exit(component_name: str, outputs: Optional[Dict[str, Any]] = None,\n    |                                                                         ^^^\n679 |                       errors: Optional[List[str]] = None):\n680 |     \"\"\"Log component exit (global).\"\"\"\n    |\n\nF821 Undefined name `Optional`\n   --> canonical_flow/evaluation/audit_logger.py:679:31\n    |\n678 | def log_component_exit(component_name: str, outputs: Optional[Dict[str, Any]] = None,\n679 |                       errors: Optional[List[str]] = None):\n    |                               ^^^^^^^^\n680 |     \"\"\"Log component exit (global).\"\"\"\n681 |     get_audit_logger().log_component_exit(component_name, outputs, errors)\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/evaluation/audit_logger.py:679:40\n    |\n678 | def log_component_exit(component_name: str, outputs: Optional[Dict[str, Any]] = None,\n679 |                       errors: Optional[List[str]] = None):\n    |                                        ^^^^\n680 |     \"\"\"Log component exit (global).\"\"\"\n681 |     get_audit_logger().log_component_exit(component_name, outputs, errors)\n    |\n\nF821 Undefined name `Optional`\n   --> canonical_flow/evaluation/audit_logger.py:684:34\n    |\n684 | def save_audit_report(file_path: Optional[str] = None) -> str:\n    |                                  ^^^^^^^^\n685 |     \"\"\"Save global audit report.\"\"\"\n686 |     return get_audit_logger().save_audit(file_path)\n    |\n\nF821 Undefined name `Set`\n  --> canonical_flow/ingestion/__init__.py:23:21\n   |\n21 |     \"\"\"\n22 |     \n23 |     VALID_SUFFIXES: Set[str] = {\n   |                     ^^^\n24 |         'text',         # 01I: PDF text extraction\n25 |         'bundle',       # 02I: Document bundle creation\n   |\n\nF821 Undefined name `Optional`\n  --> canonical_flow/ingestion/__init__.py:31:35\n   |\n29 |     }\n30 |     \n31 |     def __init__(self, base_path: Optional[str] = None):\n   |                                   ^^^^^^^^\n32 |         \"\"\"\n33 |         Initialize the ArtifactManager.\n   |\n\nF821 Undefined name `Path`\n  --> canonical_flow/ingestion/__init__.py:40:28\n   |\n38 |         if base_path is None:\n39 |             # Default to canonical_flow/ingestion/ directory\n40 |             current_file = Path(__file__).resolve()\n   |                            ^^^^\n41 |             base_path = current_file.parent\n   |\n\nF821 Undefined name `Path`\n  --> canonical_flow/ingestion/__init__.py:43:26\n   |\n41 |             base_path = current_file.parent\n42 |         \n43 |         self.base_path = Path(base_path).resolve()\n   |                          ^^^^\n44 |         \n45 |         # Validate base path is within canonical_flow/ingestion/\n   |\n\nF821 Undefined name `Any`\n  --> canonical_flow/ingestion/__init__.py:60:15\n   |\n58 |         stem: str, \n59 |         suffix: str, \n60 |         data: Any, \n   |               ^^^\n61 |         subdir: Optional[str] = None\n62 |     ) -> Path:\n   |\n\nF821 Undefined name `Optional`\n  --> canonical_flow/ingestion/__init__.py:61:17\n   |\n59 |         suffix: str, \n60 |         data: Any, \n61 |         subdir: Optional[str] = None\n   |                 ^^^^^^^^\n62 |     ) -> Path:\n63 |         \"\"\"\n   |\n\nF821 Undefined name `Path`\n  --> canonical_flow/ingestion/__init__.py:62:10\n   |\n60 |         data: Any, \n61 |         subdir: Optional[str] = None\n62 |     ) -> Path:\n   |          ^^^^\n63 |         \"\"\"\n64 |         Write JSON artifact with standardized formatting and validation.\n   |\n\nF821 Undefined name `Path`\n   --> canonical_flow/ingestion/__init__.py:133:16\n    |\n131 |             True if path follows naming convention, False otherwise\n132 |         \"\"\"\n133 |         path = Path(file_path)\n    |                ^^^^\n134 |         \n135 |         # Must be .json extension\n    |\n\ninvalid-syntax: Unexpected indentation\n  --> canonical_flow/knowledge/__init__.py:8:1\n   |\n 7 | # # # from .preflight_validator import (  # Module not found  # Module not found  # Module not found\n 8 |     KStagePreflightValidator,\n   | ^^^^\n 9 |     ValidationStatus,\n10 |     ValidationResult,\n   |\n\ninvalid-syntax: Expected a statement\n  --> canonical_flow/knowledge/__init__.py:15:1\n   |\n13 |     JSONSchemaConfig,\n14 |     run_preflight_validation\n15 | )\n   | ^\n16 |\n17 | __all__ = [\n   |\n\ninvalid-syntax: Expected a statement\n  --> canonical_flow/knowledge/__init__.py:15:2\n   |\n13 |     JSONSchemaConfig,\n14 |     run_preflight_validation\n15 | )\n   |  ^\n16 |\n17 | __all__ = [\n   |\n\ninvalid-syntax: Unexpected indentation\n  --> canonical_flow/knowledge/knowledge_audit_demo.py:17:1\n   |\n16 | # # # from canonical_flow.knowledge.knowledge_audit_system import (  # Module not found  # Module not found  # Module not found\n17 |     get_audit_system, \n   | ^^^^\n18 |     component_06K_process, \n19 |     component_07K_process\n   |\n\ninvalid-syntax: Expected a statement\n  --> canonical_flow/knowledge/knowledge_audit_demo.py:20:1\n   |\n18 |     component_06K_process, \n19 |     component_07K_process\n20 | )\n   | ^\n   |\n\ninvalid-syntax: Expected a statement\n  --> canonical_flow/knowledge/knowledge_audit_demo.py:20:2\n   |\n18 |     component_06K_process, \n19 |     component_07K_process\n20 | )\n   |  ^\n   |\n\nF401 [*] `os` imported but unused\n  --> canonical_flow/knowledge/knowledge_audit_system.py:18:8\n   |\n16 | # # # from dataclasses import dataclass, asdict  # Module not found  # Module not found  # Module not found\n17 | # # # from contextlib import contextmanager  # Module not found  # Module not found  # Module not found\n18 | import os\n   |        ^^\n19 | import threading\n   |\nhelp: Remove unused import: `os`\n\nF821 Undefined name `dataclass`\n  --> canonical_flow/knowledge/knowledge_audit_system.py:42:2\n   |\n42 | @dataclass\n   |  ^^^^^^^^^\n43 | class AuditEntry:\n44 |     \"\"\"Audit entry for component execution.\"\"\"\n   |\n\nF821 Undefined name `datetime`\n  --> canonical_flow/knowledge/knowledge_audit_system.py:48:17\n   |\n46 |     component_name: str\n47 |     execution_id: str\n48 |     start_time: datetime\n   |                 ^^^^^^^^\n49 |     end_time: Optional[datetime] = None\n50 |     duration_ms: Optional[float] = None\n   |\n\nF821 Undefined name `Optional`\n  --> canonical_flow/knowledge/knowledge_audit_system.py:49:15\n   |\n47 |     execution_id: str\n48 |     start_time: datetime\n49 |     end_time: Optional[datetime] = None\n   |               ^^^^^^^^\n50 |     duration_ms: Optional[float] = None\n51 |     peak_memory_mb: Optional[float] = None\n   |\n\nF821 Undefined name `datetime`\n  --> canonical_flow/knowledge/knowledge_audit_system.py:49:24\n   |\n47 |     execution_id: str\n48 |     start_time: datetime\n49 |     end_time: Optional[datetime] = None\n   |                        ^^^^^^^^\n50 |     duration_ms: Optional[float] = None\n51 |     peak_memory_mb: Optional[float] = None\n   |\n\nF821 Undefined name `Optional`\n  --> canonical_flow/knowledge/knowledge_audit_system.py:50:18\n   |\n48 |     start_time: datetime\n49 |     end_time: Optional[datetime] = None\n50 |     duration_ms: Optional[float] = None\n   |                  ^^^^^^^^\n51 |     peak_memory_mb: Optional[float] = None\n52 |     success: bool = True\n   |\n\nF821 Undefined name `Optional`\n  --> canonical_flow/knowledge/knowledge_audit_system.py:51:21\n   |\n49 |     end_time: Optional[datetime] = None\n50 |     duration_ms: Optional[float] = None\n51 |     peak_memory_mb: Optional[float] = None\n   |                     ^^^^^^^^\n52 |     success: bool = True\n53 |     error_details: Optional[str] = None\n   |\n\nF821 Undefined name `Optional`\n  --> canonical_flow/knowledge/knowledge_audit_system.py:53:20\n   |\n51 |     peak_memory_mb: Optional[float] = None\n52 |     success: bool = True\n53 |     error_details: Optional[str] = None\n   |                    ^^^^^^^^\n54 |     input_artifacts: List[Dict[str, Any]] = None\n55 |     output_artifacts: List[Dict[str, Any]] = None\n   |\n\nF821 Undefined name `List`\n  --> canonical_flow/knowledge/knowledge_audit_system.py:54:22\n   |\n52 |     success: bool = True\n53 |     error_details: Optional[str] = None\n54 |     input_artifacts: List[Dict[str, Any]] = None\n   |                      ^^^^\n55 |     output_artifacts: List[Dict[str, Any]] = None\n56 |     checksum: Optional[str] = None\n   |\n\nF821 Undefined name `Dict`\n  --> canonical_flow/knowledge/knowledge_audit_system.py:54:27\n   |\n52 |     success: bool = True\n53 |     error_details: Optional[str] = None\n54 |     input_artifacts: List[Dict[str, Any]] = None\n   |                           ^^^^\n55 |     output_artifacts: List[Dict[str, Any]] = None\n56 |     checksum: Optional[str] = None\n   |\n\nF821 Undefined name `Any`\n  --> canonical_flow/knowledge/knowledge_audit_system.py:54:37\n   |\n52 |     success: bool = True\n53 |     error_details: Optional[str] = None\n54 |     input_artifacts: List[Dict[str, Any]] = None\n   |                                     ^^^\n55 |     output_artifacts: List[Dict[str, Any]] = None\n56 |     checksum: Optional[str] = None\n   |\n\nF821 Undefined name `List`\n  --> canonical_flow/knowledge/knowledge_audit_system.py:55:23\n   |\n53 |     error_details: Optional[str] = None\n54 |     input_artifacts: List[Dict[str, Any]] = None\n55 |     output_artifacts: List[Dict[str, Any]] = None\n   |                       ^^^^\n56 |     checksum: Optional[str] = None\n57 |     metadata: Dict[str, Any] = None\n   |\n\nF821 Undefined name `Dict`\n  --> canonical_flow/knowledge/knowledge_audit_system.py:55:28\n   |\n53 |     error_details: Optional[str] = None\n54 |     input_artifacts: List[Dict[str, Any]] = None\n55 |     output_artifacts: List[Dict[str, Any]] = None\n   |                            ^^^^\n56 |     checksum: Optional[str] = None\n57 |     metadata: Dict[str, Any] = None\n   |\n\nF821 Undefined name `Any`\n  --> canonical_flow/knowledge/knowledge_audit_system.py:55:38\n   |\n53 |     error_details: Optional[str] = None\n54 |     input_artifacts: List[Dict[str, Any]] = None\n55 |     output_artifacts: List[Dict[str, Any]] = None\n   |                                      ^^^\n56 |     checksum: Optional[str] = None\n57 |     metadata: Dict[str, Any] = None\n   |\n\nF821 Undefined name `Optional`\n  --> canonical_flow/knowledge/knowledge_audit_system.py:56:15\n   |\n54 |     input_artifacts: List[Dict[str, Any]] = None\n55 |     output_artifacts: List[Dict[str, Any]] = None\n56 |     checksum: Optional[str] = None\n   |               ^^^^^^^^\n57 |     metadata: Dict[str, Any] = None\n   |\n\nF821 Undefined name `Dict`\n  --> canonical_flow/knowledge/knowledge_audit_system.py:57:15\n   |\n55 |     output_artifacts: List[Dict[str, Any]] = None\n56 |     checksum: Optional[str] = None\n57 |     metadata: Dict[str, Any] = None\n   |               ^^^^\n58 |\n59 |     def __post_init__(self):\n   |\n\nF821 Undefined name `Any`\n  --> canonical_flow/knowledge/knowledge_audit_system.py:57:25\n   |\n55 |     output_artifacts: List[Dict[str, Any]] = None\n56 |     checksum: Optional[str] = None\n57 |     metadata: Dict[str, Any] = None\n   |                         ^^^\n58 |\n59 |     def __post_init__(self):\n   |\n\nF821 Undefined name `Path`\n  --> canonical_flow/knowledge/knowledge_audit_system.py:82:32\n   |\n81 |     def __init__(self, audit_file_path: str = \"canonical_flow/knowledge/knowledge_audit.json\"):\n82 |         self.audit_file_path = Path(audit_file_path)\n   |                                ^^^^\n83 |         self.audit_entries: List[AuditEntry] = []\n84 |         self._lock = threading.Lock()\n   |\n\nF821 Undefined name `List`\n  --> canonical_flow/knowledge/knowledge_audit_system.py:83:29\n   |\n81 |     def __init__(self, audit_file_path: str = \"canonical_flow/knowledge/knowledge_audit.json\"):\n82 |         self.audit_file_path = Path(audit_file_path)\n83 |         self.audit_entries: List[AuditEntry] = []\n   |                             ^^^^\n84 |         self._lock = threading.Lock()\n85 |         self._current_process = psutil.Process()\n   |\n\nF821 Undefined name `datetime`\n   --> canonical_flow/knowledge/knowledge_audit_system.py:103:60\n    |\n101 | \u2026                     # Convert datetime strings back to datetime objects\n102 | \u2026                     if \"start_time\" in entry_data:\n103 | \u2026                         entry_data[\"start_time\"] = datetime.fromisoformat(entry_data[\"start_time\"])\n    |                                                      ^^^^^^^^\n104 | \u2026                     if \"end_time\" in entry_data and entry_data[\"end_time\"]:\n105 | \u2026                         entry_data[\"end_time\"] = datetime.fromisoformat(entry_data[\"end_time\"])\n    |\n\nF821 Undefined name `datetime`\n   --> canonical_flow/knowledge/knowledge_audit_system.py:105:58\n    |\n103 | \u2026                         entry_data[\"start_time\"] = datetime.fromisoformat(entry_data[\"start_time\"])\n104 | \u2026                     if \"end_time\" in entry_data and entry_data[\"end_time\"]:\n105 | \u2026                         entry_data[\"end_time\"] = datetime.fromisoformat(entry_data[\"end_time\"])\n    |                                                    ^^^^^^^^\n106 | \u2026                     \n107 | \u2026                     entry = AuditEntry(**entry_data)\n    |\n\nF821 Undefined name `contextmanager`\n   --> canonical_flow/knowledge/knowledge_audit_system.py:112:6\n    |\n110 |                 print(f\"Warning: Failed to load existing audit data: {e}\")\n111 |\n112 |     @contextmanager\n    |      ^^^^^^^^^^^^^^\n113 |     def audit_execution(self, component_code: str, input_data: Any = None, \n114 |                        metadata: Dict[str, Any] = None):\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/knowledge/knowledge_audit_system.py:113:64\n    |\n112 |     @contextmanager\n113 |     def audit_execution(self, component_code: str, input_data: Any = None, \n    |                                                                ^^^\n114 |                        metadata: Dict[str, Any] = None):\n115 |         \"\"\"Context manager for auditing component execution.\"\"\"\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/knowledge/knowledge_audit_system.py:114:34\n    |\n112 |     @contextmanager\n113 |     def audit_execution(self, component_code: str, input_data: Any = None, \n114 |                        metadata: Dict[str, Any] = None):\n    |                                  ^^^^\n115 |         \"\"\"Context manager for auditing component execution.\"\"\"\n116 |         execution_id = self._generate_execution_id(component_code)\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/knowledge/knowledge_audit_system.py:114:44\n    |\n112 |     @contextmanager\n113 |     def audit_execution(self, component_code: str, input_data: Any = None, \n114 |                        metadata: Dict[str, Any] = None):\n    |                                            ^^^\n115 |         \"\"\"Context manager for auditing component execution.\"\"\"\n116 |         execution_id = self._generate_execution_id(component_code)\n    |\n\nF821 Undefined name `datetime`\n   --> canonical_flow/knowledge/knowledge_audit_system.py:124:24\n    |\n122 |             component_name=component_name,\n123 |             execution_id=execution_id,\n124 |             start_time=datetime.now(),\n    |                        ^^^^^^^^\n125 |             metadata=metadata or {}\n126 |         )\n    |\n\nF821 Undefined name `datetime`\n   --> canonical_flow/knowledge/knowledge_audit_system.py:160:30\n    |\n159 |             # Finalize audit entry\n160 |             entry.end_time = datetime.now()\n    |                              ^^^^^^^^\n161 |             entry.duration_ms = (entry.end_time - entry.start_time).total_seconds() * 1000\n162 |             entry.peak_memory_mb = peak_memory\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/knowledge/knowledge_audit_system.py:168:71\n    |\n166 |                 self.audit_entries.append(entry)\n167 |\n168 |     def record_output_artifacts(self, execution_id: str, output_data: Any):\n    |                                                                       ^^^\n169 |         \"\"\"Record output artifacts for a given execution.\"\"\"\n170 |         with self._lock:\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/knowledge/knowledge_audit_system.py:177:40\n    |\n175 |                     break\n176 |\n177 |     def _analyze_artifacts(self, data: Any, artifact_type: str) -> List[Dict[str, Any]]:\n    |                                        ^^^\n178 |         \"\"\"Analyze input/output artifacts and extract metadata.\"\"\"\n179 |         artifacts = []\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/knowledge/knowledge_audit_system.py:177:68\n    |\n175 |                     break\n176 |\n177 |     def _analyze_artifacts(self, data: Any, artifact_type: str) -> List[Dict[str, Any]]:\n    |                                                                    ^^^^\n178 |         \"\"\"Analyze input/output artifacts and extract metadata.\"\"\"\n179 |         artifacts = []\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/knowledge/knowledge_audit_system.py:177:73\n    |\n175 |                     break\n176 |\n177 |     def _analyze_artifacts(self, data: Any, artifact_type: str) -> List[Dict[str, Any]]:\n    |                                                                         ^^^^\n178 |         \"\"\"Analyze input/output artifacts and extract metadata.\"\"\"\n179 |         artifacts = []\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/knowledge/knowledge_audit_system.py:177:83\n    |\n175 |                     break\n176 |\n177 |     def _analyze_artifacts(self, data: Any, artifact_type: str) -> List[Dict[str, Any]]:\n    |                                                                                   ^^^\n178 |         \"\"\"Analyze input/output artifacts and extract metadata.\"\"\"\n179 |         artifacts = []\n    |\n\nF821 Undefined name `Path`\n   --> canonical_flow/knowledge/knowledge_audit_system.py:184:44\n    |\n182 |             # Handle dictionary data\n183 |             for key, value in data.items():\n184 |                 if isinstance(value, (str, Path)) and self._is_file_path(str(value)):\n    |                                            ^^^^\n185 |                     artifacts.append(self._get_file_info(str(value), f\"{artifact_type}_{key}\"))\n186 |                 elif isinstance(value, bytes):\n    |\n\nF821 Undefined name `Path`\n   --> canonical_flow/knowledge/knowledge_audit_system.py:201:37\n    |\n199 |                         \"path\": None\n200 |                     })\n201 |         elif isinstance(data, (str, Path)) and self._is_file_path(str(data)):\n    |                                     ^^^^\n202 |             artifacts.append(self._get_file_info(str(data), artifact_type))\n203 |         elif isinstance(data, (list, tuple)):\n    |\n\nF821 Undefined name `Path`\n   --> canonical_flow/knowledge/knowledge_audit_system.py:205:43\n    |\n203 |         elif isinstance(data, (list, tuple)):\n204 |             for i, item in enumerate(data):\n205 |                 if isinstance(item, (str, Path)) and self._is_file_path(str(item)):\n    |                                           ^^^^\n206 |                     artifacts.append(self._get_file_info(str(item), f\"{artifact_type}_{i}\"))\n    |\n\nF821 Undefined name `Path`\n   --> canonical_flow/knowledge/knowledge_audit_system.py:213:20\n    |\n211 |         \"\"\"Check if a string represents a valid file path.\"\"\"\n212 |         try:\n213 |             path = Path(path_str)\n    |                    ^^^^\n214 |             return path.exists() and path.is_file()\n215 |         except:\n    |\n\nE722 Do not use bare `except`\n   --> canonical_flow/knowledge/knowledge_audit_system.py:215:9\n    |\n213 |             path = Path(path_str)\n214 |             return path.exists() and path.is_file()\n215 |         except:\n    |         ^^^^^^\n216 |             return False\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/knowledge/knowledge_audit_system.py:218:60\n    |\n216 |             return False\n217 |\n218 |     def _get_file_info(self, file_path: str, name: str) -> Dict[str, Any]:\n    |                                                            ^^^^\n219 |         \"\"\"Get file information for audit.\"\"\"\n220 |         try:\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/knowledge/knowledge_audit_system.py:218:70\n    |\n216 |             return False\n217 |\n218 |     def _get_file_info(self, file_path: str, name: str) -> Dict[str, Any]:\n    |                                                                      ^^^\n219 |         \"\"\"Get file information for audit.\"\"\"\n220 |         try:\n    |\n\nF821 Undefined name `Path`\n   --> canonical_flow/knowledge/knowledge_audit_system.py:221:20\n    |\n219 |         \"\"\"Get file information for audit.\"\"\"\n220 |         try:\n221 |             path = Path(file_path)\n    |                    ^^^^\n222 |             stat = path.stat()\n223 |             return {\n    |\n\nF821 Undefined name `datetime`\n   --> canonical_flow/knowledge/knowledge_audit_system.py:228:34\n    |\n226 |                 \"path\": str(path.absolute()),\n227 |                 \"size_bytes\": stat.st_size,\n228 |                 \"modified_time\": datetime.fromtimestamp(stat.st_mtime).isoformat()\n    |                                  ^^^^^^^^\n229 |             }\n230 |         except Exception as e:\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/knowledge/knowledge_audit_system.py:238:51\n    |\n236 |             }\n237 |\n238 |     def _compute_determinism_checksum(self, data: Any) -> str:\n    |                                                   ^^^\n239 |         \"\"\"Compute checksum for determinism verification.\"\"\"\n240 |         try:\n    |\n\nF841 Local variable `timestamp` is assigned to but never used\n   --> canonical_flow/knowledge/knowledge_audit_system.py:256:9\n    |\n254 |     def _generate_execution_id(self, component_code: str) -> str:\n255 |         \"\"\"Generate unique execution ID.\"\"\"\n256 |         timestamp = datetime.now().isoformat()\n    |         ^^^^^^^^^\n257 |         return f\"{component_code}_{int(time.time() * 1000)}\"\n    |\nhelp: Remove assignment to unused variable `timestamp`\n\nF821 Undefined name `datetime`\n   --> canonical_flow/knowledge/knowledge_audit_system.py:256:21\n    |\n254 |     def _generate_execution_id(self, component_code: str) -> str:\n255 |         \"\"\"Generate unique execution ID.\"\"\"\n256 |         timestamp = datetime.now().isoformat()\n    |                     ^^^^^^^^\n257 |         return f\"{component_code}_{int(time.time() * 1000)}\"\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/knowledge/knowledge_audit_system.py:259:40\n    |\n257 |         return f\"{component_code}_{int(time.time() * 1000)}\"\n258 |\n259 |     def validate_audit_schema(self) -> Dict[str, Any]:\n    |                                        ^^^^\n260 |         \"\"\"Validate that audit entries contain all required fields.\"\"\"\n261 |         validation_result = {\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/knowledge/knowledge_audit_system.py:259:50\n    |\n257 |         return f\"{component_code}_{int(time.time() * 1000)}\"\n258 |\n259 |     def validate_audit_schema(self) -> Dict[str, Any]:\n    |                                                  ^^^\n260 |         \"\"\"Validate that audit entries contain all required fields.\"\"\"\n261 |         validation_result = {\n    |\n\nF821 Undefined name `asdict`\n   --> canonical_flow/knowledge/knowledge_audit_system.py:273:26\n    |\n272 |         for i, entry in enumerate(self.audit_entries):\n273 |             entry_dict = asdict(entry)\n    |                          ^^^^^^\n274 |             for field in required_fields:\n275 |                 if field not in entry_dict or entry_dict[field] is None:\n    |\n\nF821 Undefined name `datetime`\n   --> canonical_flow/knowledge/knowledge_audit_system.py:303:37\n    |\n301 |                     \"system\": \"K_knowledge_extraction_audit\",\n302 |                     \"version\": \"1.0.0\",\n303 |                     \"generated_at\": datetime.now().isoformat(),\n    |                                     ^^^^^^^^\n304 |                     \"total_entries\": len(self.audit_entries),\n305 |                     \"validation_status\": validation\n    |\n\nF821 Undefined name `asdict`\n   --> canonical_flow/knowledge/knowledge_audit_system.py:313:30\n    |\n311 |             # Convert audit entries to serializable format\n312 |             for entry in self.audit_entries:\n313 |                 entry_dict = asdict(entry)\n    |                              ^^^^^^\n314 |                 # Convert datetime objects to ISO strings\n315 |                 if entry_dict[\"start_time\"]:\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/knowledge/knowledge_audit_system.py:330:66\n    |\n328 |                 raise IOError(f\"Failed to write audit file: {e}\")\n329 |\n330 |     def get_component_stats(self, component_code: str = None) -> Dict[str, Any]:\n    |                                                                  ^^^^\n331 |         \"\"\"Get execution statistics for a component or all components.\"\"\"\n332 |         with self._lock:\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/knowledge/knowledge_audit_system.py:330:76\n    |\n328 |                 raise IOError(f\"Failed to write audit file: {e}\")\n329 |\n330 |     def get_component_stats(self, component_code: str = None) -> Dict[str, Any]:\n    |                                                                            ^^^\n331 |         \"\"\"Get execution statistics for a component or all components.\"\"\"\n332 |         with self._lock:\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/knowledge/knowledge_audit_system.py:374:62\n    |\n374 | def audit_component_execution(component_code: str, metadata: Dict[str, Any] = None):\n    |                                                              ^^^^\n375 |     \"\"\"Decorator for auditing component execution.\"\"\"\n376 |     def decorator(func):\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/knowledge/knowledge_audit_system.py:374:72\n    |\n374 | def audit_component_execution(component_code: str, metadata: Dict[str, Any] = None):\n    |                                                                        ^^^\n375 |     \"\"\"Decorator for auditing component execution.\"\"\"\n376 |     def decorator(func):\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/knowledge/knowledge_audit_system.py:402:45\n    |\n400 | def create_placeholder_component(component_code: str, component_name: str):\n401 |     \"\"\"Create placeholder component with audit integration.\"\"\"\n402 |     def process(data=None, context=None) -> Dict[str, Any]:\n    |                                             ^^^^\n403 |         @audit_component_execution(component_code, metadata={\"component\": component_name})\n404 |         def _process_with_audit(data, context):\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/knowledge/knowledge_audit_system.py:402:55\n    |\n400 | def create_placeholder_component(component_code: str, component_name: str):\n401 |     \"\"\"Create placeholder component with audit integration.\"\"\"\n402 |     def process(data=None, context=None) -> Dict[str, Any]:\n    |                                                       ^^^\n403 |         @audit_component_execution(component_code, metadata={\"component\": component_name})\n404 |         def _process_with_audit(data, context):\n    |\n\nF401 [*] `hashlib` imported but unused\n  --> canonical_flow/knowledge/preflight_validator.py:26:8\n   |\n24 | # # # from pathlib import Path  # Module not found  # Module not found  # Module not found\n25 | # # # from typing import Dict, List, Optional, Any, Set, Tuple, Union  # Module not found  # Module not found  # Module not found\n26 | import hashlib\n   |        ^^^^^^^\n27 |\n28 | # Configure logging\n   |\nhelp: Remove unused import: `hashlib`\n\nF821 Undefined name `Enum`\n  --> canonical_flow/knowledge/preflight_validator.py:32:24\n   |\n32 | class ValidationStatus(Enum):\n   |                        ^^^^\n33 |     \"\"\"Validation check status\"\"\"\n34 |     PASS = \"pass\"\n   |\n\nF821 Undefined name `dataclass`\n  --> canonical_flow/knowledge/preflight_validator.py:40:2\n   |\n40 | @dataclass\n   |  ^^^^^^^^^\n41 | class ValidationResult:\n42 |     \"\"\"Result of a single validation check\"\"\"\n   |\n\nF821 Undefined name `Dict`\n  --> canonical_flow/knowledge/preflight_validator.py:46:14\n   |\n44 |     status: ValidationStatus\n45 |     message: str\n46 |     details: Dict[str, Any] = field(default_factory=dict)\n   |              ^^^^\n47 |     error_details: Optional[str] = None\n48 |     timestamp: datetime = field(default_factory=datetime.now)\n   |\n\nF821 Undefined name `Any`\n  --> canonical_flow/knowledge/preflight_validator.py:46:24\n   |\n44 |     status: ValidationStatus\n45 |     message: str\n46 |     details: Dict[str, Any] = field(default_factory=dict)\n   |                        ^^^\n47 |     error_details: Optional[str] = None\n48 |     timestamp: datetime = field(default_factory=datetime.now)\n   |\n\nF821 Undefined name `field`\n  --> canonical_flow/knowledge/preflight_validator.py:46:31\n   |\n44 |     status: ValidationStatus\n45 |     message: str\n46 |     details: Dict[str, Any] = field(default_factory=dict)\n   |                               ^^^^^\n47 |     error_details: Optional[str] = None\n48 |     timestamp: datetime = field(default_factory=datetime.now)\n   |\n\nF821 Undefined name `Optional`\n  --> canonical_flow/knowledge/preflight_validator.py:47:20\n   |\n45 |     message: str\n46 |     details: Dict[str, Any] = field(default_factory=dict)\n47 |     error_details: Optional[str] = None\n   |                    ^^^^^^^^\n48 |     timestamp: datetime = field(default_factory=datetime.now)\n   |\n\nF821 Undefined name `datetime`\n  --> canonical_flow/knowledge/preflight_validator.py:48:16\n   |\n46 |     details: Dict[str, Any] = field(default_factory=dict)\n47 |     error_details: Optional[str] = None\n48 |     timestamp: datetime = field(default_factory=datetime.now)\n   |                ^^^^^^^^\n   |\n\nF821 Undefined name `field`\n  --> canonical_flow/knowledge/preflight_validator.py:48:27\n   |\n46 |     details: Dict[str, Any] = field(default_factory=dict)\n47 |     error_details: Optional[str] = None\n48 |     timestamp: datetime = field(default_factory=datetime.now)\n   |                           ^^^^^\n   |\n\nF821 Undefined name `datetime`\n  --> canonical_flow/knowledge/preflight_validator.py:48:49\n   |\n46 |     details: Dict[str, Any] = field(default_factory=dict)\n47 |     error_details: Optional[str] = None\n48 |     timestamp: datetime = field(default_factory=datetime.now)\n   |                                                 ^^^^^^^^\n   |\n\nF821 Undefined name `dataclass`\n  --> canonical_flow/knowledge/preflight_validator.py:51:2\n   |\n51 | @dataclass\n   |  ^^^^^^^^^\n52 | class ChunkingPolicyConfig:\n53 |     \"\"\"Configuration for chunking policies\"\"\"\n   |\n\nF821 Undefined name `List`\n  --> canonical_flow/knowledge/preflight_validator.py:59:25\n   |\n57 |     max_chunk_size: int\n58 |     min_chunk_size: int\n59 |     separator_patterns: List[str] = field(default_factory=list)\n   |                         ^^^^\n   |\n\nF821 Undefined name `field`\n  --> canonical_flow/knowledge/preflight_validator.py:59:37\n   |\n57 |     max_chunk_size: int\n58 |     min_chunk_size: int\n59 |     separator_patterns: List[str] = field(default_factory=list)\n   |                                     ^^^^^\n   |\n\nF821 Undefined name `dataclass`\n  --> canonical_flow/knowledge/preflight_validator.py:62:2\n   |\n62 | @dataclass\n   |  ^^^^^^^^^\n63 | class EmbeddingModelConfig:\n64 |     \"\"\"Configuration for embedding models\"\"\"\n   |\n\nF821 Undefined name `Optional`\n  --> canonical_flow/knowledge/preflight_validator.py:70:17\n   |\n68 |     max_sequence_length: int\n69 |     normalization: bool = True\n70 |     model_hash: Optional[str] = None\n   |                 ^^^^^^^^\n   |\n\nF821 Undefined name `dataclass`\n  --> canonical_flow/knowledge/preflight_validator.py:73:2\n   |\n73 | @dataclass  \n   |  ^^^^^^^^^\n74 | class JSONSchemaConfig:\n75 |     \"\"\"Configuration for JSON schemas\"\"\"\n   |\n\nF821 Undefined name `Set`\n  --> canonical_flow/knowledge/preflight_validator.py:78:22\n   |\n76 |     schema_name: str\n77 |     schema_version: str\n78 |     required_fields: Set[str]\n   |                      ^^^\n79 |     optional_fields: Set[str] = field(default_factory=set)\n80 |     schema_hash: Optional[str] = None\n   |\n\nF821 Undefined name `Set`\n  --> canonical_flow/knowledge/preflight_validator.py:79:22\n   |\n77 |     schema_version: str\n78 |     required_fields: Set[str]\n79 |     optional_fields: Set[str] = field(default_factory=set)\n   |                      ^^^\n80 |     schema_hash: Optional[str] = None\n   |\n\nF821 Undefined name `field`\n  --> canonical_flow/knowledge/preflight_validator.py:79:33\n   |\n77 |     schema_version: str\n78 |     required_fields: Set[str]\n79 |     optional_fields: Set[str] = field(default_factory=set)\n   |                                 ^^^^^\n80 |     schema_hash: Optional[str] = None\n   |\n\nF821 Undefined name `Optional`\n  --> canonical_flow/knowledge/preflight_validator.py:80:18\n   |\n78 |     required_fields: Set[str]\n79 |     optional_fields: Set[str] = field(default_factory=set)\n80 |     schema_hash: Optional[str] = None\n   |                  ^^^^^^^^\n   |\n\nF821 Undefined name `Optional`\n  --> canonical_flow/knowledge/preflight_validator.py:94:38\n   |\n92 |     \"\"\"\n93 |     \n94 |     def __init__(self, project_root: Optional[Path] = None):\n   |                                      ^^^^^^^^\n95 |         self.project_root = project_root or Path(__file__).resolve().parents[2]\n96 |         self.validation_results: List[ValidationResult] = []\n   |\n\nF821 Undefined name `Path`\n  --> canonical_flow/knowledge/preflight_validator.py:94:47\n   |\n92 |     \"\"\"\n93 |     \n94 |     def __init__(self, project_root: Optional[Path] = None):\n   |                                               ^^^^\n95 |         self.project_root = project_root or Path(__file__).resolve().parents[2]\n96 |         self.validation_results: List[ValidationResult] = []\n   |\n\nF821 Undefined name `Path`\n  --> canonical_flow/knowledge/preflight_validator.py:95:45\n   |\n94 |     def __init__(self, project_root: Optional[Path] = None):\n95 |         self.project_root = project_root or Path(__file__).resolve().parents[2]\n   |                                             ^^^^\n96 |         self.validation_results: List[ValidationResult] = []\n97 |         self.k_stage_components = {\n   |\n\nF821 Undefined name `List`\n  --> canonical_flow/knowledge/preflight_validator.py:96:34\n   |\n94 |     def __init__(self, project_root: Optional[Path] = None):\n95 |         self.project_root = project_root or Path(__file__).resolve().parents[2]\n96 |         self.validation_results: List[ValidationResult] = []\n   |                                  ^^^^\n97 |         self.k_stage_components = {\n98 |             \"06K\": \"canonical_flow.mathematical_enhancers.retrieval_enhancer\",\n   |\n\nE722 Do not use bare `except`\n   --> canonical_flow/knowledge/preflight_validator.py:445:33\n    |\n443 | \u2026                                   [\"placeholder\", \"not implemented\", \"todo\", \"stub\"]):\n444 | \u2026                                 placeholder_detected = True\n445 | \u2026                     except:\n    |                       ^^^^^^\n446 | \u2026                         pass\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/knowledge/preflight_validator.py:538:47\n    |\n536 |             )\n537 |     \n538 |     def run_comprehensive_validation(self) -> Dict[str, Any]:\n    |                                               ^^^^\n539 |         \"\"\"Run all validation checks and return comprehensive results\"\"\"\n540 |         logger.info(\"Starting K-stage preflight validation\")\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/knowledge/preflight_validator.py:538:57\n    |\n536 |             )\n537 |     \n538 |     def run_comprehensive_validation(self) -> Dict[str, Any]:\n    |                                                         ^^^\n539 |         \"\"\"Run all validation checks and return comprehensive results\"\"\"\n540 |         logger.info(\"Starting K-stage preflight validation\")\n    |\n\nF821 Undefined name `datetime`\n   --> canonical_flow/knowledge/preflight_validator.py:542:28\n    |\n540 |         logger.info(\"Starting K-stage preflight validation\")\n541 |         \n542 |         validation_start = datetime.now()\n    |                            ^^^^^^^^\n543 |         \n544 |         # Clear previous results\n    |\n\nF821 Undefined name `datetime`\n   --> canonical_flow/knowledge/preflight_validator.py:576:26\n    |\n574 |                 self.validation_results.append(error_result)\n575 |         \n576 |         validation_end = datetime.now()\n    |                          ^^^^^^^^\n577 |         \n578 |         # Compile results\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/knowledge/preflight_validator.py:624:48\n    |\n622 |         return results\n623 |     \n624 |     def save_validation_results(self, results: Dict[str, Any], \n    |                                                ^^^^\n625 |                               output_path: Optional[Path] = None) -> Path:\n626 |         \"\"\"Save validation results to preflight_validation.json\"\"\"\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/knowledge/preflight_validator.py:624:58\n    |\n622 |         return results\n623 |     \n624 |     def save_validation_results(self, results: Dict[str, Any], \n    |                                                          ^^^\n625 |                               output_path: Optional[Path] = None) -> Path:\n626 |         \"\"\"Save validation results to preflight_validation.json\"\"\"\n    |\n\nF821 Undefined name `Optional`\n   --> canonical_flow/knowledge/preflight_validator.py:625:44\n    |\n624 |     def save_validation_results(self, results: Dict[str, Any], \n625 |                               output_path: Optional[Path] = None) -> Path:\n    |                                            ^^^^^^^^\n626 |         \"\"\"Save validation results to preflight_validation.json\"\"\"\n627 |         if output_path is None:\n    |\n\nF821 Undefined name `Path`\n   --> canonical_flow/knowledge/preflight_validator.py:625:53\n    |\n624 |     def save_validation_results(self, results: Dict[str, Any], \n625 |                               output_path: Optional[Path] = None) -> Path:\n    |                                                     ^^^^\n626 |         \"\"\"Save validation results to preflight_validation.json\"\"\"\n627 |         if output_path is None:\n    |\n\nF821 Undefined name `Path`\n   --> canonical_flow/knowledge/preflight_validator.py:625:70\n    |\n624 |     def save_validation_results(self, results: Dict[str, Any], \n625 |                               output_path: Optional[Path] = None) -> Path:\n    |                                                                      ^^^^\n626 |         \"\"\"Save validation results to preflight_validation.json\"\"\"\n627 |         if output_path is None:\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/knowledge/preflight_validator.py:641:35\n    |\n641 | def run_preflight_validation() -> Dict[str, Any]:\n    |                                   ^^^^\n642 |     \"\"\"Convenience function to run complete preflight validation\"\"\"\n643 |     validator = KStagePreflightValidator()\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/knowledge/preflight_validator.py:641:45\n    |\n641 | def run_preflight_validation() -> Dict[str, Any]:\n    |                                             ^^^\n642 |     \"\"\"Convenience function to run complete preflight validation\"\"\"\n643 |     validator = KStagePreflightValidator()\n    |\n\nF821 Undefined name `Path`\n   --> canonical_flow/knowledge/preflight_validator.py:676:27\n    |\n674 |         output_path = None\n675 |         if args.output:\n676 |             output_path = Path(args.output)\n    |                           ^^^^\n677 |         \n678 |         saved_path = validator.save_validation_results(results, output_path)\n    |\n\nF541 [*] f-string without any placeholders\n   --> canonical_flow/knowledge/preflight_validator.py:681:15\n    |\n680 |         # Print summary\n681 |         print(f\"\\nK-Stage Preflight Validation Results:\")\n    |               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n682 |         print(f\"Overall Status: {results['overall_status']}\")\n683 |         print(f\"Execution Time: {results['execution_time_seconds']:.2f}s\")\n    |\nhelp: Remove extraneous `f` prefix\n\ninvalid-syntax: Unexpected indentation\n  --> canonical_flow/mathematical_enhancers/__init__.py:28:1\n   |\n27 | # # # from .mathematical_pipeline_coordinator import (  # Module not found  # Module not found  # Module not found\n28 |     MathematicalPipelineCoordinator,\n   | ^^^^\n29 |     create_mathematical_pipeline_coordinator,\n30 |     StageResult,\n   |\n\ninvalid-syntax: Expected a statement\n  --> canonical_flow/mathematical_enhancers/__init__.py:32:1\n   |\n30 |     StageResult,\n31 |     ValidationResult\n32 | )\n   | ^\n33 |\n34 | # # # from .mathematical_compatibility_matrix import (  # Module not found  # Module not found  # Module not found\n   |\n\ninvalid-syntax: Expected a statement\n  --> canonical_flow/mathematical_enhancers/__init__.py:32:2\n   |\n30 |     StageResult,\n31 |     ValidationResult\n32 | )\n   |  ^\n33 |\n34 | # # # from .mathematical_compatibility_matrix import (  # Module not found  # Module not found  # Module not found\n   |\n\ninvalid-syntax: Unexpected indentation\n  --> canonical_flow/mathematical_enhancers/__init__.py:35:1\n   |\n34 | # # # from .mathematical_compatibility_matrix import (  # Module not found  # Module not found  # Module not found\n35 |     MathematicalCompatibilityMatrix,\n   | ^^^^\n36 |     CompatibilityResult\n37 | )\n   |\n\ninvalid-syntax: Expected a statement\n  --> canonical_flow/mathematical_enhancers/__init__.py:37:1\n   |\n35 |     MathematicalCompatibilityMatrix,\n36 |     CompatibilityResult\n37 | )\n   | ^\n38 |\n39 | # Import all mathematical enhancer stages\n   |\n\ninvalid-syntax: Expected a statement\n  --> canonical_flow/mathematical_enhancers/__init__.py:37:2\n   |\n35 |     MathematicalCompatibilityMatrix,\n36 |     CompatibilityResult\n37 | )\n   |  ^\n38 |\n39 | # Import all mathematical enhancer stages\n   |\n\ninvalid-syntax: Expected an indented block after `try` statement\n  --> canonical_flow/mathematical_enhancers/__init__.py:42:1\n   |\n40 | try:\n41 | # # #     from .math_stage01_ingestion_enhancer import MathStage1IngestionEnhancer  # Module not found  # Module not found  # Module n\u2026\n42 | except ImportError:\n   | ^^^^^^\n43 |     MathStage1IngestionEnhancer = None\n44 | try:\n   |\n\ninvalid-syntax: Expected a statement\n  --> canonical_flow/mathematical_enhancers/__init__.py:46:43\n   |\n44 | try:\n45 | # # #     from .math_stage02_context_enhancer import (  # Module not found  # Module not found  # Module not found\n46 |         MathematicalStage2ContextEnhancer as MathStage2ContextEnhancer,\n   |                                           ^^\n47 |         TopologicalQuantumFieldTheoryEnhancer,\n48 |         WilsonLoopOperator,\n   |\n\ninvalid-syntax: unindent does not match any outer indentation level\n  --> canonical_flow/mathematical_enhancers/__init__.py:52:1\n   |\n50 |         ChernSimonsAction,\n51 |         TQFTContextFunctor\n52 |     )  \n   | ^^^^\n53 | except ImportError:\n54 |     MathStage2ContextEnhancer = None\n   |\n\ninvalid-syntax: Expected a statement\n  --> canonical_flow/mathematical_enhancers/__init__.py:52:5\n   |\n50 |         ChernSimonsAction,\n51 |         TQFTContextFunctor\n52 |     )  \n   |     ^\n53 | except ImportError:\n54 |     MathStage2ContextEnhancer = None\n   |\n\ninvalid-syntax: Expected a statement\n  --> canonical_flow/mathematical_enhancers/__init__.py:52:8\n   |\n50 |         ChernSimonsAction,\n51 |         TQFTContextFunctor\n52 |     )  \n   |        ^\n53 | except ImportError:\n54 |     MathStage2ContextEnhancer = None\n   |\n\ninvalid-syntax: Expected a statement\n  --> canonical_flow/mathematical_enhancers/__init__.py:53:1\n   |\n51 |         TQFTContextFunctor\n52 |     )  \n53 | except ImportError:\n   | ^^^^^^\n54 |     MathStage2ContextEnhancer = None\n55 |     TopologicalQuantumFieldTheoryEnhancer = None\n   |\n\ninvalid-syntax: Expected an expression\n  --> canonical_flow/mathematical_enhancers/__init__.py:53:20\n   |\n51 |         TQFTContextFunctor\n52 |     )  \n53 | except ImportError:\n   |                    ^\n54 |     MathStage2ContextEnhancer = None\n55 |     TopologicalQuantumFieldTheoryEnhancer = None\n   |\n\ninvalid-syntax: Unexpected indentation\n  --> canonical_flow/mathematical_enhancers/__init__.py:54:1\n   |\n52 |     )  \n53 | except ImportError:\n54 |     MathStage2ContextEnhancer = None\n   | ^^^^\n55 |     TopologicalQuantumFieldTheoryEnhancer = None\n56 |     WilsonLoopOperator = None\n   |\n\ninvalid-syntax: Expected `except` or `finally` after `try` block\n  --> canonical_flow/mathematical_enhancers/__init__.py:61:1\n   |\n59 |     TQFTContextFunctor = None\n60 |\n61 | try:\n   | ^^^\n62 | # # #     from .math_stage03_knowledge_enhancer import MathStage3KnowledgeEnhancer  # Module not found  # Module not found  # Module n\u2026\n63 | except ImportError:\n   |\n\ninvalid-syntax: Expected an indented block after `try` statement\n  --> canonical_flow/mathematical_enhancers/__init__.py:63:1\n   |\n61 | try:\n62 | # # #     from .math_stage03_knowledge_enhancer import MathStage3KnowledgeEnhancer  # Module not found  # Module not found  # Module n\u2026\n63 | except ImportError:\n   | ^^^^^^\n64 |     MathStage3KnowledgeEnhancer = None\n   |\n\ninvalid-syntax: Expected an indented block after `try` statement\n  --> canonical_flow/mathematical_enhancers/__init__.py:68:1\n   |\n66 | try:\n67 | # # #     from .math_stage04_analysis_enhancer import MathStage4AnalysisEnhancer  # Module not found  # Module not found  # Module not\u2026\n68 | except ImportError:\n   | ^^^^^^\n69 |     MathStage4AnalysisEnhancer = None\n   |\n\ninvalid-syntax: Expected an indented block after `try` statement\n  --> canonical_flow/mathematical_enhancers/__init__.py:73:1\n   |\n71 | try:\n72 | # # #     from .math_stage05_scoring_enhancer import MathStage5ScoringEnhancer  # Module not found  # Module not found  # Module not f\u2026\n73 | except ImportError:\n   | ^^^^^^\n74 |     MathStage5ScoringEnhancer = None\n   |\n\ninvalid-syntax: Expected an indented block after `try` statement\n  --> canonical_flow/mathematical_enhancers/__init__.py:78:1\n   |\n76 | try:\n77 | # # #     from .math_stage06_retrieval_enhancer import MathStage6RetrievalEnhancer  # Module not found  # Module not found  # Module n\u2026\n78 | except ImportError:\n   | ^^^^^^\n79 |     MathStage6RetrievalEnhancer = None\n   |\n\ninvalid-syntax: Expected an indented block after `try` statement\n  --> canonical_flow/mathematical_enhancers/__init__.py:83:1\n   |\n81 | try:\n82 | # # #     from .math_stage07_orchestration_enhancer import MathStage7OrchestrationEnhancer  # Module not found  # Module not found  # \u2026\n83 | except ImportError:\n   | ^^^^^^\n84 |     MathStage7OrchestrationEnhancer = None\n   |\n\ninvalid-syntax: Expected an indented block after `try` statement\n  --> canonical_flow/mathematical_enhancers/__init__.py:88:1\n   |\n86 | try:\n87 | # # #     from .math_stage11_aggregation_enhancer import MathStage11AggregationEnhancer  # Module not found  # Module not found  # Mod\u2026\n88 | except ImportError:\n   | ^^^^^^\n89 |     MathStage11AggregationEnhancer = None\n   |\n\ninvalid-syntax: Expected an indented block after `try` statement\n  --> canonical_flow/mathematical_enhancers/__init__.py:93:1\n   |\n91 | try:\n92 | # # #     from .math_stage12_integration_enhancer import MathStage12IntegrationEnhancer  # Module not found  # Module not found  # Mod\u2026\n93 | except ImportError:\n   | ^^^^^^\n94 |     MathStage12IntegrationEnhancer = None\n   |\n\nF401 [*] `warnings` imported but unused\n  --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:28:8\n   |\n26 | # # # from scipy import stats  # Module not found  # Module not found  # Module not found\n27 | # # # from scipy.optimize import minimize  # Module not found  # Module not found  # Module not found\n28 | import warnings\n   |        ^^^^^^^^\n29 | # # # from typing import Callable, Complex  # Module not found  # Module not found  # Module not found\n30 | # # # from enum import Enum  # Module not found  # Module not found  # Module not found\n   |\nhelp: Remove unused import: `warnings`\n\nF401 [*] `itertools` imported but unused\n  --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:31:8\n   |\n29 | # # # from typing import Callable, Complex  # Module not found  # Module not found  # Module not found\n30 | # # # from enum import Enum  # Module not found  # Module not found  # Module not found\n31 | import itertools\n   |        ^^^^^^^^^\n32 |\n33 | # Import the base meso aggregator\n   |\nhelp: Remove unused import: `itertools`\n\nF821 Undefined name `dataclass`\n  --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:38:2\n   |\n36 | logger = logging.getLogger(__name__)\n37 |\n38 | @dataclass\n   |  ^^^^^^^^^\n39 | class ProbabilitySpace:\n40 |     \"\"\"Formal probability space (\u03a9, F, P) for aggregation analysis\"\"\"\n   |\n\nF821 Undefined name `Dict`\n  --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:41:19\n   |\n39 | class ProbabilitySpace:\n40 |     \"\"\"Formal probability space (\u03a9, F, P) for aggregation analysis\"\"\"\n41 |     sample_space: Dict[str, Any] = field(default_factory=dict)\n   |                   ^^^^\n42 |     sigma_algebra: List[str] = field(default_factory=list) \n43 |     probability_measure: Dict[str, float] = field(default_factory=dict)\n   |\n\nF821 Undefined name `Any`\n  --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:41:29\n   |\n39 | class ProbabilitySpace:\n40 |     \"\"\"Formal probability space (\u03a9, F, P) for aggregation analysis\"\"\"\n41 |     sample_space: Dict[str, Any] = field(default_factory=dict)\n   |                             ^^^\n42 |     sigma_algebra: List[str] = field(default_factory=list) \n43 |     probability_measure: Dict[str, float] = field(default_factory=dict)\n   |\n\nF821 Undefined name `field`\n  --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:41:36\n   |\n39 | class ProbabilitySpace:\n40 |     \"\"\"Formal probability space (\u03a9, F, P) for aggregation analysis\"\"\"\n41 |     sample_space: Dict[str, Any] = field(default_factory=dict)\n   |                                    ^^^^^\n42 |     sigma_algebra: List[str] = field(default_factory=list) \n43 |     probability_measure: Dict[str, float] = field(default_factory=dict)\n   |\n\nF821 Undefined name `List`\n  --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:42:20\n   |\n40 |     \"\"\"Formal probability space (\u03a9, F, P) for aggregation analysis\"\"\"\n41 |     sample_space: Dict[str, Any] = field(default_factory=dict)\n42 |     sigma_algebra: List[str] = field(default_factory=list) \n   |                    ^^^^\n43 |     probability_measure: Dict[str, float] = field(default_factory=dict)\n   |\n\nF821 Undefined name `field`\n  --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:42:32\n   |\n40 |     \"\"\"Formal probability space (\u03a9, F, P) for aggregation analysis\"\"\"\n41 |     sample_space: Dict[str, Any] = field(default_factory=dict)\n42 |     sigma_algebra: List[str] = field(default_factory=list) \n   |                                ^^^^^\n43 |     probability_measure: Dict[str, float] = field(default_factory=dict)\n   |\n\nF821 Undefined name `Dict`\n  --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:43:26\n   |\n41 |     sample_space: Dict[str, Any] = field(default_factory=dict)\n42 |     sigma_algebra: List[str] = field(default_factory=list) \n43 |     probability_measure: Dict[str, float] = field(default_factory=dict)\n   |                          ^^^^\n44 |     \n45 |     def __post_init__(self):\n   |\n\nF821 Undefined name `field`\n  --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:43:45\n   |\n41 |     sample_space: Dict[str, Any] = field(default_factory=dict)\n42 |     sigma_algebra: List[str] = field(default_factory=list) \n43 |     probability_measure: Dict[str, float] = field(default_factory=dict)\n   |                                             ^^^^^\n44 |     \n45 |     def __post_init__(self):\n   |\n\nF821 Undefined name `dataclass`\n  --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:52:2\n   |\n50 |                 logger.warning(f\"Probability measure sums to {total_measure:.6f}, not 1.0\")\n51 |\n52 | @dataclass \n   |  ^^^^^^^^^\n53 | class ConvergenceResult:\n54 | # # #     \"\"\"Results from convergence analysis\"\"\"  # Module not found  # Module not found  # Module not found\n   |\n\nF821 Undefined name `Tuple`\n  --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:60:26\n   |\n58 |     convergence_rate: float\n59 |     stability_bound: float\n60 |     confidence_interval: Tuple[float, float]\n   |                          ^^^^^\n61 |\n62 | @dataclass\n   |\n\nF821 Undefined name `dataclass`\n  --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:62:2\n   |\n60 |     confidence_interval: Tuple[float, float]\n61 |\n62 | @dataclass\n   |  ^^^^^^^^^\n63 | class StatisticalValidation:\n64 |     \"\"\"Statistical validation results for aggregation\"\"\"\n   |\n\nF821 Undefined name `Dict`\n  --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:65:29\n   |\n63 | class StatisticalValidation:\n64 |     \"\"\"Statistical validation results for aggregation\"\"\"\n65 |     hypothesis_test_result: Dict[str, Any]\n   |                             ^^^^\n66 |     concentration_bound: float\n67 |     coverage_probability: float\n   |\n\nF821 Undefined name `Any`\n  --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:65:39\n   |\n63 | class StatisticalValidation:\n64 |     \"\"\"Statistical validation results for aggregation\"\"\"\n65 |     hypothesis_test_result: Dict[str, Any]\n   |                                       ^^^\n66 |     concentration_bound: float\n67 |     coverage_probability: float\n   |\n\nF821 Undefined name `Dict`\n  --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:68:19\n   |\n66 |     concentration_bound: float\n67 |     coverage_probability: float\n68 |     mcnemar_test: Dict[str, Any]\n   |                   ^^^^\n69 |     cluster_homogeneity_test: Dict[str, Any]\n70 |     aggregation_quality_score: float\n   |\n\nF821 Undefined name `Any`\n  --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:68:29\n   |\n66 |     concentration_bound: float\n67 |     coverage_probability: float\n68 |     mcnemar_test: Dict[str, Any]\n   |                             ^^^\n69 |     cluster_homogeneity_test: Dict[str, Any]\n70 |     aggregation_quality_score: float\n   |\n\nF821 Undefined name `Dict`\n  --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:69:31\n   |\n67 |     coverage_probability: float\n68 |     mcnemar_test: Dict[str, Any]\n69 |     cluster_homogeneity_test: Dict[str, Any]\n   |                               ^^^^\n70 |     aggregation_quality_score: float\n   |\n\nF821 Undefined name `Any`\n  --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:69:41\n   |\n67 |     coverage_probability: float\n68 |     mcnemar_test: Dict[str, Any]\n69 |     cluster_homogeneity_test: Dict[str, Any]\n   |                                         ^^^\n70 |     aggregation_quality_score: float\n   |\n\nF821 Undefined name `dataclass`\n  --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:72:2\n   |\n70 |     aggregation_quality_score: float\n71 |\n72 | @dataclass\n   |  ^^^^^^^^^\n73 | class MeasureTheoreticSummary:\n74 |     \"\"\"Enhanced aggregation summary with mathematical rigor\"\"\"\n   |\n\nF821 Undefined name `Dict`\n  --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:75:23\n   |\n73 | class MeasureTheoreticSummary:\n74 |     \"\"\"Enhanced aggregation summary with mathematical rigor\"\"\"\n75 |     original_summary: Dict[str, Any]\n   |                       ^^^^\n76 |     probability_space: ProbabilitySpace\n77 |     convergence_analysis: ConvergenceResult\n   |\n\nF821 Undefined name `Any`\n  --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:75:33\n   |\n73 | class MeasureTheoreticSummary:\n74 |     \"\"\"Enhanced aggregation summary with mathematical rigor\"\"\"\n75 |     original_summary: Dict[str, Any]\n   |                                 ^^^\n76 |     probability_space: ProbabilitySpace\n77 |     convergence_analysis: ConvergenceResult\n   |\n\nF821 Undefined name `Dict`\n  --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:79:29\n   |\n77 |     convergence_analysis: ConvergenceResult\n78 |     statistical_validation: StatisticalValidation\n79 |     stability_certificates: Dict[str, Any]\n   |                             ^^^^\n80 |     uncertainty_quantification: Dict[str, Any]\n   |\n\nF821 Undefined name `Any`\n  --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:79:39\n   |\n77 |     convergence_analysis: ConvergenceResult\n78 |     statistical_validation: StatisticalValidation\n79 |     stability_certificates: Dict[str, Any]\n   |                                       ^^^\n80 |     uncertainty_quantification: Dict[str, Any]\n   |\n\nF821 Undefined name `Dict`\n  --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:80:33\n   |\n78 |     statistical_validation: StatisticalValidation\n79 |     stability_certificates: Dict[str, Any]\n80 |     uncertainty_quantification: Dict[str, Any]\n   |                                 ^^^^\n   |\n\nF821 Undefined name `Any`\n  --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:80:43\n   |\n78 |     statistical_validation: StatisticalValidation\n79 |     stability_certificates: Dict[str, Any]\n80 |     uncertainty_quantification: Dict[str, Any]\n   |                                           ^^^\n   |\n\nF821 Undefined name `Enum`\n  --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:83:17\n   |\n83 | class AnyonType(Enum):\n   |                 ^^^^\n84 |     \"\"\"Non-Abelian anyon types for topological quantum computation\"\"\"\n85 |     IDENTITY = \"I\"\n   |\n\nF821 Undefined name `List`\n  --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:92:38\n   |\n90 | class TopologicalState:\n91 |     \"\"\"Topological quantum state representation\"\"\"\n92 |     def __init__(self, anyon_config: List[AnyonType], hilbert_dim: int = 2):\n   |                                      ^^^^\n93 |         self.anyon_config = anyon_config\n94 |         self.hilbert_dim = hilbert_dim\n   |\n\nF821 Undefined name `dataclass`\n   --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:159:2\n    |\n159 | @dataclass\n    |  ^^^^^^^^^\n160 | class QuantumMemoryRegister:\n161 |     \"\"\"Quantum memory register storing aggregation patterns as topological states\"\"\"\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:163:25\n    |\n161 |     \"\"\"Quantum memory register storing aggregation patterns as topological states\"\"\"\n162 |     register_id: str\n163 |     topological_states: List[TopologicalState] = field(default_factory=list)\n    |                         ^^^^\n164 |     coherence_time: float = 1000.0  # Topological protection time\n165 |     error_threshold: float = 1e-6\n    |\n\nF821 Undefined name `field`\n   --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:163:50\n    |\n161 |     \"\"\"Quantum memory register storing aggregation patterns as topological states\"\"\"\n162 |     register_id: str\n163 |     topological_states: List[TopologicalState] = field(default_factory=list)\n    |                                                  ^^^^^\n164 |     coherence_time: float = 1000.0  # Topological protection time\n165 |     error_threshold: float = 1e-6\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:167:38\n    |\n165 |     error_threshold: float = 1e-6\n166 |     \n167 |     def store_pattern(self, pattern: Dict[str, Any]) -> None:\n    |                                      ^^^^\n168 |         \"\"\"Store aggregation pattern as topological quantum state\"\"\"\n169 |         # Convert pattern to anyon configuration\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:167:48\n    |\n165 |     error_threshold: float = 1e-6\n166 |     \n167 |     def store_pattern(self, pattern: Dict[str, Any]) -> None:\n    |                                                ^^^\n168 |         \"\"\"Store aggregation pattern as topological quantum state\"\"\"\n169 |         # Convert pattern to anyon configuration\n    |\n\nF821 Undefined name `Optional`\n   --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:187:47\n    |\n185 |             self.topological_states.append(topo_state)\n186 |     \n187 |     def retrieve_pattern(self, index: int) -> Optional[TopologicalState]:\n    |                                               ^^^^^^^^\n188 |         \"\"\"Retrieve stored topological state\"\"\"\n189 |         if 0 <= index < len(self.topological_states):\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:236:32\n    |\n235 |         # Anyon braiding history for learning persistence\n236 |         self.braiding_history: List[Tuple[int, int, bool]] = []\n    |                                ^^^^\n237 |         \n238 |         # Statistical learning parameters using non-Abelian statistics\n    |\n\nF821 Undefined name `Tuple`\n   --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:236:37\n    |\n235 |         # Anyon braiding history for learning persistence\n236 |         self.braiding_history: List[Tuple[int, int, bool]] = []\n    |                                     ^^^^^\n237 |         \n238 |         # Statistical learning parameters using non-Abelian statistics\n    |\n\nF821 Undefined name `Callable`\n   --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:252:36\n    |\n250 |     def enhance_aggregation(\n251 |         self, \n252 |         original_aggregation_func: Callable[[Dict[str, Any]], Dict[str, Any]]\n    |                                    ^^^^^^^^\n253 |     ) -> Callable[[Dict[str, Any]], Dict[str, Any]]:\n254 |         \"\"\"\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:252:46\n    |\n250 |     def enhance_aggregation(\n251 |         self, \n252 |         original_aggregation_func: Callable[[Dict[str, Any]], Dict[str, Any]]\n    |                                              ^^^^\n253 |     ) -> Callable[[Dict[str, Any]], Dict[str, Any]]:\n254 |         \"\"\"\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:252:56\n    |\n250 |     def enhance_aggregation(\n251 |         self, \n252 |         original_aggregation_func: Callable[[Dict[str, Any]], Dict[str, Any]]\n    |                                                        ^^^\n253 |     ) -> Callable[[Dict[str, Any]], Dict[str, Any]]:\n254 |         \"\"\"\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:252:63\n    |\n250 |     def enhance_aggregation(\n251 |         self, \n252 |         original_aggregation_func: Callable[[Dict[str, Any]], Dict[str, Any]]\n    |                                                               ^^^^\n253 |     ) -> Callable[[Dict[str, Any]], Dict[str, Any]]:\n254 |         \"\"\"\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:252:73\n    |\n250 |     def enhance_aggregation(\n251 |         self, \n252 |         original_aggregation_func: Callable[[Dict[str, Any]], Dict[str, Any]]\n    |                                                                         ^^^\n253 |     ) -> Callable[[Dict[str, Any]], Dict[str, Any]]:\n254 |         \"\"\"\n    |\n\nF821 Undefined name `Callable`\n   --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:253:10\n    |\n251 |         self, \n252 |         original_aggregation_func: Callable[[Dict[str, Any]], Dict[str, Any]]\n253 |     ) -> Callable[[Dict[str, Any]], Dict[str, Any]]:\n    |          ^^^^^^^^\n254 |         \"\"\"\n255 |         Enhance existing aggregation function with topological quantum learning\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:253:20\n    |\n251 |         self, \n252 |         original_aggregation_func: Callable[[Dict[str, Any]], Dict[str, Any]]\n253 |     ) -> Callable[[Dict[str, Any]], Dict[str, Any]]:\n    |                    ^^^^\n254 |         \"\"\"\n255 |         Enhance existing aggregation function with topological quantum learning\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:253:30\n    |\n251 |         self, \n252 |         original_aggregation_func: Callable[[Dict[str, Any]], Dict[str, Any]]\n253 |     ) -> Callable[[Dict[str, Any]], Dict[str, Any]]:\n    |                              ^^^\n254 |         \"\"\"\n255 |         Enhance existing aggregation function with topological quantum learning\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:253:37\n    |\n251 |         self, \n252 |         original_aggregation_func: Callable[[Dict[str, Any]], Dict[str, Any]]\n253 |     ) -> Callable[[Dict[str, Any]], Dict[str, Any]]:\n    |                                     ^^^^\n254 |         \"\"\"\n255 |         Enhance existing aggregation function with topological quantum learning\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:253:47\n    |\n251 |         self, \n252 |         original_aggregation_func: Callable[[Dict[str, Any]], Dict[str, Any]]\n253 |     ) -> Callable[[Dict[str, Any]], Dict[str, Any]]:\n    |                                               ^^^\n254 |         \"\"\"\n255 |         Enhance existing aggregation function with topological quantum learning\n    |\n\nF821 Undefined name `wraps`\n   --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:263:10\n    |\n261 |             Enhanced aggregation function with quantum memory and learning\n262 |         \"\"\"\n263 |         @wraps(original_aggregation_func)\n    |          ^^^^^\n264 |         def quantum_enhanced_aggregation(data: Dict[str, Any]) -> Dict[str, Any]:\n265 |             # Store current aggregation pattern in quantum memory\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:264:48\n    |\n262 |         \"\"\"\n263 |         @wraps(original_aggregation_func)\n264 |         def quantum_enhanced_aggregation(data: Dict[str, Any]) -> Dict[str, Any]:\n    |                                                ^^^^\n265 |             # Store current aggregation pattern in quantum memory\n266 |             self._store_aggregation_pattern(data)\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:264:58\n    |\n262 |         \"\"\"\n263 |         @wraps(original_aggregation_func)\n264 |         def quantum_enhanced_aggregation(data: Dict[str, Any]) -> Dict[str, Any]:\n    |                                                          ^^^\n265 |             # Store current aggregation pattern in quantum memory\n266 |             self._store_aggregation_pattern(data)\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:264:67\n    |\n262 |         \"\"\"\n263 |         @wraps(original_aggregation_func)\n264 |         def quantum_enhanced_aggregation(data: Dict[str, Any]) -> Dict[str, Any]:\n    |                                                                   ^^^^\n265 |             # Store current aggregation pattern in quantum memory\n266 |             self._store_aggregation_pattern(data)\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:264:77\n    |\n262 |         \"\"\"\n263 |         @wraps(original_aggregation_func)\n264 |         def quantum_enhanced_aggregation(data: Dict[str, Any]) -> Dict[str, Any]:\n    |                                                                             ^^^\n265 |             # Store current aggregation pattern in quantum memory\n266 |             self._store_aggregation_pattern(data)\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:284:48\n    |\n282 |         return quantum_enhanced_aggregation\n283 |     \n284 |     def _store_aggregation_pattern(self, data: Dict[str, Any]) -> None:\n    |                                                ^^^^\n285 |         \"\"\"Store aggregation pattern in topological quantum memory\"\"\"\n286 | # # #         # Extract pattern features from aggregation data  # Module not found  # Module not found  # Module not found\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:284:58\n    |\n282 |         return quantum_enhanced_aggregation\n283 |     \n284 |     def _store_aggregation_pattern(self, data: Dict[str, Any]) -> None:\n    |                                                          ^^^\n285 |         \"\"\"Store aggregation pattern in topological quantum memory\"\"\"\n286 | # # #         # Extract pattern features from aggregation data  # Module not found  # Module not found  # Module not found\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:310:52\n    |\n308 |         self.memory_registers[register_key].store_pattern(pattern)\n309 |         \n310 |     def _apply_topological_enhancement(self, data: Dict[str, Any]) -> Dict[str, Any]:\n    |                                                    ^^^^\n311 |         \"\"\"Apply topological enhancement to aggregation data\"\"\"\n312 |         enhanced_data = data.copy()\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:310:62\n    |\n308 |         self.memory_registers[register_key].store_pattern(pattern)\n309 |         \n310 |     def _apply_topological_enhancement(self, data: Dict[str, Any]) -> Dict[str, Any]:\n    |                                                              ^^^\n311 |         \"\"\"Apply topological enhancement to aggregation data\"\"\"\n312 |         enhanced_data = data.copy()\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:310:71\n    |\n308 |         self.memory_registers[register_key].store_pattern(pattern)\n309 |         \n310 |     def _apply_topological_enhancement(self, data: Dict[str, Any]) -> Dict[str, Any]:\n    |                                                                       ^^^^\n311 |         \"\"\"Apply topological enhancement to aggregation data\"\"\"\n312 |         enhanced_data = data.copy()\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:310:81\n    |\n308 |         self.memory_registers[register_key].store_pattern(pattern)\n309 |         \n310 |     def _apply_topological_enhancement(self, data: Dict[str, Any]) -> Dict[str, Any]:\n    |                                                                                 ^^^\n311 |         \"\"\"Apply topological enhancement to aggregation data\"\"\"\n312 |         enhanced_data = data.copy()\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:328:49\n    |\n326 |         return enhanced_data\n327 |         \n328 |     def _retrieve_relevant_patterns(self, data: Dict[str, Any]) -> List[TopologicalState]:\n    |                                                 ^^^^\n329 | # # #         \"\"\"Retrieve relevant patterns from quantum memory using similarity matching\"\"\"  # Module not found  # Module not found \u2026\n330 |         relevant_states = []\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:328:59\n    |\n326 |         return enhanced_data\n327 |         \n328 |     def _retrieve_relevant_patterns(self, data: Dict[str, Any]) -> List[TopologicalState]:\n    |                                                           ^^^\n329 | # # #         \"\"\"Retrieve relevant patterns from quantum memory using similarity matching\"\"\"  # Module not found  # Module not found \u2026\n330 |         relevant_states = []\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:328:68\n    |\n326 |         return enhanced_data\n327 |         \n328 |     def _retrieve_relevant_patterns(self, data: Dict[str, Any]) -> List[TopologicalState]:\n    |                                                                    ^^^^\n329 | # # #         \"\"\"Retrieve relevant patterns from quantum memory using similarity matching\"\"\"  # Module not found  # Module not found \u2026\n330 |         relevant_states = []\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:348:45\n    |\n346 |         return relevant_states[:self.braiding_complexity]  # Limit complexity\n347 |     \n348 |     def _extract_data_signature(self, data: Dict[str, Any]) -> TopologicalState:\n    |                                             ^^^^\n349 | # # #         \"\"\"Extract topological signature from current data\"\"\"  # Module not found  # Module not found  # Module not found\n350 |         signature_config = [AnyonType.IDENTITY]  # Start with identity\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:348:55\n    |\n346 |         return relevant_states[:self.braiding_complexity]  # Limit complexity\n347 |     \n348 |     def _extract_data_signature(self, data: Dict[str, Any]) -> TopologicalState:\n    |                                                       ^^^\n349 | # # #         \"\"\"Extract topological signature from current data\"\"\"  # Module not found  # Module not found  # Module not found\n350 |         signature_config = [AnyonType.IDENTITY]  # Start with identity\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:379:50\n    |\n377 |         return 0.0\n378 |         \n379 |     def _perform_braiding_sequence(self, states: List[TopologicalState]) -> TopologicalState:\n    |                                                  ^^^^\n380 |         \"\"\"Perform braiding sequence on relevant topological states\"\"\"\n381 |         if not states:\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:403:15\n    |\n401 |     def _integrate_braided_information(\n402 |         self, \n403 |         data: Dict[str, Any], \n    |               ^^^^\n404 |         braided_state: TopologicalState\n405 |     ) -> Dict[str, Any]:\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:403:25\n    |\n401 |     def _integrate_braided_information(\n402 |         self, \n403 |         data: Dict[str, Any], \n    |                         ^^^\n404 |         braided_state: TopologicalState\n405 |     ) -> Dict[str, Any]:\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:405:10\n    |\n403 |         data: Dict[str, Any], \n404 |         braided_state: TopologicalState\n405 |     ) -> Dict[str, Any]:\n    |          ^^^^\n406 |         \"\"\"Integrate braided topological information into aggregation data\"\"\"\n407 |         enhanced_data = data.copy()\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:405:20\n    |\n403 |         data: Dict[str, Any], \n404 |         braided_state: TopologicalState\n405 |     ) -> Dict[str, Any]:\n    |                    ^^^\n406 |         \"\"\"Integrate braided topological information into aggregation data\"\"\"\n407 |         enhanced_data = data.copy()\n    |\n\nF841 Local variable `state_norm` is assigned to but never used\n   --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:411:9\n    |\n409 | # # #         # Extract enhancement factors from braided state  # Module not found  # Module not found  # Module not found\n410 |         entanglement = braided_state.measure_entanglement()\n411 |         state_norm = np.linalg.norm(braided_state.state_vector)\n    |         ^^^^^^^^^^\n412 |         \n413 |         # Apply quantum enhancement to cluster data\n    |\nhelp: Remove assignment to unused variable `state_norm`\n\nF821 Undefined name `Dict`\n   --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:433:51\n    |\n431 |         return enhanced_data\n432 |     \n433 |     def _update_learning_parameters(self, result: Dict[str, Any]) -> None:\n    |                                                   ^^^^\n434 |         \"\"\"Update quantum learning parameters using non-Abelian statistics\"\"\"\n435 | # # #         # Extract learning signal from aggregation result  # Module not found  # Module not found  # Module not found\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:433:61\n    |\n431 |         return enhanced_data\n432 |     \n433 |     def _update_learning_parameters(self, result: Dict[str, Any]) -> None:\n    |                                                             ^^^\n434 |         \"\"\"Update quantum learning parameters using non-Abelian statistics\"\"\"\n435 | # # #         # Extract learning signal from aggregation result  # Module not found  # Module not found  # Module not found\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:462:48\n    |\n460 |             self.parameter_evolution_operator = U @ Vh\n461 |     \n462 |     def _extract_learning_signal(self, result: Dict[str, Any]) -> Optional[np.ndarray]:\n    |                                                ^^^^\n463 | # # #         \"\"\"Extract learning signal from aggregation result\"\"\"  # Module not found  # Module not found  # Module not found\n464 |         signals = []\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:462:58\n    |\n460 |             self.parameter_evolution_operator = U @ Vh\n461 |     \n462 |     def _extract_learning_signal(self, result: Dict[str, Any]) -> Optional[np.ndarray]:\n    |                                                          ^^^\n463 | # # #         \"\"\"Extract learning signal from aggregation result\"\"\"  # Module not found  # Module not found  # Module not found\n464 |         signals = []\n    |\n\nF821 Undefined name `Optional`\n   --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:462:67\n    |\n460 |             self.parameter_evolution_operator = U @ Vh\n461 |     \n462 |     def _extract_learning_signal(self, result: Dict[str, Any]) -> Optional[np.ndarray]:\n    |                                                                   ^^^^^^^^\n463 | # # #         \"\"\"Extract learning signal from aggregation result\"\"\"  # Module not found  # Module not found  # Module not found\n464 |         signals = []\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:482:47\n    |\n480 |         return np.array(signals) if signals else None\n481 |         \n482 |     def _apply_anyon_statistics(self, result: Dict[str, Any]) -> Dict[str, Any]:\n    |                                               ^^^^\n483 |         \"\"\"Apply anyon-based statistical enhancement to aggregation result\"\"\"\n484 |         enhanced_result = result.copy()\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:482:57\n    |\n480 |         return np.array(signals) if signals else None\n481 |         \n482 |     def _apply_anyon_statistics(self, result: Dict[str, Any]) -> Dict[str, Any]:\n    |                                                         ^^^\n483 |         \"\"\"Apply anyon-based statistical enhancement to aggregation result\"\"\"\n484 |         enhanced_result = result.copy()\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:482:66\n    |\n480 |         return np.array(signals) if signals else None\n481 |         \n482 |     def _apply_anyon_statistics(self, result: Dict[str, Any]) -> Dict[str, Any]:\n    |                                                                  ^^^^\n483 |         \"\"\"Apply anyon-based statistical enhancement to aggregation result\"\"\"\n484 |         enhanced_result = result.copy()\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:482:76\n    |\n480 |         return np.array(signals) if signals else None\n481 |         \n482 |     def _apply_anyon_statistics(self, result: Dict[str, Any]) -> Dict[str, Any]:\n    |                                                                            ^^^\n483 |         \"\"\"Apply anyon-based statistical enhancement to aggregation result\"\"\"\n484 |         enhanced_result = result.copy()\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:512:49\n    |\n510 |         return enhanced_result\n511 |         \n512 |     def _enhance_items_with_anyons(self, items: Dict[str, Any]) -> Dict[str, Any]:\n    |                                                 ^^^^\n513 |         \"\"\"Enhance individual items using anyonic statistical mechanics\"\"\"\n514 |         enhanced_items = {}\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:512:59\n    |\n510 |         return enhanced_result\n511 |         \n512 |     def _enhance_items_with_anyons(self, items: Dict[str, Any]) -> Dict[str, Any]:\n    |                                                           ^^^\n513 |         \"\"\"Enhance individual items using anyonic statistical mechanics\"\"\"\n514 |         enhanced_items = {}\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:512:68\n    |\n510 |         return enhanced_result\n511 |         \n512 |     def _enhance_items_with_anyons(self, items: Dict[str, Any]) -> Dict[str, Any]:\n    |                                                                    ^^^^\n513 |         \"\"\"Enhance individual items using anyonic statistical mechanics\"\"\"\n514 |         enhanced_items = {}\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:512:78\n    |\n510 |         return enhanced_result\n511 |         \n512 |     def _enhance_items_with_anyons(self, items: Dict[str, Any]) -> Dict[str, Any]:\n    |                                                                              ^^^\n513 |         \"\"\"Enhance individual items using anyonic statistical mechanics\"\"\"\n514 |         enhanced_items = {}\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:555:44\n    |\n553 |         return float(correction * 0.1)  # Scale correction to be conservative\n554 |     \n555 |     def get_quantum_state_summary(self) -> Dict[str, Any]:\n    |                                            ^^^^\n556 |         \"\"\"Get summary of current quantum enhancement state\"\"\"\n557 |         total_states = sum(\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:555:54\n    |\n553 |         return float(correction * 0.1)  # Scale correction to be conservative\n554 |     \n555 |     def get_quantum_state_summary(self) -> Dict[str, Any]:\n    |                                                      ^^^\n556 |         \"\"\"Get summary of current quantum enhancement state\"\"\"\n557 |         total_states = sum(\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:614:23\n    |\n612 |     def construct_probability_space(\n613 |         self, \n614 |         cluster_data: Dict[str, Any]\n    |                       ^^^^\n615 |     ) -> ProbabilitySpace:\n616 |         \"\"\"\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:614:33\n    |\n612 |     def construct_probability_space(\n613 |         self, \n614 |         cluster_data: Dict[str, Any]\n    |                                 ^^^\n615 |     ) -> ProbabilitySpace:\n616 |         \"\"\"\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:654:30\n    |\n652 |     def analyze_convergence(\n653 |         self, \n654 |         divergence_sequence: List[float],\n    |                              ^^^^\n655 |         score_sequences: Dict[str, List[float]]\n656 |     ) -> ConvergenceResult:\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:655:26\n    |\n653 |         self, \n654 |         divergence_sequence: List[float],\n655 |         score_sequences: Dict[str, List[float]]\n    |                          ^^^^\n656 |     ) -> ConvergenceResult:\n657 |         \"\"\"\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:655:36\n    |\n653 |         self, \n654 |         divergence_sequence: List[float],\n655 |         score_sequences: Dict[str, List[float]]\n    |                                    ^^^^\n656 |     ) -> ConvergenceResult:\n657 |         \"\"\"\n    |\n\nF841 Local variable `variance` is assigned to but never used\n   --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:709:17\n    |\n707 |             if len(all_scores) > 1:\n708 |                 n = len(all_scores)\n709 |                 variance = np.var(all_scores)\n    |                 ^^^^^^^^\n710 |                 # Assume scores are bounded in [0, 1]\n711 |                 t = np.sqrt(-np.log(self.stability_delta / 2) / (2 * n))\n    |\nhelp: Remove assignment to unused variable `variance`\n\nF821 Undefined name `Dict`\n   --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:739:23\n    |\n737 |     def validate_statistically(\n738 |         self,\n739 |         meso_summary: Dict[str, Any],\n    |                       ^^^^\n740 |         coverage_matrix: Dict[str, Any]\n741 |     ) -> StatisticalValidation:\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:739:33\n    |\n737 |     def validate_statistically(\n738 |         self,\n739 |         meso_summary: Dict[str, Any],\n    |                                 ^^^\n740 |         coverage_matrix: Dict[str, Any]\n741 |     ) -> StatisticalValidation:\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:740:26\n    |\n738 |         self,\n739 |         meso_summary: Dict[str, Any],\n740 |         coverage_matrix: Dict[str, Any]\n    |                          ^^^^\n741 |     ) -> StatisticalValidation:\n742 |         \"\"\"\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:740:36\n    |\n738 |         self,\n739 |         meso_summary: Dict[str, Any],\n740 |         coverage_matrix: Dict[str, Any]\n    |                                    ^^^\n741 |     ) -> StatisticalValidation:\n742 |         \"\"\"\n    |\n\nF821 Undefined name `stats`\n   --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:779:35\n    |\n777 |         if len(cluster_score_lists) >= 2:\n778 |             try:\n779 |                 f_stat, p_value = stats.f_oneway(*cluster_score_lists)\n    |                                   ^^^^^\n780 |                 cluster_homogeneity_test = {\n781 |                     \"f_statistic\": float(f_stat),\n    |\n\nF821 Undefined name `stats`\n   --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:807:39\n    |\n805 |                 # Kolmogorov-Smirnov test against exponential distribution\n806 |                 js_array = np.array(js_divergences)\n807 |                 ks_stat, ks_p_value = stats.kstest(js_array, 'expon')\n    |                                       ^^^^^\n808 |                 hypothesis_test_result = {\n809 |                     \"ks_statistic\": float(ks_stat),\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:889:23\n    |\n887 |     def enhance_with_topological_quantum_learning(\n888 |         self,\n889 |         cluster_data: Dict[str, Any],\n    |                       ^^^^\n890 |         enable_automatic_activation: bool = True\n891 |     ) -> MeasureTheoreticSummary:\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:889:33\n    |\n887 |     def enhance_with_topological_quantum_learning(\n888 |         self,\n889 |         cluster_data: Dict[str, Any],\n    |                                 ^^^\n890 |         enable_automatic_activation: bool = True\n891 |     ) -> MeasureTheoreticSummary:\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:909:53\n    |\n908 |         # Create quantum-enhanced aggregation function\n909 |         def quantum_enhanced_meso_aggregation(data: Dict[str, Any]) -> Dict[str, Any]:\n    |                                                     ^^^^\n910 |             \"\"\"Quantum-enhanced version of meso aggregation\"\"\"\n911 |             try:\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:909:63\n    |\n908 |         # Create quantum-enhanced aggregation function\n909 |         def quantum_enhanced_meso_aggregation(data: Dict[str, Any]) -> Dict[str, Any]:\n    |                                                               ^^^\n910 |             \"\"\"Quantum-enhanced version of meso aggregation\"\"\"\n911 |             try:\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:909:72\n    |\n908 |         # Create quantum-enhanced aggregation function\n909 |         def quantum_enhanced_meso_aggregation(data: Dict[str, Any]) -> Dict[str, Any]:\n    |                                                                        ^^^^\n910 |             \"\"\"Quantum-enhanced version of meso aggregation\"\"\"\n911 |             try:\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:909:82\n    |\n908 |         # Create quantum-enhanced aggregation function\n909 |         def quantum_enhanced_meso_aggregation(data: Dict[str, Any]) -> Dict[str, Any]:\n    |                                                                                  ^^^\n910 |             \"\"\"Quantum-enhanced version of meso aggregation\"\"\"\n911 |             try:\n    |\n\nF821 Undefined name `Dict`\n    --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:1009:23\n     |\n1007 |     def compute_stability_certificates(\n1008 |         self,\n1009 |         meso_summary: Dict[str, Any],\n     |                       ^^^^\n1010 |         probability_space: ProbabilitySpace\n1011 |     ) -> Dict[str, Any]:\n     |\n\nF821 Undefined name `Any`\n    --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:1009:33\n     |\n1007 |     def compute_stability_certificates(\n1008 |         self,\n1009 |         meso_summary: Dict[str, Any],\n     |                                 ^^^\n1010 |         probability_space: ProbabilitySpace\n1011 |     ) -> Dict[str, Any]:\n     |\n\nF821 Undefined name `Dict`\n    --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:1011:10\n     |\n1009 |         meso_summary: Dict[str, Any],\n1010 |         probability_space: ProbabilitySpace\n1011 |     ) -> Dict[str, Any]:\n     |          ^^^^\n1012 |         \"\"\"\n1013 |         Compute stability certificates using measure concentration\n     |\n\nF821 Undefined name `Any`\n    --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:1011:20\n     |\n1009 |         meso_summary: Dict[str, Any],\n1010 |         probability_space: ProbabilitySpace\n1011 |     ) -> Dict[str, Any]:\n     |                    ^^^\n1012 |         \"\"\"\n1013 |         Compute stability certificates using measure concentration\n     |\n\nF841 Local variable `divergence_stats` is assigned to but never used\n    --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:1025:9\n     |\n1024 |         # Extract divergence data\n1025 |         divergence_stats = meso_summary.get(\"divergence_stats\", {})\n     |         ^^^^^^^^^^^^^^^^\n1026 |         items = meso_summary.get(\"items\", {})\n     |\nhelp: Remove assignment to unused variable `divergence_stats`\n\nF841 Local variable `n` is assigned to but never used\n    --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:1058:17\n     |\n1056 |             if measure_values:\n1057 |                 # Estimate concentration parameter\n1058 |                 n = len(measure_values)\n     |                 ^\n1059 |                 variance = np.var(measure_values)\n     |\nhelp: Remove assignment to unused variable `n`\n\nF821 Undefined name `Dict`\n    --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:1096:23\n     |\n1094 |     def quantify_uncertainty(\n1095 |         self,\n1096 |         meso_summary: Dict[str, Any],\n     |                       ^^^^\n1097 |         convergence_result: ConvergenceResult\n1098 |     ) -> Dict[str, Any]:\n     |\n\nF821 Undefined name `Any`\n    --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:1096:33\n     |\n1094 |     def quantify_uncertainty(\n1095 |         self,\n1096 |         meso_summary: Dict[str, Any],\n     |                                 ^^^\n1097 |         convergence_result: ConvergenceResult\n1098 |     ) -> Dict[str, Any]:\n     |\n\nF821 Undefined name `Dict`\n    --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:1098:10\n     |\n1096 |         meso_summary: Dict[str, Any],\n1097 |         convergence_result: ConvergenceResult\n1098 |     ) -> Dict[str, Any]:\n     |          ^^^^\n1099 |         \"\"\"\n1100 |         Quantify uncertainty in aggregation results\n     |\n\nF821 Undefined name `Any`\n    --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:1098:20\n     |\n1096 |         meso_summary: Dict[str, Any],\n1097 |         convergence_result: ConvergenceResult\n1098 |     ) -> Dict[str, Any]:\n     |                    ^^^\n1099 |         \"\"\"\n1100 |         Quantify uncertainty in aggregation results\n     |\n\nF821 Undefined name `Any`\n    --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:1174:15\n     |\n1172 |     def enhance_aggregation(\n1173 |         self, \n1174 |         data: Any, \n     |               ^^^\n1175 |         context: Optional[Dict[str, Any]] = None\n1176 |     ) -> Dict[str, Any]:\n     |\n\nF821 Undefined name `Optional`\n    --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:1175:18\n     |\n1173 |         self, \n1174 |         data: Any, \n1175 |         context: Optional[Dict[str, Any]] = None\n     |                  ^^^^^^^^\n1176 |     ) -> Dict[str, Any]:\n1177 |         \"\"\"\n     |\n\nF821 Undefined name `Dict`\n    --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:1175:27\n     |\n1173 |         self, \n1174 |         data: Any, \n1175 |         context: Optional[Dict[str, Any]] = None\n     |                           ^^^^\n1176 |     ) -> Dict[str, Any]:\n1177 |         \"\"\"\n     |\n\nF821 Undefined name `Any`\n    --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:1175:37\n     |\n1173 |         self, \n1174 |         data: Any, \n1175 |         context: Optional[Dict[str, Any]] = None\n     |                                     ^^^\n1176 |     ) -> Dict[str, Any]:\n1177 |         \"\"\"\n     |\n\nF821 Undefined name `Dict`\n    --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:1176:10\n     |\n1174 |         data: Any, \n1175 |         context: Optional[Dict[str, Any]] = None\n1176 |     ) -> Dict[str, Any]:\n     |          ^^^^\n1177 |         \"\"\"\n1178 |         Main enhancement function that wraps meso_aggregator with mathematical rigor\n     |\n\nF821 Undefined name `Any`\n    --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:1176:20\n     |\n1174 |         data: Any, \n1175 |         context: Optional[Dict[str, Any]] = None\n1176 |     ) -> Dict[str, Any]:\n     |                    ^^^\n1177 |         \"\"\"\n1178 |         Main enhancement function that wraps meso_aggregator with mathematical rigor\n     |\n\nF841 Local variable `divergence_stats` is assigned to but never used\n    --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:1202:9\n     |\n1200 |         # Extract information for convergence analysis\n1201 |         meso_summary = original_result.get(\"meso_summary\", {})\n1202 |         divergence_stats = meso_summary.get(\"divergence_stats\", {})\n     |         ^^^^^^^^^^^^^^^^\n1203 |         \n1204 |         # Build divergence sequence for convergence analysis\n     |\nhelp: Remove assignment to unused variable `divergence_stats`\n\nF841 Local variable `enhanced_summary` is assigned to but never used\n    --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:1238:9\n     |\n1237 |         # Create enhanced summary\n1238 |         enhanced_summary = MeasureTheoreticSummary(\n     |         ^^^^^^^^^^^^^^^^\n1239 |             original_summary=original_result,\n1240 |             probability_space=probability_space,\n     |\nhelp: Remove assignment to unused variable `enhanced_summary`\n\nF821 Undefined name `wraps`\n    --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:1302:10\n     |\n1300 |     \"\"\"\n1301 |     def decorator(func):\n1302 |         @wraps(func)\n     |          ^^^^^\n1303 |         def wrapper(data: Any, context: Optional[Dict[str, Any]] = None):\n1304 |             enhancer = MathematicalAggregationEnhancer(\n     |\n\nF821 Undefined name `Any`\n    --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:1303:27\n     |\n1301 |     def decorator(func):\n1302 |         @wraps(func)\n1303 |         def wrapper(data: Any, context: Optional[Dict[str, Any]] = None):\n     |                           ^^^\n1304 |             enhancer = MathematicalAggregationEnhancer(\n1305 |                 confidence_level=confidence_level,\n     |\n\nF821 Undefined name `Optional`\n    --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:1303:41\n     |\n1301 |     def decorator(func):\n1302 |         @wraps(func)\n1303 |         def wrapper(data: Any, context: Optional[Dict[str, Any]] = None):\n     |                                         ^^^^^^^^\n1304 |             enhancer = MathematicalAggregationEnhancer(\n1305 |                 confidence_level=confidence_level,\n     |\n\nF821 Undefined name `Dict`\n    --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:1303:50\n     |\n1301 |     def decorator(func):\n1302 |         @wraps(func)\n1303 |         def wrapper(data: Any, context: Optional[Dict[str, Any]] = None):\n     |                                                  ^^^^\n1304 |             enhancer = MathematicalAggregationEnhancer(\n1305 |                 confidence_level=confidence_level,\n     |\n\nF821 Undefined name `Any`\n    --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:1303:60\n     |\n1301 |     def decorator(func):\n1302 |         @wraps(func)\n1303 |         def wrapper(data: Any, context: Optional[Dict[str, Any]] = None):\n     |                                                            ^^^\n1304 |             enhancer = MathematicalAggregationEnhancer(\n1305 |                 confidence_level=confidence_level,\n     |\n\nF821 Undefined name `Any`\n    --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:1315:19\n     |\n1315 | def process(data: Any, context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:\n     |                   ^^^\n1316 |     \"\"\"\n1317 |     Enhanced process function that provides mathematical rigor to meso aggregation\n     |\n\nF821 Undefined name `Optional`\n    --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:1315:33\n     |\n1315 | def process(data: Any, context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:\n     |                                 ^^^^^^^^\n1316 |     \"\"\"\n1317 |     Enhanced process function that provides mathematical rigor to meso aggregation\n     |\n\nF821 Undefined name `Dict`\n    --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:1315:42\n     |\n1315 | def process(data: Any, context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:\n     |                                          ^^^^\n1316 |     \"\"\"\n1317 |     Enhanced process function that provides mathematical rigor to meso aggregation\n     |\n\nF821 Undefined name `Any`\n    --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:1315:52\n     |\n1315 | def process(data: Any, context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:\n     |                                                    ^^^\n1316 |     \"\"\"\n1317 |     Enhanced process function that provides mathematical rigor to meso aggregation\n     |\n\nF821 Undefined name `Dict`\n    --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:1315:69\n     |\n1315 | def process(data: Any, context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:\n     |                                                                     ^^^^\n1316 |     \"\"\"\n1317 |     Enhanced process function that provides mathematical rigor to meso aggregation\n     |\n\nF821 Undefined name `Any`\n    --> canonical_flow/mathematical_enhancers/aggregation_enhancer.py:1315:79\n     |\n1315 | def process(data: Any, context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:\n     |                                                                               ^^^\n1316 |     \"\"\"\n1317 |     Enhanced process function that provides mathematical rigor to meso aggregation\n     |\n\ninvalid-syntax: Expected an indented block after `try` statement\n  --> canonical_flow/mathematical_enhancers/analysis_enhancer.py:32:1\n   |\n30 | # # #     from sklearn.feature_extraction.text import TfidfVectorizer  # type: ignore  # Module not found  # Module not found  # Modul\u2026\n31 | # # #     from sklearn.metrics.pairwise import cosine_similarity  # type: ignore  # Module not found  # Module not found  # Module not\u2026\n32 | except Exception:\n   | ^^^^^^\n33 |     class TfidfVectorizer:  # minimal fallback\n34 |         def __init__(self, ngram_range=(1, 1), min_df=1, max_features=None, lowercase=True, stop_words=None):\n   |\n\ninvalid-syntax: Expected an indented block after `try` statement\n  --> canonical_flow/mathematical_enhancers/analysis_enhancer.py:88:1\n   |\n86 | # # #     from question_analyzer import QuestionAnalyzer, CausalPosture  # Module not found  # Module not found  # Module not found\n87 | # # #     from adaptive_analyzer import AdaptiveAnalyzer, SystemState  # Module not found  # Module not found  # Module not found\n88 | except ImportError:\n   | ^^^^^^\n89 |     # Fallbacks for missing dependencies\n90 |     EvidenceProcessor = None\n   |\n\ninvalid-syntax: Expected an indented block after `try` statement\n  --> canonical_flow/mathematical_enhancers/context_enhancer.py:36:1\n   |\n34 | try:\n35 | # # #     from egw_query_expansion.core.immutable_context import QuestionContext, DerivationId, ContextHash  # Module not found  # Mod\u2026\n36 | except ImportError:\n   | ^^^^^^\n37 |     # Minimal fallback definitions for testing\n38 |     DerivationId = str\n   |\n\nF401 [*] `math` imported but unused\n  --> canonical_flow/mathematical_enhancers/hyperbolic_tensor_networks.py:24:8\n   |\n22 | # # # from __future__ import annotations  # Module not found  # Module not found  # Module not found\n23 |\n24 | import math\n   |        ^^^^\n25 | import warnings\n26 | # # # from typing import Any, Dict, List, Optional, Tuple, Union  # Module not found  # Module not found  # Module not found\n   |\nhelp: Remove unused import: `math`\n\nF821 Undefined name `Dict`\n  --> canonical_flow/mathematical_enhancers/hyperbolic_tensor_networks.py:37:45\n   |\n35 | # # # #     from orchestration.event_bus import publish_metric  # type: ignore  # Module not found  # Module not found  # Module not f\u2026\n36 | except Exception:  # noqa: BLE001\n37 |     def publish_metric(topic: str, payload: Dict[str, Any]) -> None:  # type: ignore\n   |                                             ^^^^\n38 |         return None\n   |\n\nF821 Undefined name `Any`\n  --> canonical_flow/mathematical_enhancers/hyperbolic_tensor_networks.py:37:55\n   |\n35 | # # # #     from orchestration.event_bus import publish_metric  # type: ignore  # Module not found  # Module not found  # Module not f\u2026\n36 | except Exception:  # noqa: BLE001\n37 |     def publish_metric(topic: str, payload: Dict[str, Any]) -> None:  # type: ignore\n   |                                                       ^^^\n38 |         return None\n   |\n\nF821 Undefined name `Tuple`\n   --> canonical_flow/mathematical_enhancers/hyperbolic_tensor_networks.py:199:78\n    |\n198 |     @trace\n199 |     def tensor_network_decomposition(self, similarity_matrix: np.ndarray) -> Tuple[Dict[str, np.ndarray], Dict[str, float]]:\n    |                                                                              ^^^^^\n200 |         \"\"\"\n201 |         Perform quantum tensor network decomposition of similarity matrix.\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/mathematical_enhancers/hyperbolic_tensor_networks.py:199:84\n    |\n198 |     @trace\n199 |     def tensor_network_decomposition(self, similarity_matrix: np.ndarray) -> Tuple[Dict[str, np.ndarray], Dict[str, float]]:\n    |                                                                                    ^^^^\n200 |         \"\"\"\n201 |         Perform quantum tensor network decomposition of similarity matrix.\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/mathematical_enhancers/hyperbolic_tensor_networks.py:199:107\n    |\n198 |     @trace\n199 |     def tensor_network_decomposition(self, similarity_matrix: np.ndarray) -> Tuple[Dict[str, np.ndarray], Dict[str, float]]:\n    |                                                                                                           ^^^^\n200 |         \"\"\"\n201 |         Perform quantum tensor network decomposition of similarity matrix.\n    |\n\nF821 Undefined name `linalg`\n   --> canonical_flow/mathematical_enhancers/hyperbolic_tensor_networks.py:218:24\n    |\n216 |         try:\n217 |             # SVD-based tensor decomposition\n218 |             U, s, Vt = linalg.svd(similarity_matrix)\n    |                        ^^^^^^\n219 |             \n220 |             # Truncate to tensor rank\n    |\n\nF821 Undefined name `linalg`\n   --> canonical_flow/mathematical_enhancers/hyperbolic_tensor_networks.py:245:36\n    |\n243 |             # Reconstruction error\n244 |             reconstructed = tensor_left @ tensor_center @ tensor_right\n245 |             reconstruction_error = linalg.norm(similarity_matrix - reconstructed, 'fro') / linalg.norm(similarity_matrix, 'fro')\n    |                                    ^^^^^^\n246 |             \n247 |             compression_metrics = {\n    |\n\nF821 Undefined name `linalg`\n   --> canonical_flow/mathematical_enhancers/hyperbolic_tensor_networks.py:245:92\n    |\n243 |             # Reconstruction error\n244 |             reconstructed = tensor_left @ tensor_center @ tensor_right\n245 |             reconstruction_error = linalg.norm(similarity_matrix - reconstructed, 'fro') / linalg.norm(similarity_matrix, 'fro')\n    |                                                                                            ^^^^^^\n246 |             \n247 |             compression_metrics = {\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/mathematical_enhancers/hyperbolic_tensor_networks.py:274:51\n    |\n273 |     @trace\n274 |     def quantum_tensor_contraction(self, tensors: Dict[str, np.ndarray], \n    |                                                   ^^^^\n275 |                                  query_embedding: np.ndarray) -> np.ndarray:\n276 |         \"\"\"\n    |\n\nF821 Undefined name `Tuple`\n   --> canonical_flow/mathematical_enhancers/hyperbolic_tensor_networks.py:307:77\n    |\n305 |     def hyperbolic_eigenvalue_enhancement(self, eigenvalues: np.ndarray, \n306 |                                         eigenvectors: np.ndarray,\n307 |                                         poincare_embeddings: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n    |                                                                             ^^^^^\n308 |         \"\"\"\n309 |         Enhance eigenvalue decomposition using hyperbolic geometry.\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/mathematical_enhancers/hyperbolic_tensor_networks.py:341:90\n    |\n340 |     @trace\n341 |     def compute_hyperbolic_clustering_features(self, poincare_embeddings: np.ndarray) -> Dict[str, Any]:\n    |                                                                                          ^^^^\n342 |         \"\"\"\n343 |         Compute clustering features in hyperbolic space.\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/mathematical_enhancers/hyperbolic_tensor_networks.py:341:100\n    |\n340 |     @trace\n341 |     def compute_hyperbolic_clustering_features(self, poincare_embeddings: np.ndarray) -> Dict[str, Any]:\n    |                                                                                                    ^^^\n342 |         \"\"\"\n343 |         Compute clustering features in hyperbolic space.\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/mathematical_enhancers/hyperbolic_tensor_networks.py:440:78\n    |\n438 |                                                 similarity_matrix: np.ndarray,\n439 |                                                 eigenvalues: np.ndarray,\n440 |                                                 eigenvectors: np.ndarray) -> Dict[str, Any]:\n    |                                                                              ^^^^\n441 |         \"\"\"\n442 |         Main integration function to enhance retrieval with hyperbolic tensor networks.\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/mathematical_enhancers/hyperbolic_tensor_networks.py:440:88\n    |\n438 |                                                 similarity_matrix: np.ndarray,\n439 |                                                 eigenvalues: np.ndarray,\n440 |                                                 eigenvectors: np.ndarray) -> Dict[str, Any]:\n    |                                                                                        ^^^\n441 |         \"\"\"\n442 |         Main integration function to enhance retrieval with hyperbolic tensor networks.\n    |\n\ninvalid-syntax: Expected an indented block after `try` statement\n  --> canonical_flow/mathematical_enhancers/ingestion_enhancer.py:31:1\n   |\n29 | try:\n30 | # # #     from models import SectionBlock, Citation  # Module not found  # Module not found  # Module not found\n31 | except ImportError:\n   | ^^^^^^\n32 |     # Fallback definitions for testing\n33 | # # #     from dataclasses import dataclass  # Module not found  # Module not found  # Module not found\n   |\n\ninvalid-syntax: Expected an indented block after `try` statement\n  --> canonical_flow/mathematical_enhancers/ingestion_enhancer.py:55:1\n   |\n53 | try:\n54 | # # #     from pdf_reader import PageContent, TextSpan  # Module not found  # Module not found  # Module not found\n55 | except ImportError:\n   | ^^^^^^\n56 |     # Fallback definitions for testing\n57 |     @dataclass\n   |\n\ninvalid-syntax: Expected an indented block after `try` statement\n  --> canonical_flow/mathematical_enhancers/ingestion_enhancer.py:75:1\n   |\n73 | try:\n74 | # # #     from normalizer import TextNormalizer  # Module not found  # Module not found  # Module not found\n75 | except ImportError:\n   | ^^^^^^\n76 |     # Fallback normalizer for testing\n77 |     class TextNormalizer:\n   |\n\ninvalid-syntax: Expected an indented block after `try` statement\n  --> canonical_flow/mathematical_enhancers/ingestion_enhancer.py:83:1\n   |\n81 | try:\n82 | # # #     from mathematical_pipeline_coordinator import ValidationResult, StageResult  # Module not found  # Module not found  # Modul\u2026\n83 | except ImportError:\n   | ^^^^^^\n84 |     # Fallback for testing\n85 | # # #     from dataclasses import dataclass  # Module not found  # Module not found  # Module not found\n   |\n\nF401 [*] `math` imported but unused\n  --> canonical_flow/mathematical_enhancers/integration_enhancer.py:30:8\n   |\n28 | \"\"\"\n29 |\n30 | import math\n   |        ^^^^\n31 | import time\n32 | # # # from typing import Any, Dict, List, Optional, Tuple, Union  # Module not found  # Module not found  # Module not found\n   |\nhelp: Remove unused import: `math`\n\nF401 [*] `scipy.optimize` imported but unused\n  --> canonical_flow/mathematical_enhancers/integration_enhancer.py:38:26\n   |\n37 | import numpy as np\n38 | import scipy.optimize as opt\n   |                          ^^^\n39 | # # # from scipy.linalg import eigvals, solve_continuous_lyapunov  # Module not found  # Module not found  # Module not found\n40 | # # # from scipy.spatial.distance import jensenshannon  # Module not found  # Module not found  # Module not found\n   |\nhelp: Remove unused import: `scipy.optimize`\n\nF821 Undefined name `Enum`\n  --> canonical_flow/mathematical_enhancers/integration_enhancer.py:46:22\n   |\n46 | class ReportingLevel(Enum):\n   |                      ^^^^\n47 |     \"\"\"Hierarchical reporting levels in the pipeline\"\"\"\n48 |     MICRO = \"micro\"  # Individual stage metrics\n   |\n\nF821 Undefined name `Enum`\n  --> canonical_flow/mathematical_enhancers/integration_enhancer.py:53:26\n   |\n53 | class OptimizationStatus(Enum):\n   |                          ^^^^\n54 |     \"\"\"Status of optimization process\"\"\"\n55 |     INITIALIZING = \"initializing\"\n   |\n\nF821 Undefined name `dataclass`\n  --> canonical_flow/mathematical_enhancers/integration_enhancer.py:62:2\n   |\n62 | @dataclass\n   |  ^^^^^^^^^\n63 | class ConvergenceMetrics:\n64 |     \"\"\"Mathematical convergence metrics with bounds\"\"\"\n   |\n\nF821 Undefined name `dataclass`\n  --> canonical_flow/mathematical_enhancers/integration_enhancer.py:81:2\n   |\n81 | @dataclass\n   |  ^^^^^^^^^\n82 | class StageMetrics:\n83 |     \"\"\"Metrics for individual pipeline stages\"\"\"\n   |\n\nF821 Undefined name `dataclass`\n   --> canonical_flow/mathematical_enhancers/integration_enhancer.py:105:2\n    |\n105 | @dataclass\n    |  ^^^^^^^^^\n106 | class FeedbackParameters:\n107 |     \"\"\"Parameters for feedback loop optimization\"\"\"\n    |\n\nF821 Undefined name `Optional`\n   --> canonical_flow/mathematical_enhancers/integration_enhancer.py:134:35\n    |\n132 |     def __init__(self, \n133 |                  num_stages: int = 12,\n134 |                  feedback_params: Optional[FeedbackParameters] = None):\n    |                                   ^^^^^^^^\n135 |         self.num_stages = num_stages\n136 |         self.feedback_params = feedback_params or FeedbackParameters()\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/mathematical_enhancers/integration_enhancer.py:145:35\n    |\n144 |         # Convergence tracking\n145 |         self.convergence_history: List[ConvergenceMetrics] = []\n    |                                   ^^^^\n146 |         self.optimization_status = OptimizationStatus.INITIALIZING\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/mathematical_enhancers/integration_enhancer.py:149:29\n    |\n148 |         # Stage metrics storage\n149 |         self.stage_metrics: Dict[int, List[StageMetrics]] = {}\n    |                             ^^^^\n150 |         self.integrated_metrics: Dict[ReportingLevel, Dict[str, float]] = {\n151 |             ReportingLevel.MICRO: {},\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/mathematical_enhancers/integration_enhancer.py:149:39\n    |\n148 |         # Stage metrics storage\n149 |         self.stage_metrics: Dict[int, List[StageMetrics]] = {}\n    |                                       ^^^^\n150 |         self.integrated_metrics: Dict[ReportingLevel, Dict[str, float]] = {\n151 |             ReportingLevel.MICRO: {},\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/mathematical_enhancers/integration_enhancer.py:150:34\n    |\n148 |         # Stage metrics storage\n149 |         self.stage_metrics: Dict[int, List[StageMetrics]] = {}\n150 |         self.integrated_metrics: Dict[ReportingLevel, Dict[str, float]] = {\n    |                                  ^^^^\n151 |             ReportingLevel.MICRO: {},\n152 |             ReportingLevel.MESO: {},\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/mathematical_enhancers/integration_enhancer.py:150:55\n    |\n148 |         # Stage metrics storage\n149 |         self.stage_metrics: Dict[int, List[StageMetrics]] = {}\n150 |         self.integrated_metrics: Dict[ReportingLevel, Dict[str, float]] = {\n    |                                                       ^^^^\n151 |             ReportingLevel.MICRO: {},\n152 |             ReportingLevel.MESO: {},\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/mathematical_enhancers/integration_enhancer.py:159:51\n    |\n157 |         self._compute_theoretical_bounds()\n158 |         \n159 |     def _initialize_aggregation_matrices(self) -> Dict[ReportingLevel, np.ndarray]:\n    |                                                   ^^^^\n160 |         \"\"\"Initialize convex aggregation matrices for each reporting level\"\"\"\n161 |         matrices = {}\n    |\n\nF821 Undefined name `eigvals`\n   --> canonical_flow/mathematical_enhancers/integration_enhancer.py:214:39\n    |\n212 |         # Lipschitz constant estimation for gradient descent\n213 |         hessian_approx = self._estimate_hessian()\n214 |         max_eigenval = np.max(np.real(eigvals(hessian_approx)))\n    |                                       ^^^^^^^\n215 |         min_eigenval = np.max([np.min(np.real(eigvals(hessian_approx))), 1e-10])\n    |\n\nF821 Undefined name `eigvals`\n   --> canonical_flow/mathematical_enhancers/integration_enhancer.py:215:47\n    |\n213 |         hessian_approx = self._estimate_hessian()\n214 |         max_eigenval = np.max(np.real(eigvals(hessian_approx)))\n215 |         min_eigenval = np.max([np.min(np.real(eigvals(hessian_approx))), 1e-10])\n    |                                               ^^^^^^^\n216 |         \n217 |         condition_number = max_eigenval / min_eigenval\n    |\n\nF821 Undefined name `eigvals`\n   --> canonical_flow/mathematical_enhancers/integration_enhancer.py:247:39\n    |\n246 |         # Ensure positive definiteness\n247 |         min_eigenval = np.min(np.real(eigvals(hessian)))\n    |                                       ^^^^^^^\n248 |         if min_eigenval <= 0:\n249 |             hessian += (abs(min_eigenval) + 1e-6) * np.eye(n)\n    |\n\nF821 Undefined name `solve_continuous_lyapunov`\n   --> canonical_flow/mathematical_enhancers/integration_enhancer.py:261:17\n    |\n259 |         Q = np.eye(A.shape[0])\n260 |         try:\n261 |             P = solve_continuous_lyapunov(A.T, -Q)\n    |                 ^^^^^^^^^^^^^^^^^^^^^^^^^\n262 |             self.lyapunov_matrix = P\n    |\n\nF821 Undefined name `eigvals`\n   --> canonical_flow/mathematical_enhancers/integration_enhancer.py:265:27\n    |\n264 | # # #             # Stability bound from Lyapunov function  # Module not found  # Module not found  # Module not found\n265 |             eigenvals_P = eigvals(P)\n    |                           ^^^^^^^\n266 |             eigenvals_A = eigvals(A)\n    |\n\nF821 Undefined name `eigvals`\n   --> canonical_flow/mathematical_enhancers/integration_enhancer.py:266:27\n    |\n264 | # # #             # Stability bound from Lyapunov function  # Module not found  # Module not found  # Module not found\n265 |             eigenvals_P = eigvals(P)\n266 |             eigenvals_A = eigvals(A)\n    |                           ^^^^^^^\n267 |             \n268 |             if np.all(np.real(eigenvals_P) > 0) and np.all(np.real(eigenvals_A) < 0):\n    |\n\nF841 Local variable `n` is assigned to but never used\n   --> canonical_flow/mathematical_enhancers/integration_enhancer.py:280:9\n    |\n278 |     def _build_system_matrix(self) -> np.ndarray:\n279 |         \"\"\"Build system dynamics matrix for stability analysis\"\"\"\n280 |         n = 4  # Simplified system size\n    |         ^\n281 |         A = np.array([\n282 |             [-1.0, 0.2, 0.1, 0.0],\n    |\nhelp: Remove assignment to unused variable `n`\n\nF821 Undefined name `Dict`\n   --> canonical_flow/mathematical_enhancers/integration_enhancer.py:349:52\n    |\n348 |     def optimize_feedback_parameters(self, \n349 |                                    target_metrics: Dict[str, float],\n    |                                                    ^^^^\n350 |                                    max_iterations: Optional[int] = None) -> Dict[str, Any]:\n351 |         \"\"\"\n    |\n\nF821 Undefined name `Optional`\n   --> canonical_flow/mathematical_enhancers/integration_enhancer.py:350:52\n    |\n348 |     def optimize_feedback_parameters(self, \n349 |                                    target_metrics: Dict[str, float],\n350 |                                    max_iterations: Optional[int] = None) -> Dict[str, Any]:\n    |                                                    ^^^^^^^^\n351 |         \"\"\"\n352 |         Optimize feedback loop parameters using gradient descent with convergence guarantees.\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/mathematical_enhancers/integration_enhancer.py:350:77\n    |\n348 |     def optimize_feedback_parameters(self, \n349 |                                    target_metrics: Dict[str, float],\n350 |                                    max_iterations: Optional[int] = None) -> Dict[str, Any]:\n    |                                                                             ^^^^\n351 |         \"\"\"\n352 |         Optimize feedback loop parameters using gradient descent with convergence guarantees.\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/mathematical_enhancers/integration_enhancer.py:350:87\n    |\n348 |     def optimize_feedback_parameters(self, \n349 |                                    target_metrics: Dict[str, float],\n350 |                                    max_iterations: Optional[int] = None) -> Dict[str, Any]:\n    |                                                                                       ^^^\n351 |         \"\"\"\n352 |         Optimize feedback loop parameters using gradient descent with convergence guarantees.\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/mathematical_enhancers/integration_enhancer.py:509:48\n    |\n507 |         }\n508 |     \n509 |     def analyze_convergence_stability(self) -> Dict[str, Any]:\n    |                                                ^^^^\n510 |         \"\"\"Analyze mathematical convergence and stability properties\"\"\"\n511 |         if not self.convergence_history:\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/mathematical_enhancers/integration_enhancer.py:509:58\n    |\n507 |         }\n508 |     \n509 |     def analyze_convergence_stability(self) -> Dict[str, Any]:\n    |                                                          ^^^\n510 |         \"\"\"Analyze mathematical convergence and stability properties\"\"\"\n511 |         if not self.convergence_history:\n    |\n\nF841 Local variable `iterations` is assigned to but never used\n   --> canonical_flow/mathematical_enhancers/integration_enhancer.py:515:9\n    |\n514 |         # Extract convergence data\n515 |         iterations = [m.iteration for m in self.convergence_history]\n    |         ^^^^^^^^^^\n516 |         gradient_norms = [m.gradient_norm for m in self.convergence_history]\n517 |         objective_values = [m.objective_value for m in self.convergence_history]\n    |\nhelp: Remove assignment to unused variable `iterations`\n\nF821 Undefined name `Dict`\n   --> canonical_flow/mathematical_enhancers/integration_enhancer.py:556:41\n    |\n554 |         }\n555 |     \n556 |     def get_integration_report(self) -> Dict[str, Any]:\n    |                                         ^^^^\n557 |         \"\"\"Generate comprehensive integration report across all levels\"\"\"\n558 |         return {\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/mathematical_enhancers/integration_enhancer.py:556:51\n    |\n554 |         }\n555 |     \n556 |     def get_integration_report(self) -> Dict[str, Any]:\n    |                                                   ^^^\n557 |         \"\"\"Generate comprehensive integration report across all levels\"\"\"\n558 |         return {\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/mathematical_enhancers/integration_enhancer.py:587:19\n    |\n587 | def process(data: Dict[str, Any], context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:\n    |                   ^^^^\n588 |     \"\"\"\n589 |     Main processing function for mathematical integration enhancement.\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/mathematical_enhancers/integration_enhancer.py:587:29\n    |\n587 | def process(data: Dict[str, Any], context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:\n    |                             ^^^\n588 |     \"\"\"\n589 |     Main processing function for mathematical integration enhancement.\n    |\n\nF821 Undefined name `Optional`\n   --> canonical_flow/mathematical_enhancers/integration_enhancer.py:587:44\n    |\n587 | def process(data: Dict[str, Any], context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:\n    |                                            ^^^^^^^^\n588 |     \"\"\"\n589 |     Main processing function for mathematical integration enhancement.\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/mathematical_enhancers/integration_enhancer.py:587:53\n    |\n587 | def process(data: Dict[str, Any], context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:\n    |                                                     ^^^^\n588 |     \"\"\"\n589 |     Main processing function for mathematical integration enhancement.\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/mathematical_enhancers/integration_enhancer.py:587:63\n    |\n587 | def process(data: Dict[str, Any], context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:\n    |                                                               ^^^\n588 |     \"\"\"\n589 |     Main processing function for mathematical integration enhancement.\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/mathematical_enhancers/integration_enhancer.py:587:80\n    |\n587 | def process(data: Dict[str, Any], context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:\n    |                                                                                ^^^^\n588 |     \"\"\"\n589 |     Main processing function for mathematical integration enhancement.\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/mathematical_enhancers/integration_enhancer.py:587:90\n    |\n587 | def process(data: Dict[str, Any], context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:\n    |                                                                                          ^^^\n588 |     \"\"\"\n589 |     Main processing function for mathematical integration enhancement.\n    |\n\ninvalid-syntax: Expected an indented block after `try` statement\n  --> canonical_flow/mathematical_enhancers/knowledge_enhancer.py:72:1\n   |\n70 | try:\n71 | # # #     from causal_graph import CausalGraph, CausalNode, CausalEdge, CausalRelationType  # Module not found  # Module not found  # \u2026\n72 | except ImportError:\n   | ^^^^^^\n73 |     # Mock classes for standalone operation\n74 | # # #     from enum import Enum  # Module not found  # Module not found  # Module not found\n   |\n\nF401 [*] `warnings` imported but unused\n  --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:25:8\n   |\n24 | import sys\n25 | import warnings\n   |        ^^^^^^^^\n26 | import platform\n27 | import re\n   |\nhelp: Remove unused import: `warnings`\n\nF401 [*] `re` imported but unused\n  --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:27:8\n   |\n25 | import warnings\n26 | import platform\n27 | import re\n   |        ^^\n28 | # # # from typing import Dict, List, Optional, Tuple, Union, Any, NamedTuple  # Module not found  # Module not found  # Module not fou\u2026\n29 | # # # from dataclasses import dataclass, field  # Module not found  # Module not found  # Module not found\n   |\nhelp: Remove unused import: `re`\n\nF401 [*] `os` imported but unused\n  --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:41:8\n   |\n39 | import traceback\n40 | import json\n41 | import os\n   |        ^^\n42 |\n43 | # Import numpy safely\n   |\nhelp: Remove unused import: `os`\n\nF821 Undefined name `Enum`\n  --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:50:21\n   |\n50 | class PythonVersion(Enum):\n   |                     ^^^^\n51 |     \"\"\"Supported Python versions\"\"\"\n52 |     PYTHON_38 = \"3.8\"\n   |\n\nF821 Undefined name `Enum`\n  --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:59:21\n   |\n59 | class StageEnhancer(Enum):\n   |                     ^^^^\n60 |     \"\"\"12 Mathematical stage enhancers\"\"\"\n61 |     DIFFERENTIAL_GEOMETRY = \"differential_geometry\"\n   |\n\nF821 Undefined name `dataclass`\n  --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:75:2\n   |\n75 | @dataclass\n   |  ^^^^^^^^^\n76 | class VersionConstraint:\n77 |     \"\"\"Version constraint specification\"\"\"\n   |\n\nF821 Undefined name `Optional`\n  --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:79:18\n   |\n77 |     \"\"\"Version constraint specification\"\"\"\n78 |     min_version: str\n79 |     max_version: Optional[str] = None\n   |                  ^^^^^^^^\n80 |     excluded_versions: List[str] = field(default_factory=list)\n81 |     notes: str = \"\"\n   |\n\nF821 Undefined name `List`\n  --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:80:24\n   |\n78 |     min_version: str\n79 |     max_version: Optional[str] = None\n80 |     excluded_versions: List[str] = field(default_factory=list)\n   |                        ^^^^\n81 |     notes: str = \"\"\n   |\n\nF821 Undefined name `field`\n  --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:80:36\n   |\n78 |     min_version: str\n79 |     max_version: Optional[str] = None\n80 |     excluded_versions: List[str] = field(default_factory=list)\n   |                                    ^^^^^\n81 |     notes: str = \"\"\n   |\n\nF821 Undefined name `dataclass`\n   --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:108:2\n    |\n108 | @dataclass\n    |  ^^^^^^^^^\n109 | class PlatformConstraint:\n110 |     \"\"\"Platform-specific constraints\"\"\"\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:111:26\n    |\n109 | class PlatformConstraint:\n110 |     \"\"\"Platform-specific constraints\"\"\"\n111 |     supported_platforms: List[str] = field(default_factory=lambda: [\"Windows\", \"Linux\", \"Darwin\"])\n    |                          ^^^^\n112 |     unsupported_platforms: List[str] = field(default_factory=list)\n113 |     platform_specific_notes: Dict[str, str] = field(default_factory=dict)\n    |\n\nF821 Undefined name `field`\n   --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:111:38\n    |\n109 | class PlatformConstraint:\n110 |     \"\"\"Platform-specific constraints\"\"\"\n111 |     supported_platforms: List[str] = field(default_factory=lambda: [\"Windows\", \"Linux\", \"Darwin\"])\n    |                                      ^^^^^\n112 |     unsupported_platforms: List[str] = field(default_factory=list)\n113 |     platform_specific_notes: Dict[str, str] = field(default_factory=dict)\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:112:28\n    |\n110 |     \"\"\"Platform-specific constraints\"\"\"\n111 |     supported_platforms: List[str] = field(default_factory=lambda: [\"Windows\", \"Linux\", \"Darwin\"])\n112 |     unsupported_platforms: List[str] = field(default_factory=list)\n    |                            ^^^^\n113 |     platform_specific_notes: Dict[str, str] = field(default_factory=dict)\n    |\n\nF821 Undefined name `field`\n   --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:112:40\n    |\n110 |     \"\"\"Platform-specific constraints\"\"\"\n111 |     supported_platforms: List[str] = field(default_factory=lambda: [\"Windows\", \"Linux\", \"Darwin\"])\n112 |     unsupported_platforms: List[str] = field(default_factory=list)\n    |                                        ^^^^^\n113 |     platform_specific_notes: Dict[str, str] = field(default_factory=dict)\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:113:30\n    |\n111 |     supported_platforms: List[str] = field(default_factory=lambda: [\"Windows\", \"Linux\", \"Darwin\"])\n112 |     unsupported_platforms: List[str] = field(default_factory=list)\n113 |     platform_specific_notes: Dict[str, str] = field(default_factory=dict)\n    |                              ^^^^\n114 |     \n115 |     def is_platform_supported(self, platform_name: str) -> bool:\n    |\n\nF821 Undefined name `field`\n   --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:113:47\n    |\n111 |     supported_platforms: List[str] = field(default_factory=lambda: [\"Windows\", \"Linux\", \"Darwin\"])\n112 |     unsupported_platforms: List[str] = field(default_factory=list)\n113 |     platform_specific_notes: Dict[str, str] = field(default_factory=dict)\n    |                                               ^^^^^\n114 |     \n115 |     def is_platform_supported(self, platform_name: str) -> bool:\n    |\n\nF821 Undefined name `dataclass`\n   --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:122:2\n    |\n122 | @dataclass \n    |  ^^^^^^^^^\n123 | class LibrarySpec:\n124 |     \"\"\"Library specification with version constraints\"\"\"\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:127:18\n    |\n125 |     name: str\n126 |     import_name: str\n127 |     constraints: Dict[PythonVersion, VersionConstraint]\n    |                  ^^^^\n128 |     platform_constraints: PlatformConstraint = field(default_factory=PlatformConstraint)\n129 |     is_optional: bool = False\n    |\n\nF821 Undefined name `field`\n   --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:128:48\n    |\n126 |     import_name: str\n127 |     constraints: Dict[PythonVersion, VersionConstraint]\n128 |     platform_constraints: PlatformConstraint = field(default_factory=PlatformConstraint)\n    |                                                ^^^^^\n129 |     is_optional: bool = False\n130 |     fallback_libraries: List[str] = field(default_factory=list)\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:130:25\n    |\n128 |     platform_constraints: PlatformConstraint = field(default_factory=PlatformConstraint)\n129 |     is_optional: bool = False\n130 |     fallback_libraries: List[str] = field(default_factory=list)\n    |                         ^^^^\n131 |     conflicts_with: List[str] = field(default_factory=list)\n132 |     install_command: Optional[str] = None\n    |\n\nF821 Undefined name `field`\n   --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:130:37\n    |\n128 |     platform_constraints: PlatformConstraint = field(default_factory=PlatformConstraint)\n129 |     is_optional: bool = False\n130 |     fallback_libraries: List[str] = field(default_factory=list)\n    |                                     ^^^^^\n131 |     conflicts_with: List[str] = field(default_factory=list)\n132 |     install_command: Optional[str] = None\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:131:21\n    |\n129 |     is_optional: bool = False\n130 |     fallback_libraries: List[str] = field(default_factory=list)\n131 |     conflicts_with: List[str] = field(default_factory=list)\n    |                     ^^^^\n132 |     install_command: Optional[str] = None\n    |\n\nF821 Undefined name `field`\n   --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:131:33\n    |\n129 |     is_optional: bool = False\n130 |     fallback_libraries: List[str] = field(default_factory=list)\n131 |     conflicts_with: List[str] = field(default_factory=list)\n    |                                 ^^^^^\n132 |     install_command: Optional[str] = None\n    |\n\nF821 Undefined name `Optional`\n   --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:132:22\n    |\n130 |     fallback_libraries: List[str] = field(default_factory=list)\n131 |     conflicts_with: List[str] = field(default_factory=list)\n132 |     install_command: Optional[str] = None\n    |                      ^^^^^^^^\n    |\n\nF821 Undefined name `dataclass`\n   --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:135:2\n    |\n135 | @dataclass\n    |  ^^^^^^^^^\n136 | class CompatibilityResult:\n137 |     \"\"\"Result of compatibility check\"\"\"\n    |\n\nF821 Undefined name `Optional`\n   --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:139:24\n    |\n137 |     \"\"\"Result of compatibility check\"\"\"\n138 |     is_compatible: bool\n139 |     installed_version: Optional[str] = None\n    |                        ^^^^^^^^\n140 |     required_version: Optional[str] = None\n141 |     issues: List[str] = field(default_factory=list)\n    |\n\nF821 Undefined name `Optional`\n   --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:140:23\n    |\n138 |     is_compatible: bool\n139 |     installed_version: Optional[str] = None\n140 |     required_version: Optional[str] = None\n    |                       ^^^^^^^^\n141 |     issues: List[str] = field(default_factory=list)\n142 |     warnings: List[str] = field(default_factory=list)\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:141:13\n    |\n139 |     installed_version: Optional[str] = None\n140 |     required_version: Optional[str] = None\n141 |     issues: List[str] = field(default_factory=list)\n    |             ^^^^\n142 |     warnings: List[str] = field(default_factory=list)\n143 |     platform_issues: List[str] = field(default_factory=list)\n    |\n\nF821 Undefined name `field`\n   --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:141:25\n    |\n139 |     installed_version: Optional[str] = None\n140 |     required_version: Optional[str] = None\n141 |     issues: List[str] = field(default_factory=list)\n    |                         ^^^^^\n142 |     warnings: List[str] = field(default_factory=list)\n143 |     platform_issues: List[str] = field(default_factory=list)\n    |\n\nF811 Redefinition of unused `warnings` from line 25\n   --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:142:5\n    |\n140 |     required_version: Optional[str] = None\n141 |     issues: List[str] = field(default_factory=list)\n142 |     warnings: List[str] = field(default_factory=list)\n    |     ^^^^^^^^ `warnings` redefined here\n143 |     platform_issues: List[str] = field(default_factory=list)\n144 |     conflicts: List[str] = field(default_factory=list)\n    |\n   ::: canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:25:8\n    |\n 24 | import sys\n 25 | import warnings\n    |        -------- previous definition of `warnings` here\n 26 | import platform\n 27 | import re\n    |\nhelp: Remove definition: `warnings`\n\nF821 Undefined name `List`\n   --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:142:15\n    |\n140 |     required_version: Optional[str] = None\n141 |     issues: List[str] = field(default_factory=list)\n142 |     warnings: List[str] = field(default_factory=list)\n    |               ^^^^\n143 |     platform_issues: List[str] = field(default_factory=list)\n144 |     conflicts: List[str] = field(default_factory=list)\n    |\n\nF821 Undefined name `field`\n   --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:142:27\n    |\n140 |     required_version: Optional[str] = None\n141 |     issues: List[str] = field(default_factory=list)\n142 |     warnings: List[str] = field(default_factory=list)\n    |                           ^^^^^\n143 |     platform_issues: List[str] = field(default_factory=list)\n144 |     conflicts: List[str] = field(default_factory=list)\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:143:22\n    |\n141 |     issues: List[str] = field(default_factory=list)\n142 |     warnings: List[str] = field(default_factory=list)\n143 |     platform_issues: List[str] = field(default_factory=list)\n    |                      ^^^^\n144 |     conflicts: List[str] = field(default_factory=list)\n145 |     recommendations: List[str] = field(default_factory=list)\n    |\n\nF821 Undefined name `field`\n   --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:143:34\n    |\n141 |     issues: List[str] = field(default_factory=list)\n142 |     warnings: List[str] = field(default_factory=list)\n143 |     platform_issues: List[str] = field(default_factory=list)\n    |                                  ^^^^^\n144 |     conflicts: List[str] = field(default_factory=list)\n145 |     recommendations: List[str] = field(default_factory=list)\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:144:16\n    |\n142 |     warnings: List[str] = field(default_factory=list)\n143 |     platform_issues: List[str] = field(default_factory=list)\n144 |     conflicts: List[str] = field(default_factory=list)\n    |                ^^^^\n145 |     recommendations: List[str] = field(default_factory=list)\n    |\n\nF821 Undefined name `field`\n   --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:144:28\n    |\n142 |     warnings: List[str] = field(default_factory=list)\n143 |     platform_issues: List[str] = field(default_factory=list)\n144 |     conflicts: List[str] = field(default_factory=list)\n    |                            ^^^^^\n145 |     recommendations: List[str] = field(default_factory=list)\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:145:22\n    |\n143 |     platform_issues: List[str] = field(default_factory=list)\n144 |     conflicts: List[str] = field(default_factory=list)\n145 |     recommendations: List[str] = field(default_factory=list)\n    |                      ^^^^\n    |\n\nF821 Undefined name `field`\n   --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:145:34\n    |\n143 |     platform_issues: List[str] = field(default_factory=list)\n144 |     conflicts: List[str] = field(default_factory=list)\n145 |     recommendations: List[str] = field(default_factory=list)\n    |                                  ^^^^^\n    |\n\nF821 Undefined name `dataclass`\n   --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:148:2\n    |\n148 | @dataclass\n    |  ^^^^^^^^^\n149 | class NumericalStabilityResult:\n150 |     \"\"\"Result of numerical stability validation\"\"\"\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:154:24\n    |\n152 |     max_error: float\n153 |     mean_error: float\n154 |     failed_operations: List[str] = field(default_factory=list)\n    |                        ^^^^\n155 |     precision_warnings: List[str] = field(default_factory=list)\n    |\n\nF821 Undefined name `field`\n   --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:154:36\n    |\n152 |     max_error: float\n153 |     mean_error: float\n154 |     failed_operations: List[str] = field(default_factory=list)\n    |                                    ^^^^^\n155 |     precision_warnings: List[str] = field(default_factory=list)\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:155:25\n    |\n153 |     mean_error: float\n154 |     failed_operations: List[str] = field(default_factory=list)\n155 |     precision_warnings: List[str] = field(default_factory=list)\n    |                         ^^^^\n    |\n\nF821 Undefined name `field`\n   --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:155:37\n    |\n153 |     mean_error: float\n154 |     failed_operations: List[str] = field(default_factory=list)\n155 |     precision_warnings: List[str] = field(default_factory=list)\n    |                                     ^^^^^\n    |\n\nF821 Undefined name `dataclass`\n   --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:158:2\n    |\n158 | @dataclass\n    |  ^^^^^^^^^\n159 | class CrossPlatformResult:\n160 |     \"\"\"Result of cross-platform compatibility test\"\"\"\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:164:22\n    |\n162 |     python_version: str\n163 |     is_compatible: bool\n164 |     library_results: Dict[str, CompatibilityResult] = field(default_factory=dict)\n    |                      ^^^^\n165 |     platform_specific_issues: List[str] = field(default_factory=list)\n166 |     recommendations: List[str] = field(default_factory=list)\n    |\n\nF821 Undefined name `field`\n   --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:164:55\n    |\n162 |     python_version: str\n163 |     is_compatible: bool\n164 |     library_results: Dict[str, CompatibilityResult] = field(default_factory=dict)\n    |                                                       ^^^^^\n165 |     platform_specific_issues: List[str] = field(default_factory=list)\n166 |     recommendations: List[str] = field(default_factory=list)\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:165:31\n    |\n163 |     is_compatible: bool\n164 |     library_results: Dict[str, CompatibilityResult] = field(default_factory=dict)\n165 |     platform_specific_issues: List[str] = field(default_factory=list)\n    |                               ^^^^\n166 |     recommendations: List[str] = field(default_factory=list)\n    |\n\nF821 Undefined name `field`\n   --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:165:43\n    |\n163 |     is_compatible: bool\n164 |     library_results: Dict[str, CompatibilityResult] = field(default_factory=dict)\n165 |     platform_specific_issues: List[str] = field(default_factory=list)\n    |                                           ^^^^^\n166 |     recommendations: List[str] = field(default_factory=list)\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:166:22\n    |\n164 |     library_results: Dict[str, CompatibilityResult] = field(default_factory=dict)\n165 |     platform_specific_issues: List[str] = field(default_factory=list)\n166 |     recommendations: List[str] = field(default_factory=list)\n    |                      ^^^^\n    |\n\nF821 Undefined name `field`\n   --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:166:34\n    |\n164 |     library_results: Dict[str, CompatibilityResult] = field(default_factory=dict)\n165 |     platform_specific_issues: List[str] = field(default_factory=list)\n166 |     recommendations: List[str] = field(default_factory=list)\n    |                                  ^^^^^\n    |\n\nF821 Undefined name `dataclass`\n   --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:169:2\n    |\n169 | @dataclass  \n    |  ^^^^^^^^^\n170 | class ValidationReport:\n171 |     \"\"\"Comprehensive validation report\"\"\"\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:175:28\n    |\n173 |     python_version: str\n174 |     platform: str\n175 |     library_compatibility: Dict[str, CompatibilityResult] = field(default_factory=dict)\n    |                            ^^^^\n176 |     stage_compatibility: Dict[str, Dict[str, bool]] = field(default_factory=dict)\n177 |     numerical_stability: Optional[NumericalStabilityResult] = None\n    |\n\nF821 Undefined name `field`\n   --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:175:61\n    |\n173 |     python_version: str\n174 |     platform: str\n175 |     library_compatibility: Dict[str, CompatibilityResult] = field(default_factory=dict)\n    |                                                             ^^^^^\n176 |     stage_compatibility: Dict[str, Dict[str, bool]] = field(default_factory=dict)\n177 |     numerical_stability: Optional[NumericalStabilityResult] = None\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:176:26\n    |\n174 |     platform: str\n175 |     library_compatibility: Dict[str, CompatibilityResult] = field(default_factory=dict)\n176 |     stage_compatibility: Dict[str, Dict[str, bool]] = field(default_factory=dict)\n    |                          ^^^^\n177 |     numerical_stability: Optional[NumericalStabilityResult] = None\n178 |     cross_platform_results: List[CrossPlatformResult] = field(default_factory=list)\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:176:36\n    |\n174 |     platform: str\n175 |     library_compatibility: Dict[str, CompatibilityResult] = field(default_factory=dict)\n176 |     stage_compatibility: Dict[str, Dict[str, bool]] = field(default_factory=dict)\n    |                                    ^^^^\n177 |     numerical_stability: Optional[NumericalStabilityResult] = None\n178 |     cross_platform_results: List[CrossPlatformResult] = field(default_factory=list)\n    |\n\nF821 Undefined name `field`\n   --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:176:55\n    |\n174 |     platform: str\n175 |     library_compatibility: Dict[str, CompatibilityResult] = field(default_factory=dict)\n176 |     stage_compatibility: Dict[str, Dict[str, bool]] = field(default_factory=dict)\n    |                                                       ^^^^^\n177 |     numerical_stability: Optional[NumericalStabilityResult] = None\n178 |     cross_platform_results: List[CrossPlatformResult] = field(default_factory=list)\n    |\n\nF821 Undefined name `Optional`\n   --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:177:26\n    |\n175 |     library_compatibility: Dict[str, CompatibilityResult] = field(default_factory=dict)\n176 |     stage_compatibility: Dict[str, Dict[str, bool]] = field(default_factory=dict)\n177 |     numerical_stability: Optional[NumericalStabilityResult] = None\n    |                          ^^^^^^^^\n178 |     cross_platform_results: List[CrossPlatformResult] = field(default_factory=list)\n179 |     faiss_conflicts: List[str] = field(default_factory=list)\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:178:29\n    |\n176 |     stage_compatibility: Dict[str, Dict[str, bool]] = field(default_factory=dict)\n177 |     numerical_stability: Optional[NumericalStabilityResult] = None\n178 |     cross_platform_results: List[CrossPlatformResult] = field(default_factory=list)\n    |                             ^^^^\n179 |     faiss_conflicts: List[str] = field(default_factory=list)\n180 |     recommendations: List[str] = field(default_factory=list)\n    |\n\nF821 Undefined name `field`\n   --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:178:57\n    |\n176 |     stage_compatibility: Dict[str, Dict[str, bool]] = field(default_factory=dict)\n177 |     numerical_stability: Optional[NumericalStabilityResult] = None\n178 |     cross_platform_results: List[CrossPlatformResult] = field(default_factory=list)\n    |                                                         ^^^^^\n179 |     faiss_conflicts: List[str] = field(default_factory=list)\n180 |     recommendations: List[str] = field(default_factory=list)\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:179:22\n    |\n177 |     numerical_stability: Optional[NumericalStabilityResult] = None\n178 |     cross_platform_results: List[CrossPlatformResult] = field(default_factory=list)\n179 |     faiss_conflicts: List[str] = field(default_factory=list)\n    |                      ^^^^\n180 |     recommendations: List[str] = field(default_factory=list)\n181 |     critical_issues: List[str] = field(default_factory=list)\n    |\n\nF821 Undefined name `field`\n   --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:179:34\n    |\n177 |     numerical_stability: Optional[NumericalStabilityResult] = None\n178 |     cross_platform_results: List[CrossPlatformResult] = field(default_factory=list)\n179 |     faiss_conflicts: List[str] = field(default_factory=list)\n    |                                  ^^^^^\n180 |     recommendations: List[str] = field(default_factory=list)\n181 |     critical_issues: List[str] = field(default_factory=list)\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:180:22\n    |\n178 |     cross_platform_results: List[CrossPlatformResult] = field(default_factory=list)\n179 |     faiss_conflicts: List[str] = field(default_factory=list)\n180 |     recommendations: List[str] = field(default_factory=list)\n    |                      ^^^^\n181 |     critical_issues: List[str] = field(default_factory=list)\n    |\n\nF821 Undefined name `field`\n   --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:180:34\n    |\n178 |     cross_platform_results: List[CrossPlatformResult] = field(default_factory=list)\n179 |     faiss_conflicts: List[str] = field(default_factory=list)\n180 |     recommendations: List[str] = field(default_factory=list)\n    |                                  ^^^^^\n181 |     critical_issues: List[str] = field(default_factory=list)\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:181:22\n    |\n179 |     faiss_conflicts: List[str] = field(default_factory=list)\n180 |     recommendations: List[str] = field(default_factory=list)\n181 |     critical_issues: List[str] = field(default_factory=list)\n    |                      ^^^^\n    |\n\nF821 Undefined name `field`\n   --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:181:34\n    |\n179 |     faiss_conflicts: List[str] = field(default_factory=list)\n180 |     recommendations: List[str] = field(default_factory=list)\n181 |     critical_issues: List[str] = field(default_factory=list)\n    |                                  ^^^^^\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:215:40\n    |\n213 |                 return PythonVersion.PYTHON_310  # Safe default\n214 |                 \n215 |     def _get_platform_details(self) -> Dict[str, str]:\n    |                                        ^^^^\n216 |         \"\"\"Get detailed platform information\"\"\"\n217 |         return {\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:227:44\n    |\n225 |         }\n226 |     \n227 |     def _initialize_library_specs(self) -> Dict[str, LibrarySpec]:\n    |                                            ^^^^\n228 |         \"\"\"Initialize all library specifications with version constraints\"\"\"\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:397:49\n    |\n395 |         return specs\n396 |     \n397 |     def _initialize_stage_dependencies(self) -> Dict[StageEnhancer, List[str]]:\n    |                                                 ^^^^\n398 |         \"\"\"Initialize dependencies for each stage enhancer\"\"\"\n399 |         return {\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:397:69\n    |\n395 |         return specs\n396 |     \n397 |     def _initialize_stage_dependencies(self) -> Dict[StageEnhancer, List[str]]:\n    |                                                                     ^^^^\n398 |         \"\"\"Initialize dependencies for each stage enhancer\"\"\"\n399 |         return {\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:489:82\n    |\n487 |         return result\n488 |         \n489 |     def _detect_library_conflicts(self, library_name: str, spec: LibrarySpec) -> List[str]:\n    |                                                                                  ^^^^\n490 |         \"\"\"Detect conflicts with other installed libraries\"\"\"\n491 |         conflicts = []\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:505:66\n    |\n503 |         return conflicts\n504 |     \n505 |     def check_stage_compatibility(self, stage: StageEnhancer) -> Dict[str, CompatibilityResult]:\n    |                                                                  ^^^^\n506 |         \"\"\"Check compatibility of all libraries required for a stage enhancer\"\"\"\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:517:42\n    |\n515 |         return results\n516 |     \n517 |     def check_all_compatibility(self) -> Dict[str, CompatibilityResult]:\n    |                                          ^^^^\n518 |         \"\"\"Check compatibility of all libraries in the matrix with parallel execution\"\"\"\n519 |         results = {}\n    |\n\nF821 Undefined name `ThreadPoolExecutor`\n   --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:524:14\n    |\n522 |             return lib_name, self.check_library_compatibility(lib_name)\n523 |         \n524 |         with ThreadPoolExecutor(max_workers=min(len(self.library_specs), 10)) as executor:\n    |              ^^^^^^^^^^^^^^^^^^\n525 |             future_to_lib = {executor.submit(check_library, lib): lib for lib in self.library_specs}\n    |\n\nF821 Undefined name `as_completed`\n   --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:527:27\n    |\n525 |             future_to_lib = {executor.submit(check_library, lib): lib for lib in self.library_specs}\n526 |             \n527 |             for future in as_completed(future_to_lib):\n    |                           ^^^^^^^^^^^^\n528 |                 try:\n529 |                     lib_name, result = future.result()\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:541:48\n    |\n540 |     def validate_numerical_stability(self, \n541 |                                    operations: List[str] = None,\n    |                                                ^^^^\n542 |                                    tolerance: float = 1e-10) -> NumericalStabilityResult:\n543 |         \"\"\"\n    |\n\nF821 Undefined name `minimize`\n   --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:655:26\n    |\n653 |                     return np.sum((x - 1)**2)\n654 |                 \n655 |                 result = minimize(objective, np.zeros(5), method='BFGS')\n    |                          ^^^^^^^^\n656 |                 \n657 |                 error = np.max(np.abs(result.x - 1.0))\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:684:43\n    |\n682 |         )\n683 |     \n684 |     def get_compatibility_report(self) -> Dict[str, Any]:\n    |                                           ^^^^\n685 |         \"\"\"Generate comprehensive compatibility report\"\"\"\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:684:53\n    |\n682 |         )\n683 |     \n684 |     def get_compatibility_report(self) -> Dict[str, Any]:\n    |                                                     ^^^\n685 |         \"\"\"Generate comprehensive compatibility report\"\"\"\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:775:42\n    |\n773 |             }\n774 |     \n775 |     def detect_library_versions(self) -> Dict[str, Dict[str, Any]]:\n    |                                          ^^^^\n776 |         \"\"\"\n777 |         Detect installed versions of all libraries with detailed metadata.\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:775:52\n    |\n773 |             }\n774 |     \n775 |     def detect_library_versions(self) -> Dict[str, Dict[str, Any]]:\n    |                                                    ^^^^\n776 |         \"\"\"\n777 |         Detect installed versions of all libraries with detailed metadata.\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:775:62\n    |\n773 |             }\n774 |     \n775 |     def detect_library_versions(self) -> Dict[str, Dict[str, Any]]:\n    |                                                              ^^^\n776 |         \"\"\"\n777 |         Detect installed versions of all libraries with detailed metadata.\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:853:70\n    |\n851 |         return detected_versions\n852 |     \n853 |     def _detect_library_capabilities(self, lib_name: str, module) -> Dict[str, bool]:\n    |                                                                      ^^^^\n854 |         \"\"\"Detect specific capabilities of a library.\"\"\"\n855 |         capabilities = {}\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:886:53\n    |\n885 |     def _calculate_library_health_score(self, lib_name: str, spec: LibrarySpec, \n886 |                                       version_info: Dict[str, Any]) -> float:\n    |                                                     ^^^^\n887 |         \"\"\"Calculate a health score (0-1) for the library installation.\"\"\"\n888 |         if not version_info['installed']:\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:886:63\n    |\n885 |     def _calculate_library_health_score(self, lib_name: str, spec: LibrarySpec, \n886 |                                       version_info: Dict[str, Any]) -> float:\n    |                                                               ^^^\n887 |         \"\"\"Calculate a health score (0-1) for the library installation.\"\"\"\n888 |         if not version_info['installed']:\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:931:51\n    |\n930 |     def validate_behavioral_consistency(self, lib_name: str, \n931 |                                       test_cases: List[Dict[str, Any]] = None) -> Dict[str, Any]:\n    |                                                   ^^^^\n932 |         \"\"\"\n933 |         Validate behavioral consistency between mock and real library implementations.\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:931:56\n    |\n930 |     def validate_behavioral_consistency(self, lib_name: str, \n931 |                                       test_cases: List[Dict[str, Any]] = None) -> Dict[str, Any]:\n    |                                                        ^^^^\n932 |         \"\"\"\n933 |         Validate behavioral consistency between mock and real library implementations.\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:931:66\n    |\n930 |     def validate_behavioral_consistency(self, lib_name: str, \n931 |                                       test_cases: List[Dict[str, Any]] = None) -> Dict[str, Any]:\n    |                                                                  ^^^\n932 |         \"\"\"\n933 |         Validate behavioral consistency between mock and real library implementations.\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:931:83\n    |\n930 |     def validate_behavioral_consistency(self, lib_name: str, \n931 |                                       test_cases: List[Dict[str, Any]] = None) -> Dict[str, Any]:\n    |                                                                                   ^^^^\n932 |         \"\"\"\n933 |         Validate behavioral consistency between mock and real library implementations.\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:931:93\n    |\n930 |     def validate_behavioral_consistency(self, lib_name: str, \n931 |                                       test_cases: List[Dict[str, Any]] = None) -> Dict[str, Any]:\n    |                                                                                             ^^^\n932 |         \"\"\"\n933 |         Validate behavioral consistency between mock and real library implementations.\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:993:70\n    |\n991 |         return result\n992 |     \n993 |     def _generate_default_test_cases(self, lib_name: str, module) -> List[Dict[str, Any]]:\n    |                                                                      ^^^^\n994 |         \"\"\"Generate default test cases for a library.\"\"\"\n995 |         test_cases = []\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:993:75\n    |\n991 |         return result\n992 |     \n993 |     def _generate_default_test_cases(self, lib_name: str, module) -> List[Dict[str, Any]]:\n    |                                                                           ^^^^\n994 |         \"\"\"Generate default test cases for a library.\"\"\"\n995 |         test_cases = []\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:993:85\n    |\n991 |         return result\n992 |     \n993 |     def _generate_default_test_cases(self, lib_name: str, module) -> List[Dict[str, Any]]:\n    |                                                                                     ^^^\n994 |         \"\"\"Generate default test cases for a library.\"\"\"\n995 |         test_cases = []\n    |\n\nF821 Undefined name `Dict`\n    --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:1041:71\n     |\n1039 |         return test_cases\n1040 |     \n1041 |     def _run_consistency_test(self, lib_name: str, module, test_case: Dict[str, Any]) -> Dict[str, Any]:\n     |                                                                       ^^^^\n1042 |         \"\"\"Run a single consistency test.\"\"\"\n1043 |         result = {\n     |\n\nF821 Undefined name `Any`\n    --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:1041:81\n     |\n1039 |         return test_cases\n1040 |     \n1041 |     def _run_consistency_test(self, lib_name: str, module, test_case: Dict[str, Any]) -> Dict[str, Any]:\n     |                                                                                 ^^^\n1042 |         \"\"\"Run a single consistency test.\"\"\"\n1043 |         result = {\n     |\n\nF821 Undefined name `Dict`\n    --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:1041:90\n     |\n1039 |         return test_cases\n1040 |     \n1041 |     def _run_consistency_test(self, lib_name: str, module, test_case: Dict[str, Any]) -> Dict[str, Any]:\n     |                                                                                          ^^^^\n1042 |         \"\"\"Run a single consistency test.\"\"\"\n1043 |         result = {\n     |\n\nF821 Undefined name `Any`\n    --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:1041:100\n     |\n1039 |         return test_cases\n1040 |     \n1041 |     def _run_consistency_test(self, lib_name: str, module, test_case: Dict[str, Any]) -> Dict[str, Any]:\n     |                                                                                                    ^^^\n1042 |         \"\"\"Run a single consistency test.\"\"\"\n1043 |         result = {\n     |\n\nF821 Undefined name `Dict`\n    --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:1071:67\n     |\n1069 |         return result\n1070 |     \n1071 |     def generate_upgrade_recommendations(self, detected_versions: Dict[str, Dict[str, Any]]) -> List[Dict[str, Any]]:\n     |                                                                   ^^^^\n1072 |         \"\"\"Generate automated upgrade recommendations based on compatibility matrix.\"\"\"\n1073 |         recommendations = []\n     |\n\nF821 Undefined name `Dict`\n    --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:1071:77\n     |\n1069 |         return result\n1070 |     \n1071 |     def generate_upgrade_recommendations(self, detected_versions: Dict[str, Dict[str, Any]]) -> List[Dict[str, Any]]:\n     |                                                                             ^^^^\n1072 |         \"\"\"Generate automated upgrade recommendations based on compatibility matrix.\"\"\"\n1073 |         recommendations = []\n     |\n\nF821 Undefined name `Any`\n    --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:1071:87\n     |\n1069 |         return result\n1070 |     \n1071 |     def generate_upgrade_recommendations(self, detected_versions: Dict[str, Dict[str, Any]]) -> List[Dict[str, Any]]:\n     |                                                                                       ^^^\n1072 |         \"\"\"Generate automated upgrade recommendations based on compatibility matrix.\"\"\"\n1073 |         recommendations = []\n     |\n\nF821 Undefined name `List`\n    --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:1071:97\n     |\n1069 |         return result\n1070 |     \n1071 |     def generate_upgrade_recommendations(self, detected_versions: Dict[str, Dict[str, Any]]) -> List[Dict[str, Any]]:\n     |                                                                                                 ^^^^\n1072 |         \"\"\"Generate automated upgrade recommendations based on compatibility matrix.\"\"\"\n1073 |         recommendations = []\n     |\n\nF821 Undefined name `Dict`\n    --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:1071:102\n     |\n1069 |         return result\n1070 |     \n1071 |     def generate_upgrade_recommendations(self, detected_versions: Dict[str, Dict[str, Any]]) -> List[Dict[str, Any]]:\n     |                                                                                                      ^^^^\n1072 |         \"\"\"Generate automated upgrade recommendations based on compatibility matrix.\"\"\"\n1073 |         recommendations = []\n     |\n\nF821 Undefined name `Any`\n    --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:1071:112\n     |\n1069 |         return result\n1070 |     \n1071 |     def generate_upgrade_recommendations(self, detected_versions: Dict[str, Dict[str, Any]]) -> List[Dict[str, Any]]:\n     |                                                                                                                ^^^\n1072 |         \"\"\"Generate automated upgrade recommendations based on compatibility matrix.\"\"\"\n1073 |         recommendations = []\n     |\n\nF821 Undefined name `Dict`\n    --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:1145:48\n     |\n1143 |         return f\">={constraint.min_version}\" + (f\",<={constraint.max_version}\" if constraint.max_version else \"\")\n1144 |     \n1145 |     def generate_comprehensive_report(self) -> Dict[str, Any]:\n     |                                                ^^^^\n1146 |         \"\"\"Generate comprehensive library status report.\"\"\"\n1147 |         # Detect all library versions\n     |\n\nF821 Undefined name `Any`\n    --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:1145:58\n     |\n1143 |         return f\">={constraint.min_version}\" + (f\",<={constraint.max_version}\" if constraint.max_version else \"\")\n1144 |     \n1145 |     def generate_comprehensive_report(self) -> Dict[str, Any]:\n     |                                                          ^^^\n1146 |         \"\"\"Generate comprehensive library status report.\"\"\"\n1147 |         # Detect all library versions\n     |\n\nF821 Undefined name `datetime`\n    --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:1164:40\n     |\n1162 |         report = {\n1163 |             'metadata': {\n1164 |                 'generated_timestamp': datetime.now().isoformat(),\n     |                                        ^^^^^^^^\n1165 |                 'python_version': self.matrix.current_python_version.value,\n1166 |                 'report_version': '1.0',\n     |\n\nF821 Undefined name `Dict`\n    --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:1180:67\n     |\n1178 |         return report\n1179 |     \n1180 |     def _calculate_overall_system_health(self, detected_versions: Dict[str, Dict[str, Any]]) -> Dict[str, Any]:\n     |                                                                   ^^^^\n1181 |         \"\"\"Calculate overall system health metrics.\"\"\"\n1182 |         total_libraries = len(detected_versions)\n     |\n\nF821 Undefined name `Dict`\n    --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:1180:77\n     |\n1178 |         return report\n1179 |     \n1180 |     def _calculate_overall_system_health(self, detected_versions: Dict[str, Dict[str, Any]]) -> Dict[str, Any]:\n     |                                                                             ^^^^\n1181 |         \"\"\"Calculate overall system health metrics.\"\"\"\n1182 |         total_libraries = len(detected_versions)\n     |\n\nF821 Undefined name `Any`\n    --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:1180:87\n     |\n1178 |         return report\n1179 |     \n1180 |     def _calculate_overall_system_health(self, detected_versions: Dict[str, Dict[str, Any]]) -> Dict[str, Any]:\n     |                                                                                       ^^^\n1181 |         \"\"\"Calculate overall system health metrics.\"\"\"\n1182 |         total_libraries = len(detected_versions)\n     |\n\nF821 Undefined name `Dict`\n    --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:1180:97\n     |\n1178 |         return report\n1179 |     \n1180 |     def _calculate_overall_system_health(self, detected_versions: Dict[str, Dict[str, Any]]) -> Dict[str, Any]:\n     |                                                                                                 ^^^^\n1181 |         \"\"\"Calculate overall system health metrics.\"\"\"\n1182 |         total_libraries = len(detected_versions)\n     |\n\nF821 Undefined name `Any`\n    --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:1180:107\n     |\n1178 |         return report\n1179 |     \n1180 |     def _calculate_overall_system_health(self, detected_versions: Dict[str, Dict[str, Any]]) -> Dict[str, Any]:\n     |                                                                                                           ^^^\n1181 |         \"\"\"Calculate overall system health metrics.\"\"\"\n1182 |         total_libraries = len(detected_versions)\n     |\n\nF821 Undefined name `Dict`\n    --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:1215:58\n     |\n1213 |             return \"CRITICAL\"\n1214 |     \n1215 |     def _assess_stage_readiness(self, detected_versions: Dict[str, Dict[str, Any]]) -> Dict[str, Dict[str, Any]]:\n     |                                                          ^^^^\n1216 |         \"\"\"Assess readiness of each stage enhancer based on library availability.\"\"\"\n1217 |         stage_readiness = {}\n     |\n\nF821 Undefined name `Dict`\n    --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:1215:68\n     |\n1213 |             return \"CRITICAL\"\n1214 |     \n1215 |     def _assess_stage_readiness(self, detected_versions: Dict[str, Dict[str, Any]]) -> Dict[str, Dict[str, Any]]:\n     |                                                                    ^^^^\n1216 |         \"\"\"Assess readiness of each stage enhancer based on library availability.\"\"\"\n1217 |         stage_readiness = {}\n     |\n\nF821 Undefined name `Any`\n    --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:1215:78\n     |\n1213 |             return \"CRITICAL\"\n1214 |     \n1215 |     def _assess_stage_readiness(self, detected_versions: Dict[str, Dict[str, Any]]) -> Dict[str, Dict[str, Any]]:\n     |                                                                              ^^^\n1216 |         \"\"\"Assess readiness of each stage enhancer based on library availability.\"\"\"\n1217 |         stage_readiness = {}\n     |\n\nF821 Undefined name `Dict`\n    --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:1215:88\n     |\n1213 |             return \"CRITICAL\"\n1214 |     \n1215 |     def _assess_stage_readiness(self, detected_versions: Dict[str, Dict[str, Any]]) -> Dict[str, Dict[str, Any]]:\n     |                                                                                        ^^^^\n1216 |         \"\"\"Assess readiness of each stage enhancer based on library availability.\"\"\"\n1217 |         stage_readiness = {}\n     |\n\nF821 Undefined name `Dict`\n    --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:1215:98\n     |\n1213 |             return \"CRITICAL\"\n1214 |     \n1215 |     def _assess_stage_readiness(self, detected_versions: Dict[str, Dict[str, Any]]) -> Dict[str, Dict[str, Any]]:\n     |                                                                                                  ^^^^\n1216 |         \"\"\"Assess readiness of each stage enhancer based on library availability.\"\"\"\n1217 |         stage_readiness = {}\n     |\n\nF821 Undefined name `Any`\n    --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:1215:108\n     |\n1213 |             return \"CRITICAL\"\n1214 |     \n1215 |     def _assess_stage_readiness(self, detected_versions: Dict[str, Dict[str, Any]]) -> Dict[str, Dict[str, Any]]:\n     |                                                                                                            ^^^\n1216 |         \"\"\"Assess readiness of each stage enhancer based on library availability.\"\"\"\n1217 |         stage_readiness = {}\n     |\n\nF821 Undefined name `Dict`\n    --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:1249:59\n     |\n1247 |         return stage_readiness\n1248 |     \n1249 |     def _perform_risk_assessment(self, detected_versions: Dict[str, Dict[str, Any]], \n     |                                                           ^^^^\n1250 |                                 consistency_results: Dict[str, Dict[str, Any]]) -> Dict[str, Any]:\n1251 |         \"\"\"Perform risk assessment based on library status and consistency.\"\"\"\n     |\n\nF821 Undefined name `Dict`\n    --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:1249:69\n     |\n1247 |         return stage_readiness\n1248 |     \n1249 |     def _perform_risk_assessment(self, detected_versions: Dict[str, Dict[str, Any]], \n     |                                                                     ^^^^\n1250 |                                 consistency_results: Dict[str, Dict[str, Any]]) -> Dict[str, Any]:\n1251 |         \"\"\"Perform risk assessment based on library status and consistency.\"\"\"\n     |\n\nF821 Undefined name `Any`\n    --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:1249:79\n     |\n1247 |         return stage_readiness\n1248 |     \n1249 |     def _perform_risk_assessment(self, detected_versions: Dict[str, Dict[str, Any]], \n     |                                                                               ^^^\n1250 |                                 consistency_results: Dict[str, Dict[str, Any]]) -> Dict[str, Any]:\n1251 |         \"\"\"Perform risk assessment based on library status and consistency.\"\"\"\n     |\n\nF821 Undefined name `Dict`\n    --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:1250:54\n     |\n1249 |     def _perform_risk_assessment(self, detected_versions: Dict[str, Dict[str, Any]], \n1250 |                                 consistency_results: Dict[str, Dict[str, Any]]) -> Dict[str, Any]:\n     |                                                      ^^^^\n1251 |         \"\"\"Perform risk assessment based on library status and consistency.\"\"\"\n1252 |         risks = []\n     |\n\nF821 Undefined name `Dict`\n    --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:1250:64\n     |\n1249 |     def _perform_risk_assessment(self, detected_versions: Dict[str, Dict[str, Any]], \n1250 |                                 consistency_results: Dict[str, Dict[str, Any]]) -> Dict[str, Any]:\n     |                                                                ^^^^\n1251 |         \"\"\"Perform risk assessment based on library status and consistency.\"\"\"\n1252 |         risks = []\n     |\n\nF821 Undefined name `Any`\n    --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:1250:74\n     |\n1249 |     def _perform_risk_assessment(self, detected_versions: Dict[str, Dict[str, Any]], \n1250 |                                 consistency_results: Dict[str, Dict[str, Any]]) -> Dict[str, Any]:\n     |                                                                          ^^^\n1251 |         \"\"\"Perform risk assessment based on library status and consistency.\"\"\"\n1252 |         risks = []\n     |\n\nF821 Undefined name `Dict`\n    --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:1250:84\n     |\n1249 |     def _perform_risk_assessment(self, detected_versions: Dict[str, Dict[str, Any]], \n1250 |                                 consistency_results: Dict[str, Dict[str, Any]]) -> Dict[str, Any]:\n     |                                                                                    ^^^^\n1251 |         \"\"\"Perform risk assessment based on library status and consistency.\"\"\"\n1252 |         risks = []\n     |\n\nF821 Undefined name `Any`\n    --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:1250:94\n     |\n1249 |     def _perform_risk_assessment(self, detected_versions: Dict[str, Dict[str, Any]], \n1250 |                                 consistency_results: Dict[str, Dict[str, Any]]) -> Dict[str, Any]:\n     |                                                                                              ^^^\n1251 |         \"\"\"Perform risk assessment based on library status and consistency.\"\"\"\n1252 |         risks = []\n     |\n\nF821 Undefined name `Dict`\n    --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:1307:35\n     |\n1305 |         }\n1306 |     \n1307 |     def save_report(self, report: Dict[str, Any] = None) -> str:\n     |                                   ^^^^\n1308 |         \"\"\"Save the comprehensive report to JSON file.\"\"\"\n1309 |         if report is None:\n     |\n\nF821 Undefined name `Any`\n    --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:1307:45\n     |\n1305 |         }\n1306 |     \n1307 |     def save_report(self, report: Dict[str, Any] = None) -> str:\n     |                                             ^^^\n1308 |         \"\"\"Save the comprehensive report to JSON file.\"\"\"\n1309 |         if report is None:\n     |\n\nF821 Undefined name `Path`\n    --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:1315:23\n     |\n1313 | # # #         from pathlib import Path  # Module not found  # Module not found  # Module not found\n1314 |         \n1315 |         report_path = Path(self.report_path)\n     |                       ^^^^\n1316 |         report_path.parent.mkdir(parents=True, exist_ok=True)\n     |\n\nF821 Undefined name `Dict`\n    --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:1323:42\n     |\n1321 |         return str(report_path)\n1322 |     \n1323 |     def invoke_status_reporting(self) -> Dict[str, Any]:\n     |                                          ^^^^\n1324 |         \"\"\"\n1325 |         Main entry point for automated status reporting during pipeline startup.\n     |\n\nF821 Undefined name `Any`\n    --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:1323:52\n     |\n1321 |         return str(report_path)\n1322 |     \n1323 |     def invoke_status_reporting(self) -> Dict[str, Any]:\n     |                                                    ^^^\n1324 |         \"\"\"\n1325 |         Main entry point for automated status reporting during pipeline startup.\n     |\n\nF821 Undefined name `datetime`\n    --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:1352:30\n     |\n1350 |                 'success': False,\n1351 |                 'error': str(e),\n1352 |                 'timestamp': datetime.now().isoformat()\n     |                              ^^^^^^^^\n1353 |             }\n     |\n\nF821 Undefined name `List`\n    --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:1355:41\n     |\n1353 |             }\n1354 |     \n1355 |     def detect_faiss_conflicts(self) -> List[str]:\n     |                                         ^^^^\n1356 |         \"\"\"Detect conflicts between faiss-cpu and faiss-gpu installations\"\"\"\n1357 |         conflicts = []\n     |\n\nF821 Undefined name `List`\n    --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:1399:71\n     |\n1397 |         return conflicts\n1398 |     \n1399 |     def validate_cross_platform_compatibility(self, target_platforms: List[str] = None, \n     |                                                                       ^^^^\n1400 |                                             target_python_versions: List[str] = None) -> List[CrossPlatformResult]:\n1401 |         \"\"\"\n     |\n\nF821 Undefined name `List`\n    --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:1400:69\n     |\n1399 |     def validate_cross_platform_compatibility(self, target_platforms: List[str] = None, \n1400 |                                             target_python_versions: List[str] = None) -> List[CrossPlatformResult]:\n     |                                                                     ^^^^\n1401 |         \"\"\"\n1402 |         Validate compatibility across different platforms and Python versions.\n     |\n\nF821 Undefined name `List`\n    --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:1400:90\n     |\n1399 |     def validate_cross_platform_compatibility(self, target_platforms: List[str] = None, \n1400 |                                             target_python_versions: List[str] = None) -> List[CrossPlatformResult]:\n     |                                                                                          ^^^^\n1401 |         \"\"\"\n1402 |         Validate compatibility across different platforms and Python versions.\n     |\n\nF821 Undefined name `datetime`\n    --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:1474:23\n     |\n1473 |         report = ValidationReport(\n1474 |             timestamp=datetime.now().isoformat(),\n     |                       ^^^^^^^^\n1475 |             python_version=self.current_python_version.value,\n1476 |             platform=self.current_platform\n     |\n\nF821 Undefined name `Tuple`\n    --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:1573:54\n     |\n1571 |             json.dump(report_dict, f, indent=2)\n1572 |     \n1573 |     def unified_pipeline_startup_validation(self) -> Tuple[bool, ValidationReport]:\n     |                                                      ^^^^^\n1574 |         \"\"\"\n1575 |         Unified validation entry point for pipeline startup.\n     |\n\nF821 Undefined name `Tuple`\n    --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:1743:29\n     |\n1742 | # New unified validation functions\n1743 | def startup_validation() -> Tuple[bool, ValidationReport]:\n     |                             ^^^^^\n1744 |     \"\"\"\n1745 |     Unified validation entry point for pipeline startup.\n     |\n\nF821 Undefined name `List`\n    --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:1754:35\n     |\n1754 | def check_faiss_installation() -> List[str]:\n     |                                   ^^^^\n1755 |     \"\"\"Check FAISS installation for conflicts\"\"\"\n1756 |     matrix = get_compatibility_matrix()\n     |\n\nF821 Undefined name `List`\n    --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:1760:40\n     |\n1760 | def validate_cross_platform(platforms: List[str] = None, \n     |                                        ^^^^\n1761 |                            python_versions: List[str] = None) -> List[CrossPlatformResult]:\n1762 |     \"\"\"Validate cross-platform compatibility\"\"\"\n     |\n\nF821 Undefined name `List`\n    --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:1761:45\n     |\n1760 | def validate_cross_platform(platforms: List[str] = None, \n1761 |                            python_versions: List[str] = None) -> List[CrossPlatformResult]:\n     |                                             ^^^^\n1762 |     \"\"\"Validate cross-platform compatibility\"\"\"\n1763 |     matrix = get_compatibility_matrix()\n     |\n\nF821 Undefined name `List`\n    --> canonical_flow/mathematical_enhancers/mathematical_compatibility_matrix.py:1761:66\n     |\n1760 | def validate_cross_platform(platforms: List[str] = None, \n1761 |                            python_versions: List[str] = None) -> List[CrossPlatformResult]:\n     |                                                                  ^^^^\n1762 |     \"\"\"Validate cross-platform compatibility\"\"\"\n1763 |     matrix = get_compatibility_matrix()\n     |\n\ninvalid-syntax: Expected a statement\n  --> canonical_flow/mathematical_enhancers/mathematical_pipeline_coordinator.py:49:5\n   |\n47 |         SimilarityResult,\n48 |         BayesianResult\n49 |     )\n   |     ^\n50 | except ImportError:\n51 |     # Fallback implementations for testing without full installation\n   |\n\ninvalid-syntax: Expected a statement\n  --> canonical_flow/mathematical_enhancers/mathematical_pipeline_coordinator.py:49:6\n   |\n47 |         SimilarityResult,\n48 |         BayesianResult\n49 |     )\n   |      ^\n50 | except ImportError:\n51 |     # Fallback implementations for testing without full installation\n   |\n\ninvalid-syntax: Expected an expression\n    --> canonical_flow/mathematical_enhancers/orchestration_enhancer.py:1023:9\n     |\n1021 |             metrics.control_effort < self.stability_bounds.max_control_effort and\n1022 | # # #             stability_measure < 1.0  # Additional stability criterion from tensor category optimization  # Module not found  #\u2026\n1023 |         )\n     |         ^\n1024 |         \n1025 |         # Update pipeline state with enhanced values\n     |\n\ninvalid-syntax: Unexpected indentation\n  --> canonical_flow/mathematical_enhancers/pre_flight_validator.py:32:1\n   |\n30 | # Import the existing compatibility matrix\n31 | # # # from .mathematical_compatibility_matrix import (  # Module not found  # Module not found  # Module not found\n32 |     MathematicalCompatibilityMatrix,\n   | ^^^^\n33 |     PythonVersion,\n34 |     StageEnhancer,\n   |\n\ninvalid-syntax: Expected a statement\n  --> canonical_flow/mathematical_enhancers/pre_flight_validator.py:38:1\n   |\n36 |     LibrarySpec,\n37 |     CompatibilityResult\n38 | )\n   | ^\n39 |\n40 | # Configure logging\n   |\n\ninvalid-syntax: Expected a statement\n  --> canonical_flow/mathematical_enhancers/pre_flight_validator.py:38:2\n   |\n36 |     LibrarySpec,\n37 |     CompatibilityResult\n38 | )\n   |  ^\n39 |\n40 | # Configure logging\n   |\n\nF401 [*] `math` imported but unused\n  --> canonical_flow/mathematical_enhancers/retrieval_enhancer.py:23:8\n   |\n21 | # # # from __future__ import annotations  # Module not found  # Module not found  # Module not found\n22 |\n23 | import math\n   |        ^^^^\n24 | import os\n25 | import warnings\n   |\nhelp: Remove unused import: `math`\n\nF821 Undefined name `Dict`\n  --> canonical_flow/mathematical_enhancers/retrieval_enhancer.py:43:45\n   |\n41 | # # # #     from orchestration.event_bus import publish_metric  # type: ignore  # Module not found  # Module not found  # Module not f\u2026\n42 | except Exception:  # noqa: BLE001\n43 |     def publish_metric(topic: str, payload: Dict[str, Any]) -> None:  # type: ignore\n   |                                             ^^^^\n44 |         return None\n   |\n\nF821 Undefined name `Any`\n  --> canonical_flow/mathematical_enhancers/retrieval_enhancer.py:43:55\n   |\n41 | # # # #     from orchestration.event_bus import publish_metric  # type: ignore  # Module not found  # Module not found  # Module not f\u2026\n42 | except Exception:  # noqa: BLE001\n43 |     def publish_metric(topic: str, payload: Dict[str, Any]) -> None:  # type: ignore\n   |                                                       ^^^\n44 |         return None\n   |\n\nF821 Undefined name `cosine_similarity`\n  --> canonical_flow/mathematical_enhancers/retrieval_enhancer.py:78:22\n   |\n76 |     if method == \"cosine\":\n77 |         # Cosine similarity\n78 |         similarity = cosine_similarity(embeddings)\n   |                      ^^^^^^^^^^^^^^^^^\n79 |         \n80 |     elif method == \"rbf\":\n   |\n\nF821 Undefined name `rbf_kernel`\n  --> canonical_flow/mathematical_enhancers/retrieval_enhancer.py:82:22\n   |\n80 |     elif method == \"rbf\":\n81 |         # RBF (Gaussian) kernel  \n82 |         similarity = rbf_kernel(embeddings, gamma=1.0 / (2 * sigma**2))\n   |                      ^^^^^^^^^^\n83 |         \n84 |     elif method == \"linear\":\n   |\n\nF821 Undefined name `Tuple`\n   --> canonical_flow/mathematical_enhancers/retrieval_enhancer.py:115:62\n    |\n113 | def _compute_graph_laplacian(similarity_matrix: np.ndarray, \n114 |                             normalized: bool = True,\n115 |                             regularization: float = 1e-8) -> Tuple[np.ndarray, np.ndarray]:\n    |                                                              ^^^^^\n116 |     \"\"\"\n117 | # # #     Compute graph Laplacian matrix from similarity matrix.  # Module not found  # Module not found  # Module not found\n    |\n\nF821 Undefined name `diags`\n   --> canonical_flow/mathematical_enhancers/retrieval_enhancer.py:139:34\n    |\n137 |         # Normalized Laplacian: L_norm = I - D^(-1/2) W D^(-1/2)\n138 |         degree_sqrt_inv = np.where(degree > 0, 1.0 / np.sqrt(degree), 0)\n139 |         degree_matrix_sqrt_inv = diags(degree_sqrt_inv)\n    |                                  ^^^^^\n140 |         \n141 |         if similarity_matrix.shape[0] > 1000:\n    |\n\nF821 Undefined name `csr_matrix`\n   --> canonical_flow/mathematical_enhancers/retrieval_enhancer.py:143:33\n    |\n141 |         if similarity_matrix.shape[0] > 1000:\n142 |             # Use sparse matrices for large graphs\n143 |             similarity_sparse = csr_matrix(similarity_matrix)\n    |                                 ^^^^^^^^^^\n144 |             laplacian = diags(np.ones(len(degree))) - degree_matrix_sqrt_inv @ similarity_sparse @ degree_matrix_sqrt_inv\n145 |             laplacian = laplacian.toarray()\n    |\n\nF821 Undefined name `diags`\n   --> canonical_flow/mathematical_enhancers/retrieval_enhancer.py:144:25\n    |\n142 |             # Use sparse matrices for large graphs\n143 |             similarity_sparse = csr_matrix(similarity_matrix)\n144 |             laplacian = diags(np.ones(len(degree))) - degree_matrix_sqrt_inv @ similarity_sparse @ degree_matrix_sqrt_inv\n    |                         ^^^^^\n145 |             laplacian = laplacian.toarray()\n146 |         else:\n    |\n\nF821 Undefined name `Tuple`\n   --> canonical_flow/mathematical_enhancers/retrieval_enhancer.py:159:54\n    |\n157 | def _compute_eigendecomposition(matrix: np.ndarray, \n158 |                                k: int = 10,\n159 |                                which: str = 'SM') -> Tuple[np.ndarray, np.ndarray, Dict[str, float]]:\n    |                                                      ^^^^^\n160 |     \"\"\"\n161 |     Compute eigendecomposition with stability analysis.\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/mathematical_enhancers/retrieval_enhancer.py:159:84\n    |\n157 | def _compute_eigendecomposition(matrix: np.ndarray, \n158 |                                k: int = 10,\n159 |                                which: str = 'SM') -> Tuple[np.ndarray, np.ndarray, Dict[str, float]]:\n    |                                                                                    ^^^^\n160 |     \"\"\"\n161 |     Compute eigendecomposition with stability analysis.\n    |\n\nF821 Undefined name `eigsh`\n   --> canonical_flow/mathematical_enhancers/retrieval_enhancer.py:180:36\n    |\n178 |         # Use sparse eigenvalue solver for efficiency\n179 |         if n > 100 and which in ['SM', 'LM']:\n180 |             eigenvals, eigenvecs = eigsh(matrix, k=k, which=which, tol=1e-10, maxiter=1000)\n    |                                    ^^^^^\n181 |         else:\n182 |             # Full eigendecomposition for smaller matrices\n    |\n\nF821 Undefined name `linalg`\n   --> canonical_flow/mathematical_enhancers/retrieval_enhancer.py:183:36\n    |\n181 |         else:\n182 |             # Full eigendecomposition for smaller matrices\n183 |             eigenvals, eigenvecs = linalg.eigh(matrix)\n    |                                    ^^^^^^\n184 |             if which == 'SM':\n185 |                 eigenvals, eigenvecs = eigenvals[:k], eigenvecs[:, :k]\n    |\n\nF821 Undefined name `ArpackError`\n   --> canonical_flow/mathematical_enhancers/retrieval_enhancer.py:189:13\n    |\n187 |                 eigenvals, eigenvecs = eigenvals[-k:], eigenvecs[:, -k:]\n188 |                 \n189 |     except (ArpackError, linalg.LinAlgError) as e:\n    |             ^^^^^^^^^^^\n190 |         warnings.warn(f\"Eigendecomposition failed: {e}. Using fallback method.\")\n191 |         # Fallback to regularized version\n    |\n\nF821 Undefined name `linalg`\n   --> canonical_flow/mathematical_enhancers/retrieval_enhancer.py:189:26\n    |\n187 |                 eigenvals, eigenvecs = eigenvals[-k:], eigenvecs[:, -k:]\n188 |                 \n189 |     except (ArpackError, linalg.LinAlgError) as e:\n    |                          ^^^^^^\n190 |         warnings.warn(f\"Eigendecomposition failed: {e}. Using fallback method.\")\n191 |         # Fallback to regularized version\n    |\n\nF821 Undefined name `linalg`\n   --> canonical_flow/mathematical_enhancers/retrieval_enhancer.py:193:32\n    |\n191 |         # Fallback to regularized version\n192 |         regularized_matrix = matrix + 1e-6 * np.eye(n)\n193 |         eigenvals, eigenvecs = linalg.eigh(regularized_matrix)\n    |                                ^^^^^^\n194 |         if which == 'SM':\n195 |             eigenvals, eigenvecs = eigenvals[:k], eigenvecs[:, :k]\n    |\n\nF821 Undefined name `linalg`\n   --> canonical_flow/mathematical_enhancers/retrieval_enhancer.py:212:33\n    |\n210 |         \"spectral_gap\": float(abs(spectral_gap)),\n211 |         \"eigenval_range\": [float(np.min(eigenvals)), float(np.max(eigenvals))],\n212 |         \"frobenius_norm\": float(linalg.norm(matrix, 'fro')),\n    |                                 ^^^^^^\n213 |         \"nuclear_norm\": float(np.sum(np.abs(eigenvals))),\n214 |     }\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/mathematical_enhancers/retrieval_enhancer.py:219:45\n    |\n219 | def _spectral_clustering_rerank(candidates: List[Dict[str, Any]], \n    |                                             ^^^^\n220 |                                embeddings: np.ndarray,\n221 |                                n_clusters: int = 5,\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/mathematical_enhancers/retrieval_enhancer.py:219:50\n    |\n219 | def _spectral_clustering_rerank(candidates: List[Dict[str, Any]], \n    |                                                  ^^^^\n220 |                                embeddings: np.ndarray,\n221 |                                n_clusters: int = 5,\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/mathematical_enhancers/retrieval_enhancer.py:219:60\n    |\n219 | def _spectral_clustering_rerank(candidates: List[Dict[str, Any]], \n    |                                                            ^^^\n220 |                                embeddings: np.ndarray,\n221 |                                n_clusters: int = 5,\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/mathematical_enhancers/retrieval_enhancer.py:222:64\n    |\n220 |                                embeddings: np.ndarray,\n221 |                                n_clusters: int = 5,\n222 |                                cluster_weight: float = 0.3) -> List[Dict[str, Any]]:\n    |                                                                ^^^^\n223 |     \"\"\"\n224 |     Rerank candidates using spectral clustering to promote semantic coherence.\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/mathematical_enhancers/retrieval_enhancer.py:222:69\n    |\n220 |                                embeddings: np.ndarray,\n221 |                                n_clusters: int = 5,\n222 |                                cluster_weight: float = 0.3) -> List[Dict[str, Any]]:\n    |                                                                     ^^^^\n223 |     \"\"\"\n224 |     Rerank candidates using spectral clustering to promote semantic coherence.\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/mathematical_enhancers/retrieval_enhancer.py:222:79\n    |\n220 |                                embeddings: np.ndarray,\n221 |                                n_clusters: int = 5,\n222 |                                cluster_weight: float = 0.3) -> List[Dict[str, Any]]:\n    |                                                                               ^^^\n223 |     \"\"\"\n224 |     Rerank candidates using spectral clustering to promote semantic coherence.\n    |\n\nF821 Undefined name `SpectralClustering`\n   --> canonical_flow/mathematical_enhancers/retrieval_enhancer.py:249:20\n    |\n247 |     try:\n248 |         # Perform spectral clustering\n249 |         spectral = SpectralClustering(\n    |                    ^^^^^^^^^^^^^^^^^^\n250 |             n_clusters=n_clusters,\n251 |             affinity='cosine',\n    |\n\nF821 Undefined name `cosine_similarity`\n   --> canonical_flow/mathematical_enhancers/retrieval_enhancer.py:264:37\n    |\n262 |                 cluster_embeddings = embeddings[cluster_mask]\n263 |                 # Intra-cluster similarity (higher is better)\n264 |                 intra_sim = np.mean(cosine_similarity(cluster_embeddings))\n    |                                     ^^^^^^^^^^^^^^^^^\n265 |                 cluster_scores[cluster_id] = intra_sim\n266 |             else:\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/mathematical_enhancers/retrieval_enhancer.py:301:71\n    |\n299 | def _validate_eigenvalue_stability(eigenvals: np.ndarray, \n300 |                                  matrix: np.ndarray,\n301 |                                  perturbation_scale: float = 1e-6) -> Dict[str, float]:\n    |                                                                       ^^^^\n302 |     \"\"\"\n303 |     Validate eigenvalue stability using perturbation theory.\n    |\n\nF821 Undefined name `linalg`\n   --> canonical_flow/mathematical_enhancers/retrieval_enhancer.py:328:34\n    |\n326 |     try:\n327 |         perturbed_matrix = matrix + perturbation\n328 |         perturbed_eigenvals, _ = linalg.eigh(perturbed_matrix)\n    |                                  ^^^^^^\n329 |         \n330 |         # Match eigenvalues by sorting\n    |\n\nF821 Undefined name `linalg`\n   --> canonical_flow/mathematical_enhancers/retrieval_enhancer.py:340:29\n    |\n339 |         # Compute stability metrics\n340 |         perturbation_norm = linalg.norm(perturbation, ord=2)\n    |                             ^^^^^^\n341 |         eigenval_changes = np.abs(eigenvals_matched - perturbed_matched)\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/mathematical_enhancers/retrieval_enhancer.py:375:59\n    |\n373 | def _validate_spectral_properties(similarity_matrix: np.ndarray,\n374 |                                 laplacian: np.ndarray,\n375 |                                 eigenvals: np.ndarray) -> Dict[str, Any]:\n    |                                                           ^^^^\n376 |     \"\"\"\n377 |     Validate mathematical properties of spectral decomposition.\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/mathematical_enhancers/retrieval_enhancer.py:375:69\n    |\n373 | def _validate_spectral_properties(similarity_matrix: np.ndarray,\n374 |                                 laplacian: np.ndarray,\n375 |                                 eigenvals: np.ndarray) -> Dict[str, Any]:\n    |                                                                     ^^^\n376 |     \"\"\"\n377 |     Validate mathematical properties of spectral decomposition.\n    |\n\nF821 Undefined name `linalg`\n   --> canonical_flow/mathematical_enhancers/retrieval_enhancer.py:390:22\n    |\n389 |     # 1. Symmetry check\n390 |     symmetry_error = linalg.norm(similarity_matrix - similarity_matrix.T, 'fro')\n    |                      ^^^^^^\n391 |     validation_results[\"symmetry_error\"] = float(symmetry_error)\n392 |     validation_results[\"is_symmetric\"] = symmetry_error < 1e-10\n    |\n\nF821 Undefined name `linalg`\n   --> canonical_flow/mathematical_enhancers/retrieval_enhancer.py:396:25\n    |\n394 |     # 2. Positive semi-definiteness of similarity matrix\n395 |     try:\n396 |         sim_eigenvals = linalg.eigvals(similarity_matrix)\n    |                         ^^^^^^\n397 |         min_sim_eigenval = np.min(np.real(sim_eigenvals))  # Take real part to avoid complex warning\n398 |         validation_results[\"similarity_psd\"] = min_sim_eigenval >= -1e-10\n    |\n\nE722 Do not use bare `except`\n   --> canonical_flow/mathematical_enhancers/retrieval_enhancer.py:400:5\n    |\n398 |         validation_results[\"similarity_psd\"] = min_sim_eigenval >= -1e-10\n399 |         validation_results[\"similarity_min_eigenval\"] = float(min_sim_eigenval)\n400 |     except:\n    |     ^^^^^^\n401 |         validation_results[\"similarity_psd\"] = False\n402 |         validation_results[\"similarity_min_eigenval\"] = float('nan')\n    |\n\nF821 Undefined name `linalg`\n   --> canonical_flow/mathematical_enhancers/retrieval_enhancer.py:440:46\n    |\n438 |         # Basic reconstruction check for small matrices\n439 |         if laplacian.shape[0] < 500:\n440 |             full_eigenvals, full_eigenvecs = linalg.eigh(laplacian)\n    |                                              ^^^^^^\n441 |             reconstructed = full_eigenvecs @ np.diag(full_eigenvals) @ full_eigenvecs.T\n442 |             reconstruction_error = linalg.norm(laplacian - reconstructed, 'fro')\n    |\n\nF821 Undefined name `linalg`\n   --> canonical_flow/mathematical_enhancers/retrieval_enhancer.py:442:36\n    |\n440 |             full_eigenvals, full_eigenvecs = linalg.eigh(laplacian)\n441 |             reconstructed = full_eigenvecs @ np.diag(full_eigenvals) @ full_eigenvecs.T\n442 |             reconstruction_error = linalg.norm(laplacian - reconstructed, 'fro')\n    |                                    ^^^^^^\n443 |     except:\n444 |         reconstruction_error = float('inf')\n    |\n\nE722 Do not use bare `except`\n   --> canonical_flow/mathematical_enhancers/retrieval_enhancer.py:443:5\n    |\n441 |             reconstructed = full_eigenvecs @ np.diag(full_eigenvals) @ full_eigenvecs.T\n442 |             reconstruction_error = linalg.norm(laplacian - reconstructed, 'fro')\n443 |     except:\n    |     ^^^^^^\n444 |         reconstruction_error = float('inf')\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/mathematical_enhancers/retrieval_enhancer.py:452:19\n    |\n452 | def process(data: Any, context: Dict[str, Any] | None = None) -> Dict[str, Any]:\n    |                   ^^^\n453 |     \"\"\"\n454 |     Main processing function for spectral graph theory retrieval enhancement.\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/mathematical_enhancers/retrieval_enhancer.py:452:33\n    |\n452 | def process(data: Any, context: Dict[str, Any] | None = None) -> Dict[str, Any]:\n    |                                 ^^^^\n453 |     \"\"\"\n454 |     Main processing function for spectral graph theory retrieval enhancement.\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/mathematical_enhancers/retrieval_enhancer.py:452:43\n    |\n452 | def process(data: Any, context: Dict[str, Any] | None = None) -> Dict[str, Any]:\n    |                                           ^^^\n453 |     \"\"\"\n454 |     Main processing function for spectral graph theory retrieval enhancement.\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/mathematical_enhancers/retrieval_enhancer.py:452:66\n    |\n452 | def process(data: Any, context: Dict[str, Any] | None = None) -> Dict[str, Any]:\n    |                                                                  ^^^^\n453 |     \"\"\"\n454 |     Main processing function for spectral graph theory retrieval enhancement.\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/mathematical_enhancers/retrieval_enhancer.py:452:76\n    |\n452 | def process(data: Any, context: Dict[str, Any] | None = None) -> Dict[str, Any]:\n    |                                                                            ^^^\n453 |     \"\"\"\n454 |     Main processing function for spectral graph theory retrieval enhancement.\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/mathematical_enhancers/retrieval_enhancer.py:465:16\n    |\n463 |     ctx = context or {}\n464 |     debug = bool(ctx.get(\"debug\", False))\n465 |     trace_log: List[Dict[str, Any]] = []\n    |                ^^^^\n466 |     \n467 | # # #     # Extract candidates and embeddings from input  # Module not found  # Module not found  # Module not found\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/mathematical_enhancers/retrieval_enhancer.py:465:21\n    |\n463 |     ctx = context or {}\n464 |     debug = bool(ctx.get(\"debug\", False))\n465 |     trace_log: List[Dict[str, Any]] = []\n    |                     ^^^^\n466 |     \n467 | # # #     # Extract candidates and embeddings from input  # Module not found  # Module not found  # Module not found\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/mathematical_enhancers/retrieval_enhancer.py:465:31\n    |\n463 |     ctx = context or {}\n464 |     debug = bool(ctx.get(\"debug\", False))\n465 |     trace_log: List[Dict[str, Any]] = []\n    |                               ^^^\n466 |     \n467 | # # #     # Extract candidates and embeddings from input  # Module not found  # Module not found  # Module not found\n    |\n\nF821 Undefined name `HyperbolicTensorNetworks`\n   --> canonical_flow/mathematical_enhancers/retrieval_enhancer.py:515:29\n    |\n514 |     # Initialize Hyperbolic Tensor Networks\n515 |     hyperbolic_tensor_net = HyperbolicTensorNetworks(\n    |                             ^^^^^^^^^^^^^^^^^^^^^^^^\n516 |         embedding_dim=embeddings.shape[1],\n517 |         tensor_rank=ctx.get(\"tensor_rank\", 8),\n    |\n\nF821 Undefined name `regularization`\n   --> canonical_flow/mathematical_enhancers/retrieval_enhancer.py:518:24\n    |\n516 |         embedding_dim=embeddings.shape[1],\n517 |         tensor_rank=ctx.get(\"tensor_rank\", 8),\n518 |         regularization=regularization\n    |                        ^^^^^^^^^^^^^^\n519 |     )\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/mathematical_enhancers/retrieval_enhancer.py:674:10\n    |\n673 |     # Build output\n674 |     out: Dict[str, Any] = {}\n    |          ^^^^\n675 |     if isinstance(data, dict):\n676 |         out.update(data)\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/mathematical_enhancers/retrieval_enhancer.py:674:20\n    |\n673 |     # Build output\n674 |     out: Dict[str, Any] = {}\n    |                    ^^^\n675 |     if isinstance(data, dict):\n676 |         out.update(data)\n    |\n\nF401 [*] `json` imported but unused\n  --> canonical_flow/mathematical_enhancers/scoring_enhancer.py:19:8\n   |\n18 | import hashlib\n19 | import json\n   |        ^^^^\n20 | import logging\n21 | # # # from dataclasses import dataclass  # Module not found  # Module not found  # Module not found\n   |\nhelp: Remove unused import: `json`\n\nF401 `ot` imported but unused; consider using `importlib.util.find_spec` to test for availability\n  --> canonical_flow/mathematical_enhancers/scoring_enhancer.py:32:12\n   |\n31 | try:\n32 |     import ot  # Python Optimal Transport\n   |            ^^\n33 |     HAS_POT = True\n34 | except ImportError:\n   |\nhelp: Remove unused import: `ot`\n\nF821 Undefined name `dataclass`\n  --> canonical_flow/mathematical_enhancers/scoring_enhancer.py:40:2\n   |\n40 | @dataclass\n   |  ^^^^^^^^^\n41 | class LyapunovBound:\n42 |     \"\"\"Lyapunov stability bound for scoring consistency.\"\"\"\n   |\n\nF821 Undefined name `dataclass`\n  --> canonical_flow/mathematical_enhancers/scoring_enhancer.py:49:2\n   |\n49 | @dataclass \n   |  ^^^^^^^^^\n50 | class StabilityAnalysis:\n51 |     \"\"\"Complete stability analysis for scoring system.\"\"\"\n   |\n\nF821 Undefined name `dataclass`\n  --> canonical_flow/mathematical_enhancers/scoring_enhancer.py:58:2\n   |\n58 | @dataclass\n   |  ^^^^^^^^^\n59 | class TransportAlignment:\n60 |     \"\"\"Optimal transport alignment between evaluations and standards.\"\"\"\n   |\n\nF821 Undefined name `dataclass`\n  --> canonical_flow/mathematical_enhancers/scoring_enhancer.py:68:2\n   |\n68 | @dataclass\n   |  ^^^^^^^^^\n69 | class EnhancedScoringResult:\n70 |     \"\"\"Enhanced scoring result with optimal transport validation.\"\"\"\n   |\n\nF821 Undefined name `Dict`\n  --> canonical_flow/mathematical_enhancers/scoring_enhancer.py:75:30\n   |\n73 |     alignment_confidence: float\n74 |     stability_verified: bool\n75 |     dnp_compliance_evidence: Dict[str, float]\n   |                              ^^^^\n76 |     mathematical_certificates: Dict[str, Any]\n   |\n\nF821 Undefined name `Dict`\n  --> canonical_flow/mathematical_enhancers/scoring_enhancer.py:76:32\n   |\n74 |     stability_verified: bool\n75 |     dnp_compliance_evidence: Dict[str, float]\n76 |     mathematical_certificates: Dict[str, Any]\n   |                                ^^^^\n   |\n\nF821 Undefined name `Any`\n  --> canonical_flow/mathematical_enhancers/scoring_enhancer.py:76:42\n   |\n74 |     stability_verified: bool\n75 |     dnp_compliance_evidence: Dict[str, float]\n76 |     mathematical_certificates: Dict[str, Any]\n   |                                          ^^^\n   |\n\nF821 Undefined name `eigvals`\n  --> canonical_flow/mathematical_enhancers/scoring_enhancer.py:98:23\n   |\n96 |             Lyapunov bound with stability verification\n97 |         \"\"\"\n98 |         eigenvalues = eigvals(jacobian)\n   |                       ^^^^^^^\n99 |         spectral_radius = max(abs(eigenvalues))\n   |\n\nF821 Undefined name `List`\n   --> canonical_flow/mathematical_enhancers/scoring_enhancer.py:117:30\n    |\n115 |         self,\n116 |         scoring_function: callable,\n117 |         input_distributions: List[np.ndarray],\n    |                              ^^^^\n118 |         perturbation_radius: float = 0.1\n119 |     ) -> bool:\n    |\n\nF821 Undefined name `Tuple`\n   --> canonical_flow/mathematical_enhancers/scoring_enhancer.py:201:10\n    |\n199 |         evaluation_features: np.ndarray,\n200 |         dnp_standards: np.ndarray\n201 |     ) -> Tuple[np.ndarray, np.ndarray]:\n    |          ^^^^^\n202 |         \"\"\"\n203 |         Compute cost matrices for Gromov-Wasserstein transport.\n    |\n\nF821 Undefined name `cdist`\n   --> canonical_flow/mathematical_enhancers/scoring_enhancer.py:213:14\n    |\n211 |         \"\"\"\n212 |         # Evaluation cost matrix (distances between evaluations)\n213 |         C1 = cdist(evaluation_features, evaluation_features, metric='euclidean')\n    |              ^^^^^\n214 |         \n215 |         # Standards cost matrix (distances between DNP standards)\n    |\n\nF821 Undefined name `cdist`\n   --> canonical_flow/mathematical_enhancers/scoring_enhancer.py:216:14\n    |\n215 |         # Standards cost matrix (distances between DNP standards)\n216 |         C2 = cdist(dnp_standards, dnp_standards, metric='euclidean')\n    |              ^^^^^\n217 |         \n218 |         return C1, C2\n    |\n\nF821 Undefined name `Tuple`\n   --> canonical_flow/mathematical_enhancers/scoring_enhancer.py:225:10\n    |\n223 |         source_weights: np.ndarray,\n224 |         target_weights: np.ndarray\n225 |     ) -> Tuple[np.ndarray, Dict[str, Any]]:\n    |          ^^^^^\n226 |         \"\"\"\n227 |         Stabilized Sinkhorn algorithm for entropic optimal transport.\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/mathematical_enhancers/scoring_enhancer.py:225:28\n    |\n223 |         source_weights: np.ndarray,\n224 |         target_weights: np.ndarray\n225 |     ) -> Tuple[np.ndarray, Dict[str, Any]]:\n    |                            ^^^^\n226 |         \"\"\"\n227 |         Stabilized Sinkhorn algorithm for entropic optimal transport.\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/mathematical_enhancers/scoring_enhancer.py:225:38\n    |\n223 |         source_weights: np.ndarray,\n224 |         target_weights: np.ndarray\n225 |     ) -> Tuple[np.ndarray, Dict[str, Any]]:\n    |                                      ^^^\n226 |         \"\"\"\n227 |         Stabilized Sinkhorn algorithm for entropic optimal transport.\n    |\n\nF821 Undefined name `norm`\n   --> canonical_flow/mathematical_enhancers/scoring_enhancer.py:260:21\n    |\n259 |             # Check convergence\n260 |             error = norm(u - u_prev, ord=np.inf)\n    |                     ^^^^\n261 |             convergence_history.append(error)\n    |\n\nE741 Ambiguous variable name: `l`\n   --> canonical_flow/mathematical_enhancers/scoring_enhancer.py:319:29\n    |\n317 |                     cost_contribution = 0.0\n318 |                     for k in range(n1):\n319 |                         for l in range(n2):\n    |                             ^\n320 |                             cost_contribution += (C1[i, k] - C2[j, l]) ** 2 * T[k, l]\n321 |                     grad[i, j] = 4.0 * cost_contribution\n    |\n\nF821 Undefined name `norm`\n   --> canonical_flow/mathematical_enhancers/scoring_enhancer.py:339:32\n    |\n338 |             # Check convergence\n339 |             transport_change = norm(T - T_prev, ord='fro')\n    |                                ^^^^\n340 |             if transport_change < self.tolerance:\n341 |                 break\n    |\n\nE741 Ambiguous variable name: `l`\n   --> canonical_flow/mathematical_enhancers/scoring_enhancer.py:378:25\n    |\n376 |             for j in range(n2):\n377 |                 for k in range(n1):\n378 |                     for l in range(n2):\n    |                         ^\n379 |                         cost += (C1[i, k] - C2[j, l]) ** 2 * T[i, j] * T[k, l]\n    |\n\nF821 Undefined name `norm`\n   --> canonical_flow/mathematical_enhancers/scoring_enhancer.py:401:25\n    |\n399 |         # Use spectral approximation for efficiency\n400 |         try:\n401 |             grad_norm = norm(4 * (np.kron(C1 ** 2, np.ones((1, n2))) - np.kron(C1, C2.T)))\n    |                         ^^^^\n402 |         except np.linalg.LinAlgError:\n403 |             # Fallback for numerical issues\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/mathematical_enhancers/scoring_enhancer.py:422:28\n    |\n420 |     def enhance_adaptive_scoring(\n421 |         self,\n422 |         evaluation_scores: Dict[str, float],\n    |                            ^^^^\n423 |         evaluation_features: np.ndarray,\n424 |         dnp_standards: np.ndarray,\n    |\n\nF821 Undefined name `Optional`\n   --> canonical_flow/mathematical_enhancers/scoring_enhancer.py:425:22\n    |\n423 |         evaluation_features: np.ndarray,\n424 |         dnp_standards: np.ndarray,\n425 |         dnp_weights: Optional[Dict[str, float]] = None\n    |                      ^^^^^^^^\n426 |     ) -> EnhancedScoringResult:\n427 |         \"\"\"\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/mathematical_enhancers/scoring_enhancer.py:425:31\n    |\n423 |         evaluation_features: np.ndarray,\n424 |         dnp_standards: np.ndarray,\n425 |         dnp_weights: Optional[Dict[str, float]] = None\n    |                               ^^^^\n426 |     ) -> EnhancedScoringResult:\n427 |         \"\"\"\n    |\n\nF821 Undefined name `Dict`\n   --> canonical_flow/mathematical_enhancers/scoring_enhancer.py:552:28\n    |\n550 |     def enhance_causal_correction_scoring(\n551 |         self,\n552 |         evaluation_scores: Dict[str, float],\n    |                            ^^^^\n553 |         pdt_context: Any,\n554 |         document_features: Optional[np.ndarray] = None\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/mathematical_enhancers/scoring_enhancer.py:553:22\n    |\n551 |         self,\n552 |         evaluation_scores: Dict[str, float],\n553 |         pdt_context: Any,\n    |                      ^^^\n554 |         document_features: Optional[np.ndarray] = None\n555 |     ) -> EnhancedScoringResult:\n    |\n\nF821 Undefined name `Optional`\n   --> canonical_flow/mathematical_enhancers/scoring_enhancer.py:554:28\n    |\n552 |         evaluation_scores: Dict[str, float],\n553 |         pdt_context: Any,\n554 |         document_features: Optional[np.ndarray] = None\n    |                            ^^^^^^^^\n555 |     ) -> EnhancedScoringResult:\n556 |         \"\"\"\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/mathematical_enhancers/scoring_enhancer.py:591:59\n    |\n589 |         )\n590 |     \n591 |     def _extract_features_from_context(self, pdt_context: Any) -> np.ndarray:\n    |                                                           ^^^\n592 | # # #         \"\"\"Extract feature matrix from PDT context.\"\"\"  # Module not found  # Module not found  # Module not found\n593 | # # #         # Create feature vector from PDT context attributes  # Module not found  # Module not found  # Module not found\n    |\n\nF821 Undefined name `List`\n   --> canonical_flow/mathematical_enhancers/scoring_enhancer.py:607:24\n    |\n605 |         self,\n606 |         scoring_function: callable,\n607 |         test_contexts: List[Any]\n    |                        ^^^^\n608 |     ) -> bool:\n609 |         \"\"\"\n    |\n\nF821 Undefined name `Any`\n   --> canonical_flow/mathematical_enhancers/scoring_enhancer.py:607:29\n    |\n605 |         self,\n606 |         scoring_function: callable,\n607 |         test_contexts: List[Any]\n    |                             ^^^\n608 |     ) -> bool:\n609 |         \"\"\"\n    |\n\ninvalid-syntax: Expected an indented block after `if` statement\n   --> canonical_flow/pipeline_state_manager.py:453:9\n    |\n451 | # # #             return self.PIPELINE_STAGES[0]  # Start from beginning  # Module not found  # Module not found  # Module not found\n452 |         \n453 |         doc_state = self.document_states[document_id]\n    |         ^^^^^^^^^\n454 |         \n455 |         for stage_name in self.PIPELINE_STAGES:\n    |\n\nFound 2526 errors.\n[*] 56 fixable with the `--fix` option (26 hidden fixes can be enabled with the `--unsafe-fixes` option).\n\n"
  ],
  "summary": {
    "files_moved": 1,
    "files_with_import_changes": 0,
    "total_import_changes": 0,
    "validation_errors": 317
  }
}