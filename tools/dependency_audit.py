#!/usr/bin/env python3
"""
Static dependency auditor for FARFAN-ULTIMATE.
- Parses requirements.txt to get declared deps.
- Loads project_analysis_report.json (if present) generated by ProjectAnalyzer to collect imports.
- Computes third-party imports used but not declared, duplicates, and heuristic conflicts.
- Emits canonical_flow/dependency_audit.json with findings and recommendations.

This tool does not install or import external packages; it uses only stdlib and JSON inputs.
"""
# # # from __future__ import annotations  # Module not found  # Module not found  # Module not found
import json
import re
import sys
# # # from pathlib import Path  # Module not found  # Module not found  # Module not found
# # # from typing import Dict, List, Set, Tuple  # Module not found  # Module not found  # Module not found

REQUIREMENT_FILES = ("requirements.txt",)

# Map import names to canonical pip package suggestions
IMPORT_TO_PIP = {
    # common aliases
    "PIL": "pillow",
    "cv2": "opencv-python",
    "sklearn": "scikit-learn",
    "yaml": "pyyaml",
    "fitz": "pymupdf",
    "PyMuPDF": "pymupdf",
    "sentence_transformers": "sentence-transformers",
    "torch_geometric": "torch-geometric",
    "node2vec": "node2vec",
    "whoosh": "whoosh",
    "pyserini": "pyserini",
    "opentelemetry": "opentelemetry-sdk",
    "rich": "rich",
    "loguru": "loguru",
    "fuzzywuzzy": "fuzzywuzzy",
    "redis": "redis",
    "ray": "ray",
    "prometheus_client": "prometheus-client",
    "aioredis": "aioredis",
    "pm4py": "pm4py",
    "deap": "deap",
    "kubernetes": "kubernetes",
    "httpx": "httpx",
    "sqlalchemy": "sqlalchemy",
    "pandas": "pandas",
    "tabula": "tabula-py",
    "camelot": "camelot-py",
    "pdfplumber": "pdfplumber",
    "pytesseract": "pytesseract",
    "easyocr": "easyocr",
    "langdetect": "langdetect",
    "textstat": "textstat",
    "unidecode": "Unidecode",
    "ftfy": "ftfy",
    "google.cloud": "google-cloud",
    "kafka": "kafka-python",
    # present already: faiss -> faiss-cpu; keep map for normalization
    "faiss": "faiss-cpu",
    # libs already declared but normalize name
    "beir": "beir",
    "ot": "POT",
}

# Optional/experimental buckets: these are used in some modules but not required for core pipeline
OPTIONAL_IMPORTS = {
    "gudhi",  # TDA
    "ripser",  # TDA
}

# Known compatibility heuristics to flag potential issues (not authoritative)
def detect_potential_conflicts(pins: Dict[str, str]) -> List[str]:
    issues = []
# # #     # Extract versions (if any) from pins like ">=1.2.3" or "==1.2.3"  # Module not found  # Module not found  # Module not found
    def ver(pkg: str) -> str:
        spec = pins.get(pkg)
        return spec or ""

    # Transformers + torch compatibility (heuristic)
    if "transformers" in pins and "torch" in pins:
        # require torch >=1.10 typically; project has torch>=2.0.0 which is fine
        pass

    # scikit-learn + numpy + scipy
    if "scikit-learn" in pins and "numpy" in pins and "scipy" in pins:
        # Minimal heuristic: require numpy >=1.20 for sklearn>=1.0, scipy>=1.7
        # Current reqs (>=1.24, >=1.11) look okay.
        pass

    # spacy extra notation
    if any(k.startswith("spacy[") for k in pins):
        issues.append(
            "spacy[en_core_web_sm] specified: language models are not installed via pip extras. "
            "Run: python -m spacy download en_core_web_sm in deployment environments."
        )

    return issues


def parse_declared_requirements(project_root: Path) -> Tuple[Dict[str, str], List[str]]:
    """Return (package->specifier, raw_lines). Keeps last specifier if duplicates appear."""
    pkg_to_spec: Dict[str, str] = {}
    raw_lines: List[str] = []
    for fname in REQUIREMENT_FILES:
        fpath = project_root / fname
        if not fpath.exists():
            continue
        for line in fpath.read_text().splitlines():
            raw_lines.append(line)
            s = line.strip()
            if not s or s.startswith("#"):
                continue
            # split on comparison operators or ; extras
            m = re.match(r"^([A-Za-z0-9_.\-\[\]]+)(.*)$", s)
            if not m:
                continue
            name = m.group(1)
            spec = m.group(2).strip()
            pkg_to_spec[name] = spec or ""
    return pkg_to_spec, raw_lines


def top_level_import(name: str) -> str:
    # Reduce dotted imports to top level for comparison
    return name.split(".")[0]


def load_project_imports(project_root: Path) -> Dict[str, Set[str]]:
# # #     """Load imports mapping from project_analysis_report.json if available.  # Module not found  # Module not found  # Module not found
    Returns mapping: file->set(import_names)
    """
    report_path = project_root / "project_analysis_report.json"
    if not report_path.exists():
        return {}
    try:
        data = json.loads(report_path.read_text())
        imports_map = data.get("imports", {})
        # Ensure sets
        return {f: set(v) for f, v in imports_map.items()}
    except Exception:
        return {}


def normalize_declared(pkgs: Dict[str, str]) -> Dict[str, str]:
    """Normalize known aliases in declared requirements (e.g., faiss-cpu vs faiss)."""
    norm = {}
    for k, v in pkgs.items():
        norm[k] = v
    return norm


def compute_missing_deps(imports_map: Dict[str, Set[str]], declared: Dict[str, str]) -> Tuple[Set[str], Dict[str, str]]:
    """Compute names of missing deps and suggested pip packages."""
    declared_base = {p.split("[")[0].lower() for p in declared.keys()}
    used: Set[str] = set()
    for imps in imports_map.values():
        for mod in imps:
            if mod.startswith(('.', 'tests.', 'egw_query_expansion', 'canonical_flow')):
                continue
            top = top_level_import(mod)
            used.add(top)
    missing: Set[str] = set()
    suggestions: Dict[str, str] = {}
    for mod in sorted(used):
        # Skip stdlib guesses quickly
        if mod in {"os","sys","json","re","pathlib","typing","enum","hashlib","logging","time","datetime","uuid","threading","queue","asyncio","tempfile","sqlite3","itertools","functools","inspect","subprocess","concurrent","math","random","statistics","gc","io","shutil","contextlib","warnings","traceback","dataclasses","collections","ast","xml","unittest","importlib","argparse"}:
            continue
        # If declared contains normalized form
        if mod.lower() in declared_base:
            continue
        # Check alias mapping
        pip_name = IMPORT_TO_PIP.get(mod)
        if not pip_name:
            # Try dotted key mapping like google.cloud
            if mod in ("google",):
                pip_name = "google-cloud"
            else:
                pip_name = mod  # fallback: same name
        missing.add(mod)
        suggestions[mod] = pip_name
    return missing, suggestions


def audit(project_root: Path) -> Dict:
    declared, raw = parse_declared_requirements(project_root)
    declared = normalize_declared(declared)
    imports_map = load_project_imports(project_root)

    missing, suggestions = compute_missing_deps(imports_map, declared)

    # Duplicates and overlaps
    name_counts: Dict[str, int] = {}
    for name in declared.keys():
        key = name.split("[")[0].lower()
        name_counts[key] = name_counts.get(key, 0) + 1
    duplicates = [k for k, c in name_counts.items() if c > 1]

    conflicts = detect_potential_conflicts(declared)

    optional_used = sorted([m for m in missing if m in OPTIONAL_IMPORTS])

    # Recommendations
    recs: List[str] = []
    if missing:
        recs.append("Add missing third-party dependencies to requirements.txt (see suggestions).")
    if optional_used:
        recs.append("Consider moving TDA libraries (gudhi, ripser) to an optional extras or keep commented with instructions.")
    if duplicates:
        recs.append(f"Deduplicate repeated requirement entries: {', '.join(duplicates)}")
    if conflicts:
        recs.extend(conflicts)
    if not recs:
        recs.append("No critical issues detected. Requirements appear consistent for core modules.")

    # Build a compact view of declared pins
    declared_list = [f"{k}{v}" if v else k for k, v in declared.items()]

    return {
        "project_root": str(project_root),
        "declared_dependencies": sorted(declared_list),
        "missing_dependencies": [
            {"import": m, "suggested_package": suggestions.get(m, m)} for m in sorted(missing)
        ],
        "optional_or_experimental": optional_used,
        "duplicates": duplicates,
        "potential_conflicts": conflicts,
        "recommendations": recs,
    }


def save_report(project_root: Path, report: Dict) -> Path:
    out_dir = project_root / "canonical_flow"
    out_dir.mkdir(parents=True, exist_ok=True)
    out_path = out_dir / "dependency_audit.json"
    out_path.write_text(json.dumps(report, indent=2))
    return out_path


def run(project_root: str) -> Path:
    root = Path(project_root).resolve()
    report = audit(root)
    return save_report(root, report)


if __name__ == "__main__":
    proj = sys.argv[1] if len(sys.argv) > 1 else "."
    path = run(proj)
    print(f"Dependency audit written to: {path}")
